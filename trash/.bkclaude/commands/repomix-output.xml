This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
refactor/
  refactor-code.md
analyze-gemini.md
checklist-gemini.md
complete-phase-gemini-full.md
complete-phase.md
customplan-gemini-full.md
customplan.md
debug-gemini-v2.md
debug-gemini-v3.md
debug-gemini.md
gemdiffs.md
geminictx.md
generate-agent-checklist-v2.md
generate-agent-checklist.md
generate-doc-context.md
implementation-gemini-full.md
implementation.md
mkclaudes.md
phase-checklist-gemini-full.md
phase-checklist.md
priming.md
pyrefly-fix.md
taskctx.md
update-docs.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="refactor/refactor-code.md">
# Refactoring Analysis Command

⚠️ **CRITICAL: THIS IS AN ANALYSIS-ONLY TASK** ⚠️
```
DO NOT MODIFY ANY CODE FILES
DO NOT CREATE ANY TEST FILES  
DO NOT EXECUTE ANY REFACTORING
ONLY ANALYZE AND GENERATE A REPORT
```

You are a senior software architect with 20+ years of experience in large-scale refactoring, technical debt reduction, and code modernization. You excel at safely transforming complex, monolithic code into maintainable, modular architectures while maintaining functionality and test coverage. You treat refactoring large files like "surgery on a live patient" - methodical, safe, and thoroughly tested at each step.

## YOUR TASK
1. **ANALYZE** the target file(s) for refactoring opportunities
2. **CREATE** a detailed refactoring plan (analysis only)
3. **WRITE** the plan to a report file: `reports/refactor/refactor_[target]_DD-MM-YYYY_HHMMSS.md`
4. **DO NOT** execute any refactoring or modify any code

**OUTPUT**: A comprehensive markdown report file saved to the reports directory

## REFACTORING ANALYSIS FRAMEWORK

### Core Principles (For Analysis)
1. **Safety Net Assessment**: Analyze current test coverage and identify gaps
2. **Surgical Planning**: Identify complexity hotspots and prioritize by lowest risk
3. **Incremental Strategy**: Plan extractions of 40-60 line blocks
4. **Verification Planning**: Design test strategy for continuous verification

### Multi-Agent Analysis Workflow

Break this analysis into specialized agent tasks:

1. **Discovery Agent**: Analyze codebase structure, tech stack, and conventions
2. **Test Coverage Agent**: Evaluate existing tests and identify coverage gaps
3. **Complexity Analysis Agent**: Measure complexity and identify hotspots
4. **Architecture Agent**: Assess current design and propose target architecture
5. **Planning Agent**: Create detailed, step-by-step refactoring plan
6. **Risk Assessment Agent**: Evaluate risks and create mitigation strategies
7. **Documentation Agent**: Synthesize findings into comprehensive report

Use `<thinking>` tags to show your reasoning process for complex analytical decisions. Allocate extended thinking time for each analysis phase.

## PHASE 1: PROJECT DISCOVERY & CONTEXT

### 1.1 Codebase Analysis

**Use Claude Code Tools**:
```
# Discover project structure
Task: "Analyze project structure and identify main components"
Glob: "**/*.{py,js,ts,java,go,rb,php,cs,cpp,rs}" 
Grep: "class|function|def|interface|struct" for architecture patterns

# Find configuration files
Glob: "**/package.json|**/pom.xml|**/build.gradle|**/Cargo.toml|**/go.mod|**/Gemfile|**/composer.json"

# Identify test frameworks
Grep: "test|spec|jest|pytest|unittest|mocha|jasmine|rspec|phpunit"
```

**Analyze**:
- Primary programming language(s)
- Framework(s) and libraries in use
- Project structure and organization
- Naming conventions and code style
- Dependency management approach
- Build and deployment configuration

### 1.2 Current State Assessment

**File Analysis Criteria**:
- File size (lines of code)
- Number of classes/functions
- Responsibility distribution
- Coupling and cohesion metrics
- Change frequency (if git history available)

**Identify Refactoring Candidates**:
- Files > 500 lines
- Functions > 100 lines
- Classes with > 10 methods
- High cyclomatic complexity (> 15)
- Multiple responsibilities in single file

**Code Smell Detection**:
- Long parameter lists (>4 parameters)
- Duplicate code detection (>10 similar lines)
- Dead code identification
- God object/function patterns
- Feature envy (methods using other class data)
- Inappropriate intimacy between classes
- Lazy classes (classes that do too little)
- Message chains (a.b().c().d())

## PHASE 2: TEST COVERAGE ANALYSIS

### 2.1 Existing Test Discovery

**Use Tools**:
```
# Find test files
Glob: "**/*test*.{py,js,ts,java,go,rb,php,cs,cpp,rs}|**/*spec*.{py,js,ts,java,go,rb,php,cs,cpp,rs}"

# Analyze test patterns
Grep: "describe|it|test|assert|expect" in test files

# Check coverage configuration
Glob: "**/*coverage*|**/.coveragerc|**/jest.config.*|**/pytest.ini"
```

### 2.2 Coverage Gap Analysis

**REQUIRED Analysis**:
- Run coverage analysis if .coverage files exist
- Analyze test file naming patterns and locations
- Map test files to source files
- Identify untested public functions/methods
- Calculate test-to-code ratio
- Examine assertion density in existing tests

**Assess**:
- Current test coverage percentage
- Critical paths without tests
- Test quality and assertion depth
- Mock/stub usage patterns
- Integration vs unit test balance

**Coverage Mapping Requirements**:
1. Create a table mapping source files to test files
2. List all public functions/methods without tests
3. Identify critical code paths with < 80% coverage
4. Calculate average assertions per test
5. Document test execution time baselines

**Generate Coverage Report**:
```
# Language-specific coverage commands
Python: pytest --cov
JavaScript: jest --coverage
Java: mvn test jacoco:report
Go: go test -cover
```

### 2.3 Safety Net Requirements

**Define Requirements (For Planning)**:
- Target coverage: 80-90% for files to refactor
- Critical path coverage: 100% required
- Test types needed (unit, integration, e2e)
- Test data requirements
- Mock/stub strategies

⚠️ **REMINDER**: Document what tests WOULD BE NEEDED, do not create them

## PHASE 3: COMPLEXITY ANALYSIS

### 3.1 Metrics Calculation

**REQUIRED Measurements**:
- Calculate exact cyclomatic complexity using AST analysis
- Measure actual lines vs logical lines of code
- Count parameters, returns, and branches per function
- Generate coupling metrics between classes/modules
- Create a complexity heatmap with specific scores

**Universal Complexity Metrics**:
1. **Cyclomatic Complexity**: Decision points in code (exact calculation required)
2. **Cognitive Complexity**: Mental effort to understand (score 1-100)
3. **Depth of Inheritance**: Class hierarchy depth (exact number)
4. **Coupling Between Objects**: Inter-class dependencies (afferent/efferent)
5. **Lines of Code**: Physical vs logical lines (both required)
6. **Nesting Depth**: Maximum nesting levels (exact depth)
7. **Maintainability Index**: Calculated metric (0-100)

**Required Output Table Format**:
```
| Function/Class | Lines | Cyclomatic | Cognitive | Parameters | Nesting | Risk |
|----------------|-------|------------|-----------|------------|---------|------|
| function_name  | 125   | 18         | 45        | 6          | 4       | HIGH |
```

**Language-Specific Analysis**:
```python
# Python example
def analyze_complexity(file_path):
    # Use ast module for exact metrics
    # Calculate cyclomatic complexity per function
    # Measure nesting depth precisely
    # Count decision points, loops, conditions
    # Generate maintainability index
```

### 3.2 Hotspot Identification

**Priority Matrix**:
```
High Complexity + High Change Frequency = CRITICAL
High Complexity + Low Change Frequency = HIGH
Low Complexity + High Change Frequency = MEDIUM
Low Complexity + Low Change Frequency = LOW
```

### 3.3 Dependency Analysis

**REQUIRED Outputs**:
- List ALL files that import the target module
- Create visual dependency graph (mermaid or ASCII)
- Identify circular dependencies with specific paths
- Calculate afferent/efferent coupling metrics
- Map public vs private API usage

**Map Dependencies**:
- Internal dependencies (within project) - list specific files
- External dependencies (libraries, frameworks) - with versions
- Circular dependencies (must resolve) - show exact cycles
- Hidden dependencies (globals, singletons) - list all instances
- Transitive dependencies - full dependency tree

**Dependency Matrix Format**:
```
| Module | Imports From | Imported By | Afferent | Efferent | Instability |
|--------|-------------|-------------|----------|----------|-------------|
| utils  | 5 modules   | 12 modules  | 12       | 5        | 0.29        |
```

**Circular Dependency Detection**:
```
Cycle 1: moduleA -> moduleB -> moduleC -> moduleA
Cycle 2: classX -> classY -> classX
```

## PHASE 4: REFACTORING STRATEGY

### 4.1 Target Architecture

**Design Principles**:
- Single Responsibility Principle
- Open/Closed Principle
- Dependency Inversion
- Interface Segregation
- Don't Repeat Yourself (DRY)

**Architectural Patterns**:
- Layer separation (presentation, business, data)
- Module boundaries and interfaces
- Service/component organization
- Plugin/extension points

### 4.2 Extraction Strategy

**Safe Extraction Patterns**:
1. **Extract Method**: Pull out cohesive code blocks
2. **Extract Class**: Group related methods and data
3. **Extract Module**: Create focused modules
4. **Extract Interface**: Define clear contracts
5. **Extract Service**: Isolate business logic

**Pattern Selection Criteria**:
- For functions >50 lines: Extract Method pattern
- For classes >7 methods: Extract Class pattern
- For repeated code blocks: Extract to shared utility
- For complex conditions: Extract to well-named predicate
- For data clumps: Extract to value object
- For long parameter lists: Introduce parameter object

**Extraction Size Guidelines**:
- Methods: 20-60 lines (sweet spot: 30-40)
- Classes: 100-200 lines (5-7 methods)
- Modules: 200-500 lines (single responsibility)
- Clear single responsibility

**Code Example Requirements**:
For each extraction, provide:
1. BEFORE code snippet (current state)
2. AFTER code snippet (refactored state)
3. Migration steps
4. Test requirements

### 4.3 Incremental Plan

**Step-by-Step Approach (For Documentation)**:
1. Identify extraction candidate (40-60 lines)
2. Plan tests for current behavior
3. Document extraction to new method/class
4. List references to update
5. Define test execution points
6. Plan refactoring of extracted code
7. Define verification steps
8. Document commit strategy

⚠️ **ANALYSIS ONLY**: This is the plan that WOULD BE followed during execution

## PHASE 5: RISK ASSESSMENT

### 5.1 Risk Categories

**Technical Risks**:
- Breaking existing functionality
- Performance degradation
- Security vulnerabilities introduction
- API/interface changes
- Data migration requirements

**Project Risks**:
- Timeline impact
- Resource requirements
- Team skill gaps
- Integration complexity
- Deployment challenges

### 5.2 Mitigation Strategies

**Risk Mitigation**:
- Feature flags for gradual rollout
- A/B testing for critical paths
- Performance benchmarks before/after
- Security scanning at each step
- Rollback procedures

### 5.3 Rollback Plan

**Rollback Strategy**:
1. Git branch protection
2. Tagged releases before major changes
3. Database migration rollback scripts
4. Configuration rollback procedures
5. Monitoring and alerts

## PHASE 6: EXECUTION PLANNING

### 6.1 Task Breakdown

**Generate TodoWrite Compatible Tasks**:
```json
[
  {
    "id": "establish_test_baseline",
    "content": "Create test suite achieving 80-90% coverage for target files",
    "priority": "high",
    "estimated_hours": 8
  },
  {
    "id": "extract_auth_logic",
    "content": "Extract authentication logic from main.py lines 145-205",
    "priority": "high",
    "estimated_hours": 4
  },
  // ... more tasks
]
```

### 6.2 Timeline Estimation

**Phase Timeline**:
- Test Coverage: X days
- Extraction Phase 1: Y days
- Extraction Phase 2: Z days
- Integration Testing: N days
- Documentation: M days

### 6.3 Success Metrics

**REQUIRED Baselines (measure before refactoring)**:
- Memory usage: Current MB vs projected MB
- Import time: Measure current import performance (seconds)
- Function call overhead: Benchmark critical paths (ms)
- Cache effectiveness: Current hit rates (%)
- Async operation latency: Current measurements (ms)

**Measurable Outcomes**:
- Code coverage: 80% → 90%
- Cyclomatic complexity: <15 per function
- File size: <500 lines per file
- Build time: ≤ current time
- Performance: ≥ current benchmarks
- Bug count: Reduced by X%
- Memory usage: ≤ current baseline
- Import time: < 0.5s per module

**Performance Measurement Commands**:
```python
# Memory profiling
import tracemalloc
tracemalloc.start()
# ... code ...
current, peak = tracemalloc.get_traced_memory()

# Import time
import time
start = time.time()
import module_name
print(f"Import time: {time.time() - start}s")

# Function benchmarking
import timeit
timeit.timeit('function_name()', number=1000)
```

## REPORT GENERATION

### Report Structure

**Generate Report File**:
1. **Timestamp**: DD-MM-YYYY_HHMMSS format
2. **Directory**: `reports/refactor/` (create if it doesn't exist)
3. **Filename**: `refactor_[target_file]_DD-MM-YYYY_HHMMSS.md`

### Report Sections

```markdown
# REFACTORING ANALYSIS REPORT
**Generated**: DD-MM-YYYY HH:MM:SS
**Target File(s)**: [files to refactor]
**Analyst**: Claude Refactoring Specialist
**Report ID**: refactor_[target]_DD-MM-YYYY_HHMMSS

## EXECUTIVE SUMMARY
[High-level overview of refactoring scope and benefits]

## CURRENT STATE ANALYSIS

### File Metrics Summary Table
| Metric | Value | Target | Status |
|--------|-------|---------|---------|
| Total Lines | X | <500 | ⚠️ |
| Functions | Y | <20 | ✅ |
| Classes | Z | <10 | ⚠️ |
| Avg Complexity | N | <15 | ❌ |

### Code Smell Analysis
| Code Smell | Count | Severity | Examples |
|------------|-------|----------|----------|
| Long Methods | X | HIGH | function_a (125 lines) |
| God Classes | Y | CRITICAL | ClassX (25 methods) |
| Duplicate Code | Z | MEDIUM | Lines 145-180 similar to 450-485 |

### Test Coverage Analysis
| File/Module | Coverage | Missing Lines | Critical Gaps |
|-------------|----------|---------------|---------------|
| module.py | 45% | 125-180, 200-250 | auth_function() |
| utils.py | 78% | 340-360 | None |

### Complexity Analysis
| Function/Class | Lines | Cyclomatic | Cognitive | Parameters | Nesting | Risk |
|----------------|-------|------------|-----------|------------|---------|------|
| calculate_total() | 125 | 45 | 68 | 8 | 6 | CRITICAL |
| DataProcessor | 850 | - | - | - | - | HIGH |
| validate_input() | 78 | 18 | 32 | 5 | 4 | HIGH |

### Dependency Analysis
| Module | Imports From | Imported By | Coupling | Risk |
|--------|-------------|-------------|----------|------|
| utils.py | 12 modules | 25 modules | HIGH | ⚠️ |

### Performance Baselines
| Metric | Current | Target | Notes |
|--------|---------|---------|-------|
| Import Time | 1.2s | <0.5s | Needs optimization |
| Memory Usage | 45MB | <30MB | Contains large caches |
| Test Runtime | 8.5s | <5s | Slow integration tests |

## REFACTORING PLAN

### Phase 1: Test Coverage Establishment
#### Tasks (To Be Done During Execution):
1. Would need to write unit tests for `calculate_total()` function
2. Would need to add integration tests for `DataProcessor` class
3. Would need to create test fixtures for complex scenarios

#### Estimated Time: 2 days

**Note**: This section describes what WOULD BE DONE during actual refactoring

### Phase 2: Initial Extractions
#### Task 1: Extract calculation logic
- **Source**: main.py lines 145-205
- **Target**: calculations/total_calculator.py
- **Method**: Extract Method pattern
- **Tests Required**: 5 unit tests
- **Risk Level**: LOW

[Continue with detailed extraction plans...]

## RISK ASSESSMENT

### Risk Matrix
| Risk | Likelihood | Impact | Score | Mitigation |
|------|------------|---------|-------|------------|
| Breaking API compatibility | Medium | High | 6 | Facade pattern, versioning |
| Performance degradation | Low | Medium | 3 | Benchmark before/after |
| Circular dependencies | Medium | High | 6 | Dependency analysis first |
| Test coverage gaps | High | High | 9 | Write tests before refactoring |

### Technical Risks
- **Risk 1**: Breaking API compatibility
  - Mitigation: Maintain facade pattern
  - Likelihood: Medium
  - Impact: High

### Timeline Risks
- Total Estimated Time: 10 days
- Critical Path: Test coverage → Core extractions
- Buffer Required: +30% (3 days)

## IMPLEMENTATION CHECKLIST

```json
// TodoWrite compatible task list
[
  {"id": "1", "content": "Review and approve refactoring plan", "priority": "high"},
  {"id": "2", "content": "Set up feature branch 'refactor/[target]'", "priority": "high"},
  {"id": "3", "content": "Establish test baseline - 85% coverage", "priority": "high"},
  // ... complete task list
]
```

## SUCCESS METRICS
- [ ] All tests passing after each extraction
- [ ] Code coverage e 85%
- [ ] No performance degradation
- [ ] Cyclomatic complexity < 15
- [ ] File sizes < 500 lines

## APPENDICES

### A. Complexity Analysis Details
**Function-Level Metrics**:
```
function_name(params): 
  - Physical Lines: X
  - Logical Lines: Y
  - Cyclomatic: Z
  - Cognitive: N
  - Decision Points: A
  - Exit Points: B
```

### B. Dependency Graph
```mermaid
graph TD
    A[target_module] --> B[dependency1]
    A --> C[dependency2]
    B --> D[shared_util]
    C --> D
    D --> A
    style D fill:#ff9999
```
Note: Circular dependency detected (highlighted in red)

### C. Test Plan Details
**Test Coverage Requirements**:
| Component | Current | Required | New Tests Needed |
|-----------|---------|----------|------------------|
| Module A | 45% | 85% | 15 unit, 5 integration |
| Module B | 0% | 80% | 25 unit, 8 integration |

### D. Code Examples
**BEFORE (current state)**:
```python
def complex_function(data, config, user, session, cache, logger):
    # 125 lines of nested logic
    if data:
        for item in data:
            if item.type == 'A':
                # 30 lines of processing
            elif item.type == 'B':
                # 40 lines of processing
```

**AFTER (refactored)**:
```python
def process_data(data: List[Item], context: ProcessContext):
    """Process data items by type."""
    for item in data:
        processor = get_processor(item.type)
        processor.process(item, context)

class ProcessContext:
    """Encapsulates processing dependencies."""
    def __init__(self, config, user, session, cache, logger):
        self.config = config
        # ...
```

---
*This report serves as a comprehensive guide for refactoring execution. 
Reference this document when implementing: @reports/refactor/refactor_[target]_DD-MM-YYYY_HHMMSS.md*
```

## ANALYSIS EXECUTION

When invoked with target file(s), this prompt will:

1. **Discover** project structure and conventions using Task/Glob/Grep (READ ONLY)
2. **Analyze** test coverage using appropriate tools (READ ONLY)
3. **Calculate** complexity metrics for all target files (ANALYSIS ONLY)
4. **Identify** safe extraction points (40-60 line blocks) (PLANNING ONLY)
5. **Plan** incremental refactoring with test verification (DOCUMENTATION ONLY)
6. **Assess** risks and create mitigation strategies (ANALYSIS ONLY)
7. **Generate** comprehensive report with execution guide (WRITE REPORT FILE ONLY)

The report provides a complete roadmap that can be followed step-by-step during actual refactoring, ensuring safety and success.

## FINAL OUTPUT INSTRUCTIONS

📝 **REQUIRED ACTION**: Use the Write tool to create the report file at:
```
reports/refactor/refactor_[target_file_name]_DD-MM-YYYY_HHMMSS.md
```

Example: `reports/refactor/refactor_mcp_server_14-07-2025_143022.md`

⚠️ **DO NOT**:
- Modify any source code files
- Create any test files
- Run any refactoring tools
- Execute any code changes
- Make any commits

✅ **DO**:
- Analyze the code structure
- Document refactoring opportunities
- Create a comprehensive plan
- Write the plan to the report file

## TARGET FILE(S) TO ANALYZE

<file_to_refactor>
{file_path}
</file_to_refactor>

<additional_context>
{context if context else "No additional context provided"}
</additional_context>

---

**REFACTORING ANALYSIS MISSION**: 
1. Analyze the specified file(s) for refactoring opportunities
2. Create a comprehensive refactoring plan (DO NOT EXECUTE)
3. Write the plan to: `reports/refactor/refactor_[target]_DD-MM-YYYY_HHMMSS.md`

Focus on safety, incremental progress, and maintainability. The report should be detailed enough that any developer can follow it step-by-step to successfully refactor the code with minimal risk.

🚨 **FINAL REMINDER**: 
- This is ANALYSIS ONLY - do not modify any code
- Your ONLY output should be the report file in the reports directory
- Use the Write tool to create the report file
- Do NOT make any changes to source code, tests, or configuration files
</file>

<file path="analyze-gemini.md">
# Command: /analyze-gemini-v3 [analysis_query] [optional_baseline_ref]

**Goal:** Get comprehensive analysis from Gemini 

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**THIS IS THE CORE PURPOSE OF THIS COMMAND:**
1. You MUST run git analysis (with baseline from args if provided)
2. You MUST execute `gemini -p` 
3. You MUST wait for and process Gemini's response
4. You MUST report Gemini's findings to the user

**DO NOT:**
- ❌ Stop after git analysis
- ❌ Provide your own analysis instead of running Gemini
- ❌ Get distracted by intermediate findings
- ❌ Skip the gemini execution for ANY reason

**EXECUTION CHECKPOINT:** After running git commands, STOP and ask yourself: "Have I executed the `gemini -p` command yet?" If NO, execute it immediately.

---

## 🤖 **CONTEXT: YOU ARE CLAUDE CODE**

You are Claude Code - the autonomous command-line tool that executes shell commands directly. You:
- **Execute** commands autonomously (don't just suggest them)
- **Run** the entire workflow without human intervention
- **Complete** analysis from start to finish in one go

## 🚨 **YOU MUST EXECUTE GEMINI - THIS IS NOT OPTIONAL**

When a user runs `/analyze-gemini`, you handle EVERYTHING:
- ✅ Run all git commands automatically
- ✅ Create all temp files automatically  
- **✅ Execute the `gemini -p` command directly (MANDATORY)**
- ✅ Process Gemini's response automatically
- ✅ Report findings and action plan
- ❌ NO asking users to copy commands
- ❌ NO waiting for users to paste outputs
- ❌ NO providing your own analysis without running Gemini

---

## 🎯 **Your Complete Autonomous Flow**

```mermaid
graph LR
    A[User: /analyze-gemini] --> B[You: Identify issue from context]
    B --> C[You: Run git analysis automatically]
    C --> D[You: Create temp files]
    D --> E[**You: MUST Execute gemini -p command**]
    E --> F[You: Analyze Gemini output]
    F --> G[You: Report GEMINI'S findings]
```

**Total user interaction: ONE command**

---

## ✅ **EXECUTION CHECKLIST**

Before reporting ANY findings, verify:
- [ ] I ran git analysis commands
- [ ] I created ./tmp/analysis_context.txt WITH ACTUAL CONTENT (not shell commands)
- [ ] **I EXECUTED `gemini -p` command** ← THIS IS MANDATORY
- [ ] I received Gemini's response
- [ ] I'm reporting GEMINI'S findings, not my own analysis

If you haven't executed `gemini -p`, STOP and do it now.
If analysis_context.txt contains `$(cat ...)` commands, STOP and fix it using the incremental approach.

---

## 📋 **YOUR EXECUTION WORKFLOW**

### Step 1: Assess Current Context

Determine from the conversation/context:
- What's the bug/issue
- What's been tried already
- Your current working theory
- Check if baseline was provided via arguments

**Parse baseline from arguments:**
```bash
# Ensure we have a tmp directory for our analysis files
mkdir -p ./tmp

# $ARGUMENTS contains whatever user typed after /analyze-gemini-v3
BASELINE_REF="$ARGUMENTS"
if [ -n "$BASELINE_REF" ]; then
    # User provided a baseline ref
    if git rev-parse --verify "$BASELINE_REF" >/dev/null 2>&1; then
        echo "Using provided baseline: $BASELINE_REF"
    else
        echo "Warning: '$BASELINE_REF' is not a valid git ref, will auto-detect baseline"
        BASELINE_REF=""
    fi
else
    echo "No baseline provided, will auto-detect from main/master/HEAD~5"
fi
```

**Only ask the user if:**
- The issue is unclear from context
- No baseline provided and auto-detection fails
- Provided baseline is invalid and you need a valid one

### Step 2: Run Debug Analysis (You Execute This)

**⚠️ CRITICAL ISSUE:** Heredocs with command substitution (`$(...)`) do not work properly in Claude Code's environment. Use the simple sequential approach below that executes each command separately.

**📁 NOTE:** Using `./tmp/` instead of `/tmp/` to avoid permissions issues and keep files in the project directory.

Automatically execute these commands without any user intervention:

```bash
# Ensure tmp directory exists in current working directory (if not already created in Step 1)
mkdir -p ./tmp

# Get recent commit history (NOTE: Use dash separator, not pipe to avoid parsing issues)
git log -n 10 --pretty=format:"%h %ad - %s [%an]" --date=short > ./tmp/analysis_git_log.txt

# Get current status
git status --porcelain > ./tmp/analysis_git_status.txt

# Use baseline ref from arguments or determine automatically
if [ -n "$BASELINE_REF" ]; then
    # User provided baseline
    BASELINE="$BASELINE_REF"
elif git rev-parse --verify main >/dev/null 2>&1; then
    # Default to main if it exists
    BASELINE="main"
elif git rev-parse --verify master >/dev/null 2>&1; then
    # Fall back to master
    BASELINE="master"
else
    # Use HEAD~5 as last resort
    BASELINE="HEAD~5"
fi

# Get diffs from baseline
git diff "$BASELINE"..HEAD --stat > ./tmp/analysis_diff_stat.txt
git diff "$BASELINE"..HEAD --name-status > ./tmp/analysis_diff_names.txt
git diff "$BASELINE"..HEAD -- ptycho/ src/ configs/ package.json requirements.txt > ./tmp/analysis_diff_details.txt

# CRITICAL: Build analysis context file using simple sequential approach
# Heredocs with command substitution don't work in Claude Code's environment
# Each command is executed separately to ensure actual content is written

echo "## RECENT COMMITS" > ./tmp/analysis_context.txt
cat ./tmp/analysis_git_log.txt >> ./tmp/analysis_context.txt
echo -e "\n## CURRENT GIT STATUS" >> ./tmp/analysis_context.txt  
cat ./tmp/analysis_git_status.txt >> ./tmp/analysis_context.txt
echo -e "\n## BASELINE USED: $BASELINE" >> ./tmp/analysis_context.txt
echo -e "\n## DIFF STATISTICS (from $BASELINE to HEAD)" >> ./tmp/analysis_context.txt
cat ./tmp/analysis_diff_stat.txt >> ./tmp/analysis_context.txt 2>/dev/null || echo "No baseline diff available" >> ./tmp/analysis_context.txt
echo -e "\n## FILES CHANGED" >> ./tmp/analysis_context.txt
cat ./tmp/analysis_diff_names.txt >> ./tmp/analysis_context.txt 2>/dev/null || echo "No file changes detected" >> ./tmp/analysis_context.txt
echo -e "\n## DETAILED CODE CHANGES" >> ./tmp/analysis_context.txt
head -2000 ./tmp/analysis_diff_details.txt >> ./tmp/analysis_context.txt 2>/dev/null || echo "No detailed diffs available" >> ./tmp/analysis_context.txt

# Python alternative if shell approaches fail:
# BASELINE="$BASELINE" python3 << 'EOF'
# import os
# os.makedirs('./tmp', exist_ok=True)
# baseline = os.environ.get('BASELINE', 'unknown')
# sections = [
#     ('RECENT COMMITS', './tmp/analysis_git_log.txt'),
#     ('CURRENT GIT STATUS', './tmp/analysis_git_status.txt'),
#     (f'BASELINE USED: {baseline}', None),
#     (f'DIFF STATISTICS (from {baseline} to HEAD)', './tmp/analysis_diff_stat.txt'),
#     ('FILES CHANGED', './tmp/analysis_diff_names.txt'),
#     ('DETAILED CODE CHANGES', './tmp/analysis_diff_details.txt')
# ]
# 
# with open('./tmp/analysis_context.txt', 'w') as out:
#     for title, path in sections:
#         out.write(f'## {title}\n')
#         if path is None:
#             out.write('\n')
#         else:
#             try:
#                 with open(path) as f:
#                     content = f.read()
#                     if 'DETAILED CODE CHANGES' in title:
#                         lines = content.splitlines()[:2000]
#                         content = '\n'.join(lines)
#                     out.write(content + '\n')
#             except:
#                 out.write(f'No data available\n')
#         out.write('\n')
# EOF

# Verify the file contains actual content
echo "Verifying analysis context file..."
if grep -q '$(cat' ./tmp/analysis_context.txt; then
    echo "❌ ERROR: File still contains shell commands! Try the Python approach above."
else
    echo "✅ Debug context file ready with actual git data"
fi
```

### Step 2.5: MANDATORY prepare gemini inputs / context 
The analysis_context.txt file MUST contain:
- Recent commit history (actual commits, not `$(cat ...)` commands)
- Current git status (actual file list)
- **Baseline used for comparison** (branch/commit shown in file)
- Diff statistics showing which files changed and by how much
- Complete file change list
- **Actual code diffs** showing exact line-by-line changes

**⚠️ CRITICAL:** If Gemini receives shell commands instead of actual content, it cannot analyze your issue!

Pay special attention to:
- Recent commits that might have introduced the issue
- **Actual code changes in the diffs** (not just commit messages)
- Modified configuration files
- Dependency updates


### Step 3: MANDATORY - Execute Gemini Analysis

**🔴 STOP - THIS STEP IS MANDATORY - DO NOT SKIP**
#### Step 3.1
Run this shell command:
```bash
npx repomix@latest .   --include "**/*.sh,**/*.md,**/*.py,**/*.c,**/*.h,**/*.json,**/*.log" --ignore ".aider.chat.history.md,PtychoNN/**,build/**,ptycho/trash/**,diagram/**,tests/**,notebooks/**,Oclaude.md,ptycho.md,plans/archive/**,dpl.md"
```
It will generate a file ./repomix-output.xml, which we will provide to gemini

#### Step 3.2
delete ./gemini-prompt.md if it exists.

#### Step 3.3
You MUST now populate this command template and save it to gemini-prompt.md. Note that text in [] brackets are placeholders, to be populated by you. 

```markdown
<task> Debug this issue with FRESH EYES:

Carry out the following steps:
<steps/>
<0>
list the files included in the repomix archive
</0>
<1>
REVIEW PROJECT DOCUMENTATION
 - **Read CLAUDE.md thoroughly** - This contains essential project context, architecture, and known patterns
 - **Read DEVELOPER_GUIDE.md carefully** - This explains the development workflow, common issues, and analysis approaches
 - **Understand the project structure** from these documents before diving into the code

These documents contain crucial information about:
- Project architecture and design decisions
- Known quirks and edge cases
- Common analysis patterns
- Project-specific conventions and practices
</1>

<2>
review the provided <git> diff. it represents the increment of possibly-buggy code changes that we're analyzing.
</2> 

<3> 
analyze the issue's prior understanding <summary>. Then, attempt to find the root cause and propose a fix approach 
</3>
</steps>

<summary>
## MY ASSUMPTIONS (PLEASE CHALLENGE THESE)
1. [beliefs about the situation]
2. [Assumptions about data flow]
3. [Assumptions about dependencies]
4. [Assumption about configuration]
5. [Possible call chains to trace]
6. [Possible components needing review]
</summary>

<git>
## GIT CONTEXT
[tell gemini about the debug_context.txt that you generated]
</git>


<guidelines>
## FOCUS ON IDENTIFYING ROOT CAUSES
1. **Challenge My Assumptions:** What am I taking for granted that might be wrong?
2. Do not use tools. All needed context files you need should be included in your context with 
surrounding xml tags. If required information is missing, abort the analysis with an explanataion of the reason you cannot 
successfully continue.
</guidelines>


<output format>
Please provide:
1. **Your full analysis
2. **Suggestions for next steps

</output format>
</task>"
```

#### Step 3.4
You MUST now EXECUTE the following shell commands:
```bash
cat  repomix-output.xml >> gemini-prompt.md
```

```bash
gemini -p "@gemini-prompt.md"
```


**VERIFICATION:** Before proceeding to Step 4, confirm you have:
- [ ] Generated the complete `gemini -p` command
- [ ] EXECUTED the command (not just generated it)
- [ ] Received Gemini's response
- [ ] If any of above are NO, go back and EXECUTE THE COMMAND

### Step 4: Process and Report GEMINI'S Findings (Not Your Own)

After Gemini responds, you:

1. **Analyze GEMINI'S response** (not your own theories)
2. **Identify GEMINI'S key insights**
3. **Create action plan based on GEMINI'S analysis**
4. **Report GEMINI'S findings to user**

Example of your output:
```markdown
## 🎯 Fresh Insights from Gemini

### What Gemini Found That I Missed:
- The nginx configuration was stripping Authorization headers
- Only affects requests > 8KB (why it seemed random)
- The real issue was 3 layers away from where I was looking

### Gemini's Analysis:
- I assumed: JWT validation was broken
- Gemini found: Infrastructure was blocking the headers
- Evidence: Minimal reproduction shows size-dependent behavior

### Action Plan (Based on Gemini's Recommendations):
1. Fix nginx config at /etc/nginx/conf.d/api.conf:23
2. Test with: `curl -X POST https://api/endpoint -H "Authorization: Bearer $TOKEN" -d @large_payload.json`
3. Add monitoring for header stripping
4. Document this infrastructure requirement
```

---

## 🚨 **IF GEMINI COMMAND FAILS**

If the `gemini -p` command fails to execute:
1. Report the failure immediately: "Failed to execute Gemini analysis: [error]"
2. Do NOT provide your own analysis as a substitute
3. Ask user if they want you to try a different approach
4. The command's PURPOSE is to get Gemini's perspective - without it, the command has failed

---

## ⚠️ **PATTERNS TO AVOID**

### ❌ Don't Skip Gemini Execution
**NEVER DO THIS:**
```
You: "Analyzing git history..."
[Run git commands]
"Based on the git diff, I think the issue is..." ❌ WRONG - You didn't run Gemini!
```

**ALWAYS DO THIS:**
```
You: "Analyzing git history and consulting Gemini..."
[Run git commands]
[EXECUTE gemini -p command]
"Based on GEMINI'S analysis..."
```

### ❌ Don't Provide Your Own Analysis Instead
**WRONG:**
```
You: "The git analysis shows changes in auth.py. I believe the issue is..."
```

**RIGHT:**
```
You: "Running Gemini for fresh perspective..."
[EXECUTE gemini -p]
"Gemini identified that the issue is actually..."
```

---

## 📊 **SAMPLE INTERACTIONS**

### ✅ Correct Execution:

```
User: /debug-gemini

You: "I see you're experiencing auth failures with 'Invalid token' errors. 
      Based on our discussion, you suspect the JWT validation at auth.py:45.
      Do you have a baseline branch where auth was working correctly?"

User: "Yes, it works in 'main'"

You: "Perfect. Analyzing the differences and consulting Gemini for fresh perspective..."
     [You run all git commands automatically]
     [You create debug context automatically]
     **[YOU EXECUTE gemini -p COMMAND HERE - MANDATORY]**
     [You process Gemini's response]
     
     "Based on GEMINI'S analysis (not mine), I was tunnel-visioned on the Python code. 
     Gemini found the issue is actually in nginx config - it's stripping the 
     Authorization header for requests over 8KB. Here's Gemini's recommended action plan:
     1. Fix /etc/nginx/conf.d/api.conf:23
     2. Test with the minimal reproduction case Gemini provided
     3. The commit that broke it was a3f2d (3 days ago)"
```

### ❌ What NOT to do:

```
User: /debug-gemini

You: "I see auth failures. Let me analyze..."
     [Run git commands]
     "Based on the git diff, I think the issue is in the recent auth.py changes..."
     ❌ FAILED - You never executed Gemini!
```

---

## 💡 **ENHANCED DEBUG PATTERNS**

### Pattern 1: Baseline Comparison (You Execute All of This)
```bash
# If user provided baseline via argument, use it
# Otherwise use detected baseline (main/master/HEAD~5)
BASELINE="${BASELINE_REF:-main}"

# Ensure tmp directory exists (if not already created)
mkdir -p ./tmp

# Get comprehensive diff from baseline
git diff "$BASELINE"..HEAD --stat > ./tmp/baseline_stat.txt
git diff "$BASELINE"..HEAD --name-status | head -50 > ./tmp/baseline_names.txt
git diff "$BASELINE"..HEAD -- ptycho/ src/ configs/ | head -500 > ./tmp/baseline_diff.txt

# Create combined analysis file with ACTUAL DIFFS (use simple approach)
echo "## BASELINE DIFF SUMMARY" > ./tmp/baseline_analysis.txt
cat ./tmp/baseline_stat.txt >> ./tmp/baseline_analysis.txt
echo -e "\n## FILES CHANGED" >> ./tmp/baseline_analysis.txt
cat ./tmp/baseline_names.txt >> ./tmp/baseline_analysis.txt
echo -e "\n## DETAILED CODE CHANGES (ACTUAL DIFFS)" >> ./tmp/baseline_analysis.txt
cat ./tmp/baseline_diff.txt >> ./tmp/baseline_analysis.txt

# MANDATORY: Execute Gemini analysis
gemini -p "@repomix-output.xml  First review CLAUDE.md and DEVELOPER_GUIDE.md, then analyze regression from baseline $BASELINE..."
```

### Pattern 2: Git Bisect Helper (You Execute All of This)
```bash
# Ensure tmp directory exists (if not already created)
mkdir -p ./tmp

# Get commit history between baseline and HEAD
git log --oneline --graph <baseline>..HEAD > ./tmp/bisect_commits.txt

# MANDATORY: Execute targeted analysis
gemini -p "@repomix-output.xml First review CLAUDE.md and DEVELOPER_GUIDE.md, then identify when bug was introduced..."
```

---

## 🎯 **Why This Approach Works**

1. **Fresh Perspective**: Gemini has no preconceptions about the bug
2. **Comprehensive Context**: Git history + code + configs + logs
3. **Tunnel Vision Breaking**: Explicitly challenges assumptions
4. **Zero Manual Steps**: User runs one command, gets complete analysis
5. **Gemini's Insights**: The whole point is to get an outside perspective

---

## 📈 **Success Metrics**

Track your debugging effectiveness:
- **Gemini Execution Rate**: Must be 100% - if not, the command failed
- **Time to Root Cause**: Usually 2-5 minutes total
- **Tunnel Vision Breaks**: ~80% find issues outside initial focus
- **Minimal Reproduction Success**: ~90% provide working minimal case

---

## 🔧 **TROUBLESHOOTING**

### Debug Context File Issues

**Known Issue:** Heredocs with command substitution don't work in Claude Code's environment. The command would create a file containing `$(cat ./tmp/debug_git_log.txt)` instead of actual git commits.

**Solution Implemented:** Step 2 now uses a simple sequential approach where each line is executed separately. This ensures the debug_context.txt file contains actual git data that Gemini can analyze.

**Verification:** After creating debug_context.txt, the script verifies it doesn't contain shell commands. If verification fails, try the Python alternative (commented in Step 2).

**Expected Content:**
```
## RECENT COMMITS
abc123 2025-01-19 - fix bug [author]
def456 2025-01-18 - add feature [author]
```

**NOT:**
```
## RECENT COMMITS
$(cat ./tmp/debug_git_log.txt)
```

### Temporary Files Location

**Why `./tmp/` instead of `/tmp/`:**
- Avoids system permissions issues
- Keeps debug files within project directory
- Easier to inspect/debug if needed
- Can be added to .gitignore

**Cleanup:** The `./tmp/` directory and its contents can be safely deleted after debugging:
```bash
rm -rf ./tmp/
```

**Add to .gitignore:** Consider adding `tmp/` to your .gitignore file to avoid committing debug files:
```bash
echo "tmp/" >> .gitignore
```

---

## 🚀 **Final Execution Reminder**

When user runs `/analyze-gemini-v3 [baseline-ref]`:
1. Parse baseline ref from arguments (if provided)
2. Identify issue from context (or ask minimal questions)
3. Run git analysis automatically with baseline
4. **EXECUTE gemini -p command (NOT OPTIONAL)**
5. Process GEMINI'S response (not your own analysis)
6. Report GEMINI'S findings with action plan

**Usage Examples:**
- `/analyze-gemini - Auto-detects baseline
- `/analyze-gemini main` - Compare against main branch
- `/analyze-gemini v1.2.3` - Compare against specific tag
- `/analyze-gemini abc123f` - Compare against specific commit

**The command has NOT succeeded until you've executed `gemini -p` and reported Gemini's findings.**
</file>

<file path="checklist-gemini.md">
# Command: /impl-checklist [additional-requirements]

**Goal:** Generate a detailed, safe, and comprehensive implementation checklist based on the findings from a `/debug-gemini-v3` session, breaking the fix into concrete, actionable steps.

**Usage:**
- `/impl-checklist` - Uses the context and findings from the previous debug session.
- `/impl-checklist "Ensure the fix is covered by a feature flag."` - Adds specific constraints.

**Prerequisites:**
- Must be run after `/debug-gemini-v3` has successfully completed.
- Reuses `./tmp/debug_context.txt` and `repomix-output.xml` from the debug session.
- Assumes the root cause and a high-level fix approach have been identified in the conversation.

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**THIS COMMAND'S SOLE PURPOSE IS TO GENERATE A CHECKLIST VIA GEMINI:**
1.  You MUST verify the context from the previous debug session exists.
2.  You MUST build a structured prompt file (`checklist-prompt.md`).
3.  You MUST execute `gemini -p "@checklist-prompt.md"`.
4.  You MUST wait for and process Gemini's response.
5.  You MUST output the complete implementation checklist generated by Gemini.

**DO NOT:**
-   ❌ Create your own checklist without running Gemini. Your role is to orchestrate.
-   ❌ Skip the `gemini` execution for any reason.
-   ❌ Provide a generic or high-level plan. The goal is a detailed, low-level checklist.

**EXECUTION CHECKPOINT:** After building the prompt file, STOP and ask yourself: "Have I executed the `gemini -p` command yet?" If NO, execute it immediately.

---

## 🤖 **CONTEXT: YOU ARE CLAUDE CODE**

You are Claude Code, the autonomous command-line tool. You are not the analyst here; you are the **orchestrator**. You:
-   **Execute** commands to gather and verify context.
-   **Build** structured prompts for other AI models.
-   **Delegate** the core analysis (checklist generation) to Gemini.
-   **Complete** the entire workflow autonomously without human intervention.

---

## 📋 **YOUR EXECUTION WORKFLOW**

### Step 1: Prepare Environment and Context

```bash
# Ensure tmp directory exists for our files
mkdir -p ./tmp

# Parse any additional requirements from arguments
ADDITIONAL_REQUIREMENTS="$ARGUMENTS"

# Identify the fix summary from the preceding conversation context.
# This is CRITICAL. You need to synthesize the root cause and fix
# approach identified by the /debug-gemini-v3 command's output.
# Example: "The root cause was nginx stripping auth headers on large requests.
# The fix is to increase buffer sizes and ensure headers are passed through."
FIX_SUMMARY="[Synthesize the fix summary from the conversation here]"
```

### Step 2: Verify and Gather Context Files

This command relies on the output of `/debug-gemini-v3`. Verify its artifacts exist.

```bash
# Check for essential context files from the previous run
if [ ! -f ./tmp/debug_context.txt ] || [ ! -f ./repomix-output.xml ]; then
    echo "❌ ERROR: Critical context from /debug-gemini-v3 is missing."
    echo "One or both of these files were not found:"
    echo "  - ./tmp/debug_context.txt (git analysis)"
    echo "  - ./repomix-output.xml (codebase context)"
    echo "Please run /debug-gemini-v3 successfully before running this command."
    exit 1
fi

# Optional: Check if context is stale (e.g., > 60 mins old)
if find ./repomix-output.xml -mmin +60 | grep -q .; then
    echo "⚠️ Warning: The project context is over 60 minutes old."
    echo "Consider running 'npx repomix@latest ...' again if the codebase has changed significantly."
fi

echo "✅ Found all necessary context from the previous debug session."
```

### Step 3: MANDATORY - Build the Prompt File

You will now build the prompt for Gemini in a file using the structured XML pattern.

#### Step 3.1: Create Base Prompt File
```bash
# Clean start for the prompt file
rm -f ./checklist-prompt.md 2>/dev/null

# Create the structured prompt with placeholders
# Note the use of XML tags for clarity
cat > ./checklist-prompt.md << 'PROMPT'
<task>
Generate a detailed, step-by-step implementation checklist to fix a bug. The checklist must be safe, thorough, and ready for a developer to execute.

Carry out the following steps:
<steps>
<1>
**Review Project Standards:** Thoroughly read the provided project documentation (`CLAUDE.md`, `DEVELOPER_GUIDE.md`) to understand coding conventions, testing standards, and deployment procedures. This is not optional.
</1>
<2>
**Analyze the Problem Context:**
- Review the `<fix_summary>` to understand the root cause and the high-level solution.
- Review the `<git_context>` to see the exact code changes that led to the bug.
- Review the `<codebase_context>` to understand the surrounding code and architecture.
</2>
<3>
**Generate the Checklist:** Based on all provided context, create the implementation checklist according to the specified `<output_format>` and `<guidelines>`.
</3>
</steps>

<context>
<!-- The following sections will be populated with actual data -->
<fix_summary>
[Placeholder for the fix summary]
</fix_summary>

<additional_requirements>
[Placeholder for additional requirements from user]
</additional_requirements>

<git_context>
<!-- Placeholder for content from debug_context.txt -->
</git_context>

<codebase_context>
<!-- Placeholder for content from repomix-output.xml -->
</codebase_context>
</context>

<guidelines>
## CHECKLIST REQUIREMENTS
- **Safety First:** Prioritize steps that prevent regressions. Include branching, backups, and verification.
- **Specificity:** Use exact file paths, line numbers, function names, and commands. Avoid vague instructions like "fix the code."
- **Atomicity:** Each checklist item should be a single, concrete, verifiable action.
- **Logical Order:** Sequence the steps logically (e.g., setup, code changes, tests, validation, cleanup).
- **Testability:** For every functional change, include a corresponding testing step (unit, integration, or manual).
- **Consider Side Effects:** Think about what else this change could impact. Add checks for potential downstream effects.
</guidelines>

<output format>
Provide the checklist in Markdown format.

- Use nested checkboxes for phases and steps.
- Use fenced code blocks (with language identifiers) for all code snippets.
- Use backticks for `file/paths` and `commands`.
- Use **bold** or `**CRITICAL**` for high-risk steps.
- Include estimated time and risk level at the top.

### Example Structure:
```markdown
## 🔧 Implementation Checklist: [A Clear Title for the Fix]

**Estimated Time:** [e.g., 2-3 hours]
**Risk Level:** [e.g., Low/Medium/High]

### Phase 1: Preparation & Safety (Est: 15 min)
- [ ] Create a new feature branch: `git checkout -b fix/short-name-for-bug`
- [ ] Run all existing tests for the affected module to confirm they pass before changes: `npm test src/affected-module/`

### Phase 2: Core Implementation (Est: 1.5 hours)
- [ ] **CRITICAL:** In `config/app.json`, update the `timeout` setting from `500` to `1500`.
- [ ] In `src/services/api-handler.js:45`, refactor the `handleRequest` function to use the new async utility:
  ```javascript
  // OLD CODE
  const response = oldSyncMethod(data);

  // NEW CODE
  const response = await newAsyncMethod(data);
  ```
- [ ] ... more specific steps ...

### Phase 3: Testing & Validation (Est: 45 min)
- [ ] Add a new unit test in `tests/unit/api-handler.test.js` to cover the timeout scenario.
- [ ] Manually test the end-to-end flow by running [specific command or UI action].
- [ ] Verify that the old, related functionality in [other module] has not regressed.

### Phase 4: Documentation & Cleanup (Est: 15 min)
- [ ] Add a comment to `src/services/api-handler.js:45` explaining why the change was made.
- [ ] Create a changelog entry under the "Fixes" section.
```
</output format>
</task>
PROMPT
```

#### Step 3.2: Append Dynamic Context to the Prompt File
```bash
# Now, inject the actual context into the prompt file we just created.
# This is a critical step that replaces the placeholders.

# Inject the fix summary and additional requirements
# Note: Using a temporary file to handle multi-line variables and special characters safely.
echo "$FIX_SUMMARY" > ./tmp/fix_summary.txt
echo "$ADDITIONAL_REQUIREMENTS" > ./tmp/additional_reqs.txt

sed -i.bak -e '/\[Placeholder for the fix summary\]/r ./tmp/fix_summary.txt' -e '//d' ./checklist-prompt.md
sed -i.bak -e '/\[Placeholder for additional requirements from user\]/r ./tmp/additional_reqs.txt' -e '//d' ./checklist-prompt.md

# Append the git context
echo -e "\n<git_context>" >> ./checklist-prompt.md
cat ./tmp/debug_context.txt >> ./checklist-prompt.md
echo -e "\n</git_context>" >> ./checklist-prompt.md

# Append the codebase context
echo -e "\n<codebase_context>" >> ./checklist-prompt.md
cat ./repomix-output.xml >> ./checklist-prompt.md
echo -e "\n</codebase_context>" >> ./checklist-prompt.md

# Verify the prompt file was created correctly
if [ ! -s ./checklist-prompt.md ]; then
    echo "❌ ERROR: Failed to build the prompt file ./checklist-prompt.md. Aborting."
    exit 1
fi
echo "✅ Successfully built structured prompt file: ./checklist-prompt.md"
```

### Step 4: MANDATORY - Execute Gemini Analysis

You MUST now execute Gemini using the single, clean, and verifiable prompt file.

```bash
# Execute Gemini with the fully-formed prompt file
gemini -p "@./checklist-prompt.md"
```

### Step 5: Process and Report Gemini's Findings

```bash
# [You will receive Gemini's markdown checklist as a response]

# Save the checklist for reference and present it to the user
# (Assuming Gemini's output is captured into a variable like $GEMINI_RESPONSE)
# For this example, we'll simulate saving and printing.

echo "✅ Implementation Checklist Generated by Gemini."
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
# echo "$GEMINI_RESPONSE" # In a real scenario, you'd print the variable
# For demonstration, we'll assume the response is now available to be displayed.
echo "## 🔧 Implementation Checklist: Fix Nginx Auth Header Stripping"
echo "**Risk Level:** High"
echo "### Phase 1: Preparation"
echo "- [ ] Create branch: \`git checkout -b fix/nginx-auth-header\`"
echo "..."
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo "A copy of the full checklist has been saved to ./tmp/implementation_checklist.md"
# echo "$GEMINI_RESPONSE" > ./tmp/implementation_checklist.md
```

---

## ✅ **VERIFICATION CHECKLIST**

Before reporting completion, verify you have performed these steps:
-   [ ] Verified `debug_context.txt` and `repomix-output.xml` exist.
-   [ ] Synthesized the `FIX_SUMMARY` from the conversation.
-   [ ] Successfully created `./checklist-prompt.md` with the XML structure.
-   [ ] Correctly appended all context (`fix_summary`, `git_context`, `codebase_context`) to the prompt file.
-   [ ] **I EXECUTED the `gemini -p "@./checklist-prompt.md"` command.** ← MANDATORY
-   [ ] I received Gemini's checklist response.
-   [ ] I presented the formatted checklist to the user.
</file>

<file path="complete-phase-gemini-full.md">
# Command: /complete-phase-gemini-full <initiative-path>

**Goal:** Autonomously verify the completion of the current project phase, report the verdict, and prepare for the next phase by delegating all analysis and decision-making to Gemini.

**Usage:**
- `/complete-phase-gemini-full plans/active/real-time-notifications`

**Prerequisites:**
- An `implementation.md` and `PROJECT_STATUS.md` must exist.
- The command should be run after a phase's checklist is believed to be complete.

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**YOUR ROLE IS AN AUTONOMOUS ORCHESTRATOR AND FILE MANAGER. YOU MAKE NO DECISIONS.**
1.  You MUST identify the current phase and its success criteria from the project files.
2.  You MUST run `repomix` to create a complete, fresh snapshot of the codebase, including the recent changes.
3.  You MUST build a structured prompt file (`tmp/verify-prompt.md`) using the XML format.
4.  You MUST execute `gemini -p "@tmp/verify-prompt.md"` to delegate the entire verification process.
5.  You MUST parse Gemini's verdict from the response.
6.  You MUST execute the correct file management actions (advance phase or create fix-list) based **only** on Gemini's verdict.

**DO NOT:**
-   ❌ Make any judgment calls on whether a phase is complete.
-   ❌ Modify, interpret, or enhance Gemini's analysis.
-   ❌ Skip any step.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### Step 1: Prepare Context from Project State

```bash
# Parse arguments
INITIATIVE_PATH="$1"
IMPLEMENTATION_PLAN_PATH="$INITIATIVE_PATH/implementation.md"
PROJECT_STATUS_PATH="PROJECT_STATUS.md"

# Verify required files exist
if [ ! -f "$IMPLEMENTATION_PLAN_PATH" ] || [ ! -f "$PROJECT_STATUS_PATH" ]; then
    echo "❌ ERROR: Required project files (implementation.md or PROJECT_STATUS.md) not found."
    exit 1
fi

# Extract current phase number and info
CURRENT_PHASE_NUMBER=$(grep 'Current Phase:' "$PROJECT_STATUS_PATH" | sed 's/Current Phase: Phase \([0-9]*\).*/\1/')
CURRENT_PHASE_INFO=$(awk "/### \*\*Phase $CURRENT_PHASE_NUMBER:/{f=1} f && /^### \*\*Phase|## 📊 PROGRESS TRACKING/{if (!/Phase $CURRENT_PHASE_NUMBER/) f=0} f" "$IMPLEMENTATION_PLAN_PATH")
CURRENT_PHASE_CHECKLIST_PATH="$INITIATIVE_PATH/phase_${CURRENT_PHASE_NUMBER}_checklist.md"
CURRENT_PHASE_CHECKLIST=$(cat "$CURRENT_PHASE_CHECKLIST_PATH")

if [ -z "$CURRENT_PHASE_NUMBER" ] || [ -z "$CURRENT_PHASE_INFO" ]; then
    echo "❌ ERROR: Could not determine current phase from project files."
    exit 1
fi
echo "✅ Loaded context for current Phase $CURRENT_PHASE_NUMBER."
```

### Step 2: Aggregate Codebase Context with Repomix

```bash
# Use repomix for a complete, single-file context snapshot.
npx repomix@latest . \
  --include "**/*.{js,py,md,sh,json,c,h,log,yml,toml}" \
  --ignore "build/**,node_modules/**,dist/**,*.lock,tmp/**"

if [ ! -s ./repomix-output.xml ]; then
    echo "❌ ERROR: Repomix failed to generate the codebase context. Aborting."
    exit 1
fi
echo "✅ Codebase context aggregated into repomix-output.xml."
```

### Step 3: MANDATORY - Build the Prompt File

```bash
# Clean start for the prompt file
mkdir -p ./tmp
rm -f ./tmp/verify-prompt.md 2>/dev/null

# Create the structured prompt
cat > ./tmp/verify-prompt.md << 'PROMPT'
<task>
You are an automated, rigorous Quality Assurance and Verification system. Your task is to perform a complete verification of a software development phase and determine if it is complete. You must be strict and objective.

<steps>
<1>
Analyze the provided context: `<phase_info>`, `<phase_checklist>`, and the full `<codebase_context>`.
</1>
<2>
Perform all verification checks as detailed in the `<output_format>` section. This includes implementation review, test analysis, and quality checks.
</2>
<3>
Execute the success test command defined in the `<phase_info>` and compare the actual output to the expected outcome.
</3>
<4>
Provide a definitive, overall verdict: `COMPLETE` or `INCOMPLETE`. This is the most critical part of your output.
</4>
<5>
If the verdict is `INCOMPLETE`, provide a list of all `BLOCKER` issues.
</5>
<6>
If the verdict is `COMPLETE`, provide a detailed preparation plan for the next phase.
</6>
</steps>

<context>
<phase_info>
[Placeholder for the current phase info from implementation.md]
</phase_info>

<phase_checklist>
[Placeholder for the current phase's checklist.md]
</phase_checklist>

<codebase_context>
<!-- Placeholder for content from repomix-output.xml -->
</codebase_context>
</context>

<output_format>
Your entire response must be a single Markdown block.
The most important line of your output MUST be `OVERALL_VERDICT: [COMPLETE|INCOMPLETE]`.
Do not include any conversational text before or after your analysis.

OVERALL_VERDICT: [COMPLETE|INCOMPLETE]

## 1. IMPLEMENTATION VERIFICATION
- **Checklist Completion:** [Analyze the checklist. Are all items marked as done? Report status.]
- **Code Review:** [Review the code changes relevant to this phase. Do they correctly implement the goals? Are there any obvious bugs, anti-patterns, or style violations?]
- **Deliverable Check:** [Does the deliverable specified in the phase_info exist and is it correct?]

## 2. TEST VERIFICATION
- **Test Execution:** [Simulate running the `Success Test` command from the phase_info. Report the expected outcome.]
- **Test Coverage:** [Analyze the tests. Do they adequately cover the new or modified code?]
- **Regression Check:** [Does the implementation introduce any obvious regressions or break existing functionality described in the codebase context?]

## 3. QUALITY & DOCUMENTATION
- **Code Quality:** [Assess the quality of the new code. Is it clean, readable, and maintainable?]
- **Documentation:** [Are docstrings, comments, and any relevant external documentation updated?]

## 4. BLOCKERS (if INCOMPLETE)
- **Blocker 1:** [A specific, actionable reason why the phase is not complete.]
- **Blocker 2:** [Another specific, actionable reason.]

## 5. NEXT PHASE PREPARATION (if COMPLETE)
- **Next Phase Goal:** [Based on the implementation plan, what is the goal of the next phase?]
- **Key Modules for Next Phase:** [Identify files/modules that will be important for the next phase.]
- **Potential Challenges:** [Anticipate any risks or challenges for the next phase.]

## 6. GEMINI VERIFICATION SUMMARY
- **Overall Assessment:** [A brief summary of your findings.]
- **Confidence Score:** [HIGH | MEDIUM | LOW]

END OF VERIFICATION
</output_format>
</task>
PROMPT

# Inject the dynamic context
echo "$CURRENT_PHASE_INFO" > ./tmp/phase_info.txt
echo "$CURRENT_PHASE_CHECKLIST" > ./tmp/phase_checklist.txt
sed -i.bak -e '/\[Placeholder for the current phase info from implementation.md\]/r ./tmp/phase_info.txt' -e '//d' ./tmp/verify-prompt.md
sed -i.bak -e '/\[Placeholder for the current phase.s checklist.md\]/r ./tmp/phase_checklist.txt' -e '//d' ./tmp/verify-prompt.md

echo -e "\n<codebase_context>" >> ./tmp/verify-prompt.md
cat ./repomix-output.xml >> ./tmp/verify-prompt.md
echo -e "\n</codebase_context>" >> ./tmp/verify-prompt.md

echo "✅ Successfully built structured prompt file: ./tmp/verify-prompt.md"
```

### Step 4: MANDATORY - Execute Gemini Verification

```bash
# Execute Gemini with the fully-formed prompt file
GEMINI_RESPONSE=$(gemini -p "@./tmp/verify-prompt.md")
```

### Step 5: Process Gemini's Verdict and Manage Files

```bash
# Parse the verdict from the first line of the response.
VERDICT=$(echo "$GEMINI_RESPONSE" | grep '^OVERALL_VERDICT: ' | sed 's/^OVERALL_VERDICT: //')
REPORT_CONTENT=$(echo "$GEMINI_RESPONSE" | sed '1d') # Get the rest of the content

if [ "$VERDICT" == "COMPLETE" ]; then
    echo "✅ Phase $CURRENT_PHASE_NUMBER VERIFIED COMPLETE by Gemini."
    echo "$REPORT_CONTENT" # Display the full successful report

    # Update implementation.md and PROJECT_STATUS.md
    echo "Updating project tracking files..."
    # (Logic to mark phase complete and update status would go here)

    # Check if this was the final phase
    # If final, archive the project.
    # If not final, announce next step.
    echo "Next step: Run \`/phase-checklist-gemini-full $((CURRENT_PHASE_NUMBER + 1)) $INITIATIVE_PATH\` to generate the detailed checklist for the next phase."

elif [ "$VERDICT" == "INCOMPLETE" ]; then
    echo "❌ Phase $CURRENT_PHASE_NUMBER verification FAILED."
    
    # Extract and save the list of blockers
    BLOCKERS=$(echo "$REPORT_CONTENT" | awk '/## 4. BLOCKERS/,/## 5. NEXT PHASE PREPARATION/' | grep 'Blocker')
    FIX_LIST_PATH="$INITIATIVE_PATH/phase_${CURRENT_PHASE_NUMBER}_fixes.md"
    echo "# Phase $CURRENT_PHASE_NUMBER Fix-List (Blockers Only)" > "$FIX_LIST_PATH"
    echo "Generated on $(date)" >> "$FIX_LIST_PATH"
    echo "$BLOCKERS" >> "$FIX_LIST_PATH"

    echo "Gemini found BLOCKERS that must be fixed:"
    echo "$BLOCKERS"
    echo ""
    echo "A detailed fix-list has been saved to: $FIX_LIST_PATH"
    echo "After fixing all blockers, run this command again to re-verify."

else
    echo "❌ ERROR: Could not determine phase verdict from Gemini's output."
    echo "--- Gemini's Raw Output ---"
    echo "$GEMINI_RESPONSE"
    exit 1
fi
```
</file>

<file path="complete-phase.md">
# Command: /complete-phase

**Goal:** Manage the end-of-phase transition using a formal review cycle and a hardened, intent-driven Git commit process. This command operates in two distinct modes, determined by the presence of a review file.

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**You MUST operate in one of two modes. You are not allowed to mix them.**

**Mode 1: Request Review (Default)**
*   **Trigger:** No `review_phase_N.md` file exists for the current phase.
*   **Action:** You MUST execute the **Review Request Generation Protocol** below and then HALT.

**Mode 2: Process Review**
*   **Trigger:** A `review_phase_N.md` file EXISTS for the current phase.
*   **Action:** You MUST read the review, parse the `VERDICT`, and then either commit the changes (on `ACCEPT`) using the **Safe Staging and Commit Protocol** or report the required fixes (on `REJECT`).

**DO NOT:**
-   ❌ Commit any code without a `VERDICT: ACCEPT` from a review file.
-   ❌ Use `git add -A` or `git add .`. You must use the explicit, plan-driven staging logic.
-   ❌ Guess or perform "detective work." If the state of the repository does not match the plan, you must halt and report the specific discrepancy.

---

## 📋 **YOUR EXECUTION WORKFLOW**

### Step 1: Determine Current Mode
-   Read `PROJECT_STATUS.md` to get the current initiative path and phase number (`N`).
-   Check if the file `<path>/review_phase_N.md` exists.
-   If it exists, proceed to **Mode 2: Process Review**.
-   If it does not exist, proceed to **Mode 1: Request Review**.

---

### **MODE 1: REQUEST REVIEW**

#### **Review Request Generation Protocol**
You will now execute the following shell script block in its entirety. It contains all the logic for identifying files, staging new files for review, generating the diff, and creating the final review request file.

```bash
# --- START OF PROTOCOL ---

# 1. SETUP: Define paths and ensure a clean state.
mkdir -p ./tmp
INITIATIVE_PATH=$(grep 'Path:' PROJECT_STATUS.md | awk '{print $2}' | tr -d '`')
PHASE_NUM=$(grep 'Current Phase:' PROJECT_STATUS.md | sed 's/.*Phase \([0-9]*\).*/\1/')
IMPL_FILE="$INITIATIVE_PATH/implementation.md"
CHECKLIST_FILE="$INITIATIVE_PATH/phase_${PHASE_NUM}_checklist.md"
echo "INFO: Preparing review request for Phase $PHASE_NUM of initiative at '$INITIATIVE_PATH'."

# 2. PARSE THE PLAN: Identify all files intended for this phase.
#    This Python script is a robust way to parse the checklist.
intended_files_str=$(python -c "
import re, sys
try:
    with open('$CHECKLIST_FILE', 'r') as f: content = f.read()
    files = re.findall(r'\`([a-zA-Z0-9/._-]+)\`', content)
    valid_files = sorted(list({f for f in files if '/' in f and '.' in f}))
    print(' '.join(valid_files))
except FileNotFoundError:
    print(f\"ERROR: Checklist file not found at '$CHECKLIST_FILE'\", file=sys.stderr)
    sys.exit(1)
")

if [ -z "$intended_files_str" ]; then
    echo "❌ ERROR: Could not parse any intended file paths from '$CHECKLIST_FILE'. Halting."
    exit 1
fi
echo "INFO: Plan indicates the following files should be modified:"
echo "$intended_files_str" | tr ' ' '\n' | sed 's/^/ - /'

# 3. VERIFY STATE: Check that all intended files are actually present in git status.
all_changed_files=$(git status --porcelain | awk '{print $2}')
for intended_file in $intended_files_str; do
    if ! echo "$all_changed_files" | grep -q "^${intended_file}$"; then
        echo "❌ ERROR: A planned file is missing from git's changed files list: $intended_file"
        echo "Please ensure the file was created/modified as per the checklist. Halting."
        exit 1
    fi
done
echo "✅ INFO: All planned files are present in git status."

# 4. STAGE NEW FILES FOR REVIEW: Add only the untracked files that were part of the plan.
untracked_files=$(git status --porcelain | grep '^??' | awk '{print $2}')
for file in $untracked_files; do
    if echo "$intended_files_str" | grep -q "\b$file\b"; then
        echo "INFO: Staging new file for review diff: $file"
        git add "$file"
    fi
done

# 5. GENERATE DIFF: Create a comprehensive diff including staged new files.
diff_base=$(grep 'Last Phase Commit Hash:' "$IMPL_FILE" | awk '{print $4}')
# Use --staged to include newly added files, and also diff against HEAD for modified but not staged files.
git diff --staged "$diff_base" -- . ':(exclude)*.ipynb' > ./tmp/phase_diff.txt
git diff HEAD -- . ':(exclude)*.ipynb' >> ./tmp/phase_diff.txt
echo "INFO: Comprehensive diff generated."

# 6. GENERATE REVIEW FILE: Programmatically build the review request.
PHASE_NAME=$(awk -F': ' "/### \\*\\*Phase $PHASE_NUM:/{print \$2}" "$IMPL_FILE" | head -n 1)
INITIATIVE_NAME=$(grep 'Name:' PROJECT_STATUS.md | sed 's/Name: //')
REVIEW_FILE="$INITIATIVE_PATH/review_request_phase_$PHASE_NUM.md"
PLAN_FILE="$INITIATIVE_PATH/plan.md"
DIFF_FILE="./tmp/phase_diff.txt"

# Create the header
{
    echo "# Review Request: Phase $PHASE_NUM - $PHASE_NAME"
    echo ""
    echo "**Initiative:** $INITIATIVE_NAME"
    echo "**Generated:** $(date '+%Y-%m-%d %H:%M:%S')"
    echo ""
    echo "## Instructions for Reviewer"
    echo "1.  Analyze the planning documents and the code changes (\`git diff\`) below."
    echo "2.  Create a new file named \`review_phase_${PHASE_NUM}.md\` in this same directory (\`$INITIATIVE_PATH/\`)."
    echo "3.  In your review file, you **MUST** provide a clear verdict on a single line: \`VERDICT: ACCEPT\` or \`VERDICT: REJECT\`."
    echo "4.  If rejecting, you **MUST** provide a list of specific, actionable fixes under a \"Required Fixes\" heading."
    echo ""
    echo "---"
} > "$REVIEW_FILE"

# Append planning documents
{
    echo "## 1. Planning Documents"
    echo ""
    echo "### R&D Plan (\`plan.md\`)"
    echo '```markdown'
    cat "$PLAN_FILE"
    echo '```'
    echo ""
    echo "### Implementation Plan (\`implementation.md\`)"
    echo '```markdown'
    cat "$IMPL_FILE"
    echo '```'
    echo ""
    echo "### Phase Checklist (\`phase_${PHASE_NUM}_checklist.md\`)"
    echo '```markdown'
    cat "$CHECKLIST_FILE"
    echo '```'
    echo ""
} >> "$REVIEW_FILE"

# Append the git diff
{
    echo "---"
    echo "## 2. Code Changes for This Phase"
    echo ""
    echo "**Baseline Commit:** $diff_base"
    echo "**Current Branch:** $(git rev-parse --abbrev-ref HEAD)"
    echo ""
    echo '```diff'
    cat "$DIFF_FILE"
    echo '```'
} >> "$REVIEW_FILE"

echo "✅ Review request file generated programmatically at $REVIEW_FILE"

# 7. UNSTAGE FILES: Reset the index to leave the repository clean for the user.
echo "INFO: Unstaging new files. They will be re-staged during the commit process after review."
git reset > /dev/null

# --- END OF PROTOCOL ---
```

#### **Final Step: Notify and Halt**
-   Inform the user that the review request is ready at `<path>/review_request_phase_N.md`.
-   **HALT.** Your task for this run is complete.

---

### **MODE 2: PROCESS REVIEW**

#### Step 2.1: Read and Parse Review File
-   Read the file `<path>/review_phase_N.md`.
-   Find the line starting with `VERDICT:`. Extract the verdict (`ACCEPT` or `REJECT`).
-   If no valid verdict is found, report an error and stop.

#### Step 2.2: 🔴 MANDATORY - Conditional Execution (On `ACCEPT`)
-   If `VERDICT: ACCEPT`, you MUST execute the **Safe Staging and Commit Protocol** below.

#### Step 2.3: Conditional Execution (On `REJECT`)
-   If `VERDICT: REJECT`, extract all lines from the "Required Fixes" section of the review file.
-   Present these fixes clearly to the user.
-   **HALT.** Make no changes to Git or status files.

---

## 🔒 **Safe Staging and Commit Protocol (For `ACCEPT` Verdict)**

You must execute this precise sequence of commands.

```bash
# 1. Re-Identify Intended Files from the Checklist for Verification
INITIATIVE_PATH=$(grep 'Path:' PROJECT_STATUS.md | awk '{print $2}' | tr -d '`')
PHASE_NUM=$(grep 'Current Phase:' PROJECT_STATUS.md | sed 's/.*Phase \([0-9]*\).*/\1/')
CHECKLIST_FILE="$INITIATIVE_PATH/phase_${PHASE_NUM}_checklist.md"
IMPL_FILE="$INITIATIVE_PATH/implementation.md"

intended_files_str=$(python -c "
import re, sys
try:
    with open('$CHECKLIST_FILE', 'r') as f: content = f.read()
    files = re.findall(r'\`([a-zA-Z0-9/._-]+)\`', content)
    valid_files = sorted(list({f for f in files if '/' in f and '.' in f}))
    print(' '.join(valid_files))
except FileNotFoundError:
    sys.exit(1)
")
read -r -a intended_files <<< "$intended_files_str"
echo "Verifying staged files against the plan:"
printf " - %s\n" "${intended_files[@]}"

# 2. Get a list of ALL changed files (staged, modified, untracked)
all_changed_files=$(git status --porcelain | awk '{print $2}')

# 3. Explicitly Stage ALL Intended Files (Handles both new and modified)
echo "Staging all intended and modified files for commit:"
staged_count=0
for file in $all_changed_files; do
    is_intended=false
    for intended_file in "${intended_files[@]}"; do
        if [[ "$file" == "$intended_file" ]]; then
            is_intended=true
            break
        fi
    done

    if $is_intended; then
        echo "- Staging $file"
        git add "$file"
        staged_count=$((staged_count + 1))
    fi
done

if [ $staged_count -eq 0 ]; then
    echo "⚠️ WARNING: No files were staged. This might mean the changes were already committed or the checklist file paths are incorrect."
fi

# 4. HALT on Unplanned Changes
#    Check for any remaining unstaged or untracked files, ignoring already staged files.
unintended_changes=$(git status --porcelain | grep -v '^A ')
if [ -n "$unintended_changes" ]; then
    echo "❌ ERROR: Unplanned changes detected. The following files were modified or created but were not part of the phase plan:"
    echo "$unintended_changes"
    echo "Please review these files. Either add them to the phase checklist or revert them before committing."
    exit 1
fi

# 5. Commit the Staged Changes
echo "Committing staged changes..."
phase_deliverable=$(awk -F': ' "/^**Deliverable**/{print \$2}" "$IMPL_FILE" | head -n 1)
git commit -m "feat: Phase $PHASE_NUM - $phase_deliverable"

# 6. Verify the commit was successful and capture the new hash
if [ $? -ne 0 ]; then
    echo "❌ ERROR: Git commit failed. Halting."
    exit 1
fi
new_hash=$(git rev-parse HEAD)
echo "New commit hash is: $new_hash"

# 7. Proceed with State Updates
#    (Update implementation.md, PROJECT_STATUS.md, etc.)
#    This part would be another set of sed/awk commands to update the status files.
```
</file>

<file path="customplan-gemini-full.md">
# Command: /customplan-gemini-full <objective> [path/to/requirements.md]

**Goal:** Autonomously generate a complete, code-aware R&D plan by delegating the analysis and authoring to Gemini, then saving the resulting artifacts to the project structure.

**Usage:**
- `/customplan-gemini-full "Implement a real-time notification system using WebSockets"`
- `/customplan-gemini-full "Refactor the auth module to support OAuth2" initiatives/auth-refactor/reqs.md`

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**YOUR ROLE IS AN AUTONOMOUS ORCHESTRATOR AND FILE MANAGER.**
1.  You MUST gather user requirements from the command arguments.
2.  You MUST run `repomix` to create a complete, fresh snapshot of the codebase context.
3.  You MUST build a structured prompt file (`plan-prompt.md`) using the XML format.
4.  You MUST execute `gemini -p "@plan-prompt.md"` to delegate the plan generation.
5.  You MUST parse Gemini's response to get the initiative name.
6.  You MUST save the plan and update `PROJECT_STATUS.md` exactly as specified.

**DO NOT:**
-   ❌ Modify, interpret, or enhance Gemini's output in any way.
-   ❌ Create the plan yourself. Your job is to run the process.
-   ❌ Skip any step. The workflow is non-negotiable.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### Step 1: Prepare Context from User Requirements

Parse arguments and gather all user-provided requirements.

```bash
# Parse arguments
OBJECTIVE="$1"
REQUIREMENTS_FILE="$2"
USER_REQUIREMENTS_CONTENT=""

# Combine objective and file content into a single requirements block
if [ -n "$OBJECTIVE" ]; then
    USER_REQUIREMENTS_CONTENT="OBJECTIVE: $OBJECTIVE\n"
fi

if [ -f "$REQUIREMENTS_FILE" ]; then
    USER_REQUIREMENTS_CONTENT+="\nADDITIONAL REQUIREMENTS FROM FILE ($REQUIREMENTS_FILE):\n"
    USER_REQUIREMENTS_CONTENT+=$(cat "$REQUIREMENTS_FILE")
    echo "✅ Loaded additional requirements from '$REQUIREMENTS_FILE'."
elif [ -n "$REQUIREMENTS_FILE" ]; then
    echo "⚠️ Warning: Requirements file '$REQUIREMENTS_FILE' not found. Proceeding with objective only."
fi

if [ -z "$USER_REQUIREMENTS_CONTENT" ]; then
    echo "❌ ERROR: No objective or requirements provided. Please specify an objective."
    exit 1
fi

echo "✅ User requirements collected."
```

### Step 2: Aggregate Codebase Context with Repomix

Create a comprehensive and reliable context snapshot of the entire project for Gemini.

```bash
# Use repomix for a complete, single-file context snapshot.
npx repomix@latest . \
  --include "**/*.{js,py,md,sh,json,c,h,log,yml,toml}" \
  --ignore "build/**,node_modules/**,dist/**,*.lock"

# Verify that the context was created successfully.
if [ ! -s ./repomix-output.xml ]; then
    echo "❌ ERROR: Repomix failed to generate the codebase context. Aborting."
    exit 1
fi

echo "✅ Codebase context aggregated into repomix-output.xml."
```

### Step 3: MANDATORY - Build the Prompt File

You will now build the prompt for Gemini in a file using the structured XML pattern.

#### Step 3.1: Create Base Prompt File
```bash
# Clean start for the prompt file
rm -f ./plan-prompt.md 2>/dev/null

# Create the structured prompt with placeholders using the v3.0 XML pattern
cat > ./plan-prompt.md << 'PROMPT'
<task>
You are an expert Staff Engineer tasked with creating a comprehensive R&D plan. Your plan must be deeply informed by an analysis of the provided codebase.

<steps>
<1>
Analyze the `<user_requirements>` to fully understand the project's goals, problems, and constraints.
</1>
<2>
Thoroughly analyze the entire `<codebase_context>` to identify relevant modules, existing patterns, potential risks, and implementation details.
</2>
<3>
Generate the complete R&D plan. The plan must strictly adhere to the format specified in `<output_format>`. All sections must be filled out based on your analysis.
</3>
</steps>

<context>
<user_requirements>
[Placeholder for the user's requirements]
</user_requirements>

<codebase_context>
<!-- Placeholder for content from repomix-output.xml -->
</codebase_context>
</context>

<output_format>
Your entire response must be a single Markdown block containing the plan.
The first line of your output MUST be `INITIATIVE_NAME: [kebab-case-name-you-generate]`.
Do not include any other text, conversation, or summaries before or after the plan.

INITIATIVE_NAME: [Generate a kebab-case-name for the project]

# R&D Plan: [Title Case Version of Name]

*Created: $(date +%Y-%m-%d)*
*Generated By: Gemini Full Analysis*

## 🎯 **OBJECTIVE & HYPOTHESIS**
...
[The entire detailed Markdown template from the original prompt goes here, verbatim.]
...
## 🔗 **IMPLEMENTATION HINTS**
...
END OF PLAN
</output_format>
</task>
PROMPT
```

#### Step 3.2: Append Dynamic Context to the Prompt File
```bash
# Inject the user requirements and the repomix context into the prompt file.
# Using a temp file for requirements handles multi-line variables and special characters safely.
echo -e "$USER_REQUIREMENTS_CONTENT" > ./tmp/user_reqs.txt
sed -i.bak -e '/\[Placeholder for the user.s requirements\]/r ./tmp/user_reqs.txt' -e '//d' ./plan-prompt.md

# Append the codebase context
echo -e "\n<codebase_context>" >> ./plan-prompt.md
cat ./repomix-output.xml >> ./plan-prompt.md
echo -e "\n</codebase_context>" >> ./plan-prompt.md

echo "✅ Successfully built structured prompt file: ./plan-prompt.md"
```

### Step 4: MANDATORY - Execute Gemini Analysis

You MUST now execute Gemini using the single, clean, and verifiable prompt file.

```bash
# Execute Gemini with the fully-formed prompt file
gemini -p "@./plan-prompt.md"
```

### Step 5: Save Plan and Update Project Status

Your final role: receive the output from your command, parse it, and save the artifacts without modification.

```bash
# [You will receive Gemini's plan as a response from the command above]
# For this example, we'll assume the response is captured into $GEMINI_RESPONSE.

# Parse the initiative name from the first line of the response. This is reliable.
INITIATIVE_NAME=$(echo "$GEMINI_RESPONSE" | grep '^INITIATIVE_NAME: ' | sed 's/^INITIATIVE_NAME: //')
PLAN_CONTENT=$(echo "$GEMINI_RESPONSE" | sed '1d') # Get the rest of the content

if [ -z "$INITIATIVE_NAME" ]; then
    echo "❌ ERROR: Gemini's output did not include the required 'INITIATIVE_NAME:' line. Cannot proceed."
    echo "--- Gemini's Raw Output ---"
    echo "$GEMINI_RESPONSE"
    exit 1
fi

# Create the directory and save the plan
PLAN_DIR="plans/active/$INITIATIVE_NAME"
mkdir -p "$PLAN_DIR"
echo "$PLAN_CONTENT" > "$PLAN_DIR/plan.md"
echo "✅ Saved Gemini's R&D plan to: $PLAN_DIR/plan.md"

# Update PROJECT_STATUS.md
# This is a complex update; a script or more robust tool might be better in production,
# but for this command, we can use a template and sed/awk.
# For simplicity, we'll create a new status block.
STATUS_BLOCK="## 📍 Current Active Initiative\n\n**Name:** $INITIATIVE_NAME\n**Path:** \`$PLAN_DIR/\`\n**Started:** $(date +%Y-%m-%d)\n**Created By:** Gemini Full Analysis\n**Current Phase:** Planning"
# A real implementation would replace an existing block. Here we just show the content.
echo -e "\nUpdating PROJECT_STATUS.md..."
# (Logic to update PROJECT_STATUS.md would go here)
echo "✅ Updated PROJECT_STATUS.md"

# Announce completion to the user
echo ""
echo "Next step: Review '$PLAN_DIR/plan.md', then run \`/implementation-gemini-full\` to have Gemini create the implementation plan."
```

---

## ✅ **VERIFICATION CHECKLIST**

Before reporting completion, verify you have performed these steps:
-   [ ] Parsed user requirements from arguments.
-   [ ] Successfully ran `repomix` to generate `repomix-output.xml`.
-   [ ] Created `./plan-prompt.md` with the correct XML structure.
-   [ ] Injected all dynamic context (`user_requirements`, `codebase_context`) into the prompt file.
-   [ ] **I EXECUTED the `gemini -p "@./plan-prompt.md"` command.** ← MANDATORY
-   [ ] I received Gemini's plan response.
-   [ ] I successfully parsed the `INITIATIVE_NAME` from the response.
-   [ ] I saved the plan to the correct directory.
-   [ ] I updated `PROJECT_STATUS.md`.
</file>

<file path="customplan.md">
### **File: `.claude/commands/customplan.md` (Revised and Hardened)**

```markdown
# Command: /customplan [initiative-description]

**Goal:** Generate and save a focused R&D plan document for the next development cycle, including the mandatory setup of a Git feature branch.

**Usage:**
- `/customplan Add multi-trial statistics to generalization study`

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**THIS COMMAND MUST FOLLOW THIS EXACT SEQUENCE:**
1.  You MUST engage the user to get a clear objective.
2.  You MUST generate a `kebab-case` initiative name.
3.  You MUST capture the baseline branch name (`main`, `master`, etc.) *before* creating a new branch.
4.  You MUST create and check out a new feature branch named `feature/<initiative-name>`.
5.  You MUST then generate the `plan.md` document.
6.  You MUST update `PROJECT_STATUS.md` with the new initiative and branch information.

**DO NOT:**
-   ❌ Generate any planning documents before the Git branch has been successfully created.
-   ❌ Proceed if any Git command fails. Report the error and stop.
-   ❌ Forget to update `PROJECT_STATUS.md`.

**EXECUTION CHECKPOINT:** Before generating the `plan.md` file, you must verify that you are on a new feature branch.

---

## 🤖 **CONTEXT: YOU ARE CLAUDE CODE**

You are Claude Code, an autonomous command-line tool. You will execute the Git commands and file operations described below directly and without human intervention.

---

## 📋 **YOUR EXECUTION WORKFLOW**

### Step 1: Understand Objective & Name Initiative
-   Engage with the user based on their initial prompt (`$ARGUMENTS`) to clarify the objective.
-   Generate a concise, descriptive, `kebab-case` name for the initiative.

### Step 2: Perform Git Operations
-   Execute the following shell commands to set up the development branch.

```bash
# 1. Generate the initiative name (e.g., from user input)
#    Example: initiative_name="multi-trial-statistics"
initiative_name="<generated-kebab-case-name>"

# 2. Get the current branch name as the baseline reference.
#    This is the branch you will diff against later.
ref_branch=$(git rev-parse --abbrev-ref HEAD)
echo "Baseline branch identified as: $ref_branch"

# 3. Create and check out the new feature branch.
feature_branch="feature/$initiative_name"
git checkout -b "$feature_branch"

# 4. Verify successful branch creation.
current_branch=$(git rev-parse --abbrev-ref HEAD)
if [ "$current_branch" = "$feature_branch" ]; then
    echo "✅ Successfully created and checked out new branch: $feature_branch"
else
    echo "❌ ERROR: Failed to create or switch to the new feature branch."
    exit 1
fi
```

### Step 3: Generate and Save Planning Documents
-   Create the directory: `plans/active/<initiative-name>/`.
-   Generate the content for `plan.md` using the "R&D PLAN TEMPLATE" below.
-   Save the content to `plans/active/<initiative-name>/plan.md`.

### Step 4: Update Project Status
-   Update the `PROJECT_STATUS.md` file at the project root using the "PROJECT STATUS UPDATE TEMPLATE" below, making sure to include the new `Branch` field.

### Step 5: Confirm and Present
-   Announce all actions taken (branch creation, file generation, status update).
-   Present the full content of the generated `plan.md` for the user's review.

---

## 템플릿 & 가이드라인 (Templates & Guidelines)

### **R&D PLAN TEMPLATE**
*This template is for the content of `plan.md`.*
```markdown
# R&D Plan: <Initiative Name in Title Case>

*Created: <Today's date in YYYY-MM-DD format>*

## 🎯 **OBJECTIVE & HYPOTHESIS**
... (template content remains the same as previous version) ...
---

## ✅ **VALIDATION & VERIFICATION PLAN**
... (template content remains the same as previous version) ...
---

## 📁 **File Organization**

**Initiative Path:** `plans/active/<initiative-name>/`

**Next Step:** Run `/implementation` to generate the phased implementation plan.
```

### **PROJECT STATUS UPDATE TEMPLATE**
*This is the content to add/update in `PROJECT_STATUS.md`.*
```markdown
## 📍 Current Active Initiative

**Name:** <Initiative Name in Title Case>
**Path:** `plans/active/<initiative-name>/`
**Branch:** `feature/<initiative-name>` (baseline: <ref_branch>)
**Started:** <Today's date YYYY-MM-DD>
**Current Phase:** Planning
**Progress:** ░░░░░░░░░░░░░░░░ 0%
**Next Milestone:** Generate implementation plan
**R&D Plan:** `plans/active/<initiative-name>/plan.md`
**Implementation Plan:** *To be created*
```

---

## 📊 **SAMPLE INTERACTION**

### ✅ Correct Execution:
```
User: /customplan Add multi-trial statistics to generalization study

You: "Objective received: 'Add multi-trial statistics to generalization study'.
       I will create a new initiative named 'multi-trial-statistics'.
       First, I will set up the feature branch."

     [You execute the following commands autonomously]
     $ initiative_name="multi-trial-statistics"
     $ ref_branch=$(git rev-parse --abbrev-ref HEAD)
     $ echo "Baseline branch identified as: $ref_branch"
     $ feature_branch="feature/$initiative_name"
     $ git checkout -b "$feature_branch"
     $ # ... verification command ...

You: "✅ Git branch 'feature/multi-trial-statistics' has been created from baseline 'main'.
       Now generating the R&D plan and updating the project status..."

     [You create the directory, generate plan.md, and update PROJECT_STATUS.md]

You: "✅ Initiative 'Multi-Trial Statistics' has been created.
       - **R&D Plan saved to:** `plans/active/multi-trial-statistics/plan.md`
       - **Project status has been updated.**
       - **All new work should be committed to the `feature/multi-trial-statistics` branch.**

       Here is the R&D plan for your review:
       ---
       # R&D Plan: Multi-Trial Statistics
       *Created: 2025-07-20*
       ... (full plan content) ..."
```

### ❌ What NOT to do:
```
User: /customplan Add multi-trial statistics

You: "Okay, here is the plan for that initiative:"
     [Generates plan.md without creating a branch first]
     ❌ FAILED - You did not perform the mandatory Git operations. The initiative is not properly isolated.
```
```
</file>

<file path="debug-gemini-v2.md">
# Command: /debug-gemini-v3

**Goal:** Get comprehensive debugging help from Gemini with fresh perspective, especially when Claude Code might have tunnel vision about the root cause.

---

## 🤖 **CRITICAL CONTEXT: THIS IS FOR CLAUDE CODE**

This command is for **Claude Code** - the autonomous command-line tool that executes shell commands directly. Claude Code is NOT Claude AI. Claude Code:
- **Executes** commands autonomously (doesn't just suggest them)
- **Runs** the entire workflow without human intervention
- **Completes** debugging from start to finish in one go

## 🚀 **ZERO MANUAL STEPS REQUIRED**

When a user runs `/debug-gemini`, Claude Code handles EVERYTHING:
- ✅ Runs all git commands automatically
- ✅ Creates all temp files automatically  
- ✅ Executes the `gemini -p` command directly
- ✅ Processes Gemini's response automatically
- ✅ Reports findings and action plan
- ❌ NO copying commands
- ❌ NO pasting outputs
- ❌ NO manual execution by the user

---

## 🎯 **The Complete Autonomous Flow**

```mermaid
graph LR
    A[User: /debug-gemini] --> B[Claude Code: Identifies issue from context]
    B --> C[Claude Code: Runs git analysis automatically]
    C --> D[Claude Code: Creates temp files]
    D --> E[Claude Code: Executes gemini -p command]
    E --> F[Claude Code: Analyzes Gemini output]
    F --> G[Claude Code: Reports findings & action plan]
```

**Total user interaction: ONE command**

---

## 📋 **EXECUTION WORKFLOW FOR CLAUDE CODE**

### Step 1: Assess Current Context

Claude Code determines from the conversation/context:
- What's the bug/issue
- What's been tried already
- Current working theory
- Whether there's a baseline branch where it worked

**Only ask the user if:**
- The issue is unclear from context
- A baseline branch name is needed
- Critical details are missing

### Step 2: Run Debug Analysis (Fully Automated)

Claude Code automatically executes these commands without any user intervention:

```bash
# Get recent commit history
git log -n 10 --pretty=format:"%h %ad | %s [%an]" --date=short > /tmp/debug_git_log.txt

# If baseline branch provided, get the diff
git diff <baseline-branch>..HEAD --stat > /tmp/debug_diff_stat.txt
git diff <baseline-branch>..HEAD --name-status > /tmp/debug_diff_names.txt

# Get detailed changes in key areas
git diff <baseline-branch>..HEAD -- ptycho/ src/ configs/ package.json requirements.txt > /tmp/debug_diff_details.txt

# Combine all debug info into one file
cat > /tmp/debug_context.txt << 'EOF'
## RECENT COMMITS
$(cat /tmp/debug_git_log.txt)

## BASELINE DIFF STATISTICS
$(cat /tmp/debug_diff_stat.txt)

## FILES CHANGED
$(cat /tmp/debug_diff_names.txt)

## DETAILED CHANGES
$(cat /tmp/debug_diff_details.txt | head -500)
EOF
```

### Step 3: Execute Gemini Analysis (Fully Automated)

Claude Code then generates and **executes** this `gemini -p` command directly:

```bash
gemini -p "@ptycho/ @src/ @tests/ @docs/ @configs/ @logs/ @.github/ @scripts/ @benchmarks/ @examples/ @/tmp/debug_context.txt Debug this issue with FRESH EYES:

## ISSUE SUMMARY
**Symptoms:** [Detailed symptoms with specific errors, stack traces, or behaviors]
**When It Happens:** [Specific conditions, inputs, or sequences that trigger it]
**When It Doesn't Happen:** [Cases where it works fine]
**Environment:** [Dev/staging/prod, OS, versions]
**Baseline Branch:** [If provided - where it last worked correctly]

## GIT CONTEXT
All git history and diffs are in the included debug_context.txt file.
Pay special attention to:
- Recent commits that might have introduced the issue
- Changes between baseline and current branch
- Modified configuration files
- Dependency updates

## CLAUDE CODE'S CURRENT UNDERSTANDING
**My Leading Theory:** [What Claude Code thinks is wrong]
**Evidence For This Theory:** 
- [Specific observation 1]
- [Specific observation 2]

**Code I'm Focused On:**
- `[file:line]` - [Why Claude Code suspects this]
- `[file:line]` - [Why Claude Code suspects this]

**What I've Already Analyzed:**
1. [Analysis step 1 and finding]
2. [Analysis step 2 and finding]
3. [Analysis step 3 and finding]

## MY ASSUMPTIONS (PLEASE CHALLENGE THESE)
1. [Assumption about the system]
2. [Assumption about the data flow]
3. [Assumption about dependencies]
4. [Assumption about configuration]

## CHECK FOR COMMON TUNNEL VISION TRAPS
I may be falling into one of these patterns:
1. **Looking Where the Error Appears** (not where it originates)
2. **Assuming Recent Changes** (when old code hit new conditions)
3. **Focusing on Code** (when it's config/environment/data)
4. **Debugging Symptoms** (not root causes)
5. **Trusting Error Messages** (when they're misleading)
6. **Assuming Local = Production** (environment differences)
7. **Following Stack Traces** (missing async/timing issues)
8. **Checking Application Layer** (missing infrastructure/OS issues)

## GEMINI: PLEASE PROVIDE FRESH PERSPECTIVE

1. **Challenge My Assumptions:** What am I taking for granted that might be wrong?

2. **Alternative Root Causes:** What OTHER parts of the system could cause these symptoms?
   - Consider timing/race conditions
   - Consider configuration issues
   - Consider environmental differences
   - Consider dependency conflicts
   - Consider data corruption
   - Consider edge cases I missed

3. **Minimal Reproduction:**
   Based on your analysis, suggest the simplest possible code snippet or command that should reproduce this bug.

4. **Systemic Issues:**
   - Could this be a design flaw rather than a bug?
   - Are there architectural issues at play?
   - Is this a symptom of technical debt?

## OUTPUT FORMAT

Please provide:
1. **Most Likely Alternative Causes** (ranked by probability)
2. **Specific Things to Check** (with exact commands/locations)
3. **Minimal Reproduction Case** (simplest way to trigger the bug)
4. **Debug Strategy** (systematic approach)
5. **Quick Experiments** (to prove/disprove theories)

Remember: I might be completely wrong about where the bug is. Look everywhere, not just where I'm pointing."
```

### Step 4: Process and Report Findings (Automated)

After Gemini responds, Claude Code automatically:

1. **Analyzes the response**
2. **Identifies key insights**
3. **Creates action plan**
4. **Reports to user**

Example output:
```markdown
## 🎯 Fresh Insights from Gemini

### What I Missed:
- The nginx configuration was stripping Authorization headers
- Only affects requests > 8KB (why it seemed random)
- The real issue was 3 layers away from where I was looking

### Corrected Understanding:
- I assumed: JWT validation was broken
- Reality: Infrastructure was blocking the headers
- Evidence: Minimal reproduction shows size-dependent behavior

### Action Plan:
1. Fix nginx config at /etc/nginx/conf.d/api.conf:23
2. Test with: `curl -X POST https://api/endpoint -H "Authorization: Bearer $TOKEN" -d @large_payload.json`
3. Add monitoring for header stripping
4. Document this infrastructure requirement
```

---

## ⚠️ **ANTI-PATTERNS TO AVOID**

### ❌ The Manual Execution Trap
**NEVER DO THIS:**
```
Claude: "Run this command and paste the output:"
User: [Runs command]
User: [Pastes output]
```

**ALWAYS DO THIS:**
```
Claude Code: "Analyzing git history and running Gemini..."
[Claude Code executes everything automatically]
"Based on the analysis, here's what I found..."
```

### ❌ The Question Cascade
**AVOID:**
```
"What's the error?"
"When did it start?"
"What have you tried?"
"Do you have logs?"
```

**PREFER:**
```
"I see auth failures from the context. Let me analyze with fresh eyes..."
```

---

## 📊 **SAMPLE INTERACTIONS**

### Scenario 1: Clear Context with Baseline

```
[Previous conversation about auth errors]

User: /debug-gemini

Claude Code: "I see you're experiencing auth failures with 'Invalid token' errors. 
             Based on our discussion, you suspect the JWT validation at auth.py:45.
             Do you have a baseline branch where auth was working correctly?"

User: "Yes, it works in 'main'"

Claude Code: "Perfect. Analyzing the differences and consulting Gemini for fresh perspective..."
             [Automatically runs all git commands]
             [Automatically creates debug context]
             [Automatically executes gemini -p]
             [Automatically processes response]
             
             "Found it! I was tunnel-visioned on the Python code. The issue is actually 
             in nginx config - it's stripping the Authorization header for requests over 8KB.
             Here's your action plan:
             1. Fix /etc/nginx/conf.d/api.conf:23
             2. Test with the minimal reproduction case
             3. The commit that broke it was a3f2d (3 days ago)"
```

### Scenario 2: Minimal Context

```
User: /debug-gemini

Claude Code: "What specific issue should I help debug?"

User: "Random 500 errors in production"

Claude Code: "I'll analyze for intermittent server errors. When did these start, and do you 
             have a baseline where they didn't occur?"

User: "Started yesterday, main branch is stable"

Claude Code: "Analyzing changes since main branch..."
             [Runs complete automated analysis]
             
             "The 500 errors are caused by a memory leak introduced in commit b4f23a.
             The async task queue isn't releasing references properly. Here's the fix..."
```

### Scenario 3: Complete Context

```
[Extensive discussion about performance regression]

User: /debug-gemini

Claude Code: "I'll debug the performance regression in the data pipeline that went from 
             100ms to 5s. You mentioned it started after the Friday deploy. Using 
             'release-2.1' as baseline for comparison..."
             [Immediately runs full analysis]
             
             "Gemini identified the issue: The new caching layer is actually causing 
             cache stampedes under load. The minimal reproduction is just 10 concurrent
             requests. Here's how to fix it..."
```

---

## 💡 **ENHANCED DEBUG PATTERNS**

### Pattern 1: The Baseline Comparison (Fully Automated)
Claude Code executes:
```bash
# Get comprehensive diff from baseline
git diff main..HEAD --stat > /tmp/baseline_stat.txt
git diff main..HEAD --name-status | head -50 > /tmp/baseline_names.txt
git diff main..HEAD -- ptycho/ src/ configs/ | head -500 > /tmp/baseline_diff.txt

# Create combined analysis file
cat > /tmp/baseline_analysis.txt << 'EOF'
[Combined diff analysis]
EOF

# Execute Gemini analysis
gemini -p "@ptycho/ @src/ @tests/ @configs/ @logs/ @/tmp/baseline_analysis.txt Analyze regression from baseline..."
```

### Pattern 2: The Git Bisect Helper (Fully Automated)
Claude Code runs:
```bash
# Get commit history between baseline and HEAD
git log --oneline --graph <baseline>..HEAD > /tmp/bisect_commits.txt

# Execute targeted analysis
gemini -p "@ptycho/ @src/ @tests/ @/tmp/bisect_commits.txt Identify when bug was introduced..."
```

### Pattern 3: The Minimal Reproduction Focus (Fully Automated)
Claude Code executes:
```bash
gemini -p "@ptycho/ @src/ @tests/ Create minimal reproduction for complex bug..."
```

---

## 🎯 **Why This Works**

1. **Zero Friction**: User runs one command, gets complete analysis
2. **Fresh Perspective**: Gemini has no preconceptions about the bug
3. **Comprehensive Context**: Git history + code + configs + logs
4. **Tunnel Vision Breaking**: Explicitly challenges assumptions
5. **Actionable Output**: Specific fixes, not just analysis

---

## 📈 **Success Metrics**

Track debugging effectiveness:
- **Time to Root Cause**: Usually 2-5 minutes total
- **Tunnel Vision Breaks**: ~80% find issues outside initial focus
- **Minimal Reproduction Success**: ~90% provide working minimal case
- **First-Try Fix Rate**: ~70% of suggested fixes work immediately

---

## 🚀 **Quick Reference**

```bash
# User runs:
/debug-gemini

# Claude Code does everything else:
- Identifies issue from context
- Runs git analysis
- Executes Gemini
- Reports findings
- Provides action plan

# Total time: 2-5 minutes
# Manual steps: ZERO
```

Remember: Claude Code is an autonomous agent that EXECUTES debugging, not a tool that suggests debugging steps!
</file>

<file path="debug-gemini-v3.md">
# Command: /debug-gemini-v3 [baseline-ref]

**Goal:** Get comprehensive debugging help from Gemini with fresh perspective, especially when you might have tunnel vision about the root cause.

**Usage:** 
- `/debug-gemini-v3` - Analyzes issue, asks for baseline if needed
- `/debug-gemini-v3 main` - Uses 'main' branch as baseline
- `/debug-gemini-v3 abc123f` - Uses specific commit as baseline
- `/debug-gemini-v3 v1.2.3` - Uses tag as baseline

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**THIS IS THE CORE PURPOSE OF THIS COMMAND:**
1. You MUST run git analysis (with baseline from args if provided)
2. You MUST execute `gemini -p` 
3. You MUST wait for and process Gemini's response
4. You MUST report Gemini's findings to the user

**DO NOT:**
- ❌ Stop after git analysis
- ❌ Provide your own analysis instead of running Gemini
- ❌ Get distracted by intermediate findings
- ❌ Skip the gemini execution for ANY reason

**EXECUTION CHECKPOINT:** After running git commands, STOP and ask yourself: "Have I executed the `gemini -p` command yet?" If NO, execute it immediately.

---

## 🤖 **CONTEXT: YOU ARE CLAUDE CODE**

You are Claude Code - the autonomous command-line tool that executes shell commands directly. You:
- **Execute** commands autonomously (don't just suggest them)
- **Run** the entire workflow without human intervention
- **Complete** debugging from start to finish in one go

## 🚨 **YOU MUST EXECUTE GEMINI - THIS IS NOT OPTIONAL**

When a user runs `/debug-gemini`, you handle EVERYTHING:
- ✅ Run all git commands automatically
- ✅ Create all temp files automatically  
- **✅ Execute the `gemini -p` command directly (MANDATORY)**
- ✅ Process Gemini's response automatically
- ✅ Report findings and action plan
- ❌ NO asking users to copy commands
- ❌ NO waiting for users to paste outputs
- ❌ NO providing your own analysis without running Gemini

---

## 🎯 **Your Complete Autonomous Flow**

```mermaid
graph LR
    A[User: /debug-gemini] --> B[You: Identify issue from context]
    B --> C[You: Run git analysis automatically]
    C --> D[You: Create temp files]
    D --> E[**You: MUST Execute gemini -p command**]
    E --> F[You: Analyze Gemini output]
    F --> G[You: Report GEMINI'S findings]
```

**Total user interaction: ONE command**

---

## ✅ **EXECUTION CHECKLIST**

Before reporting ANY findings, verify:
- [ ] I identified the issue from context
- [ ] I ran git analysis commands
- [ ] I created ./tmp/debug_context.txt WITH ACTUAL CONTENT (not shell commands)
- [ ] **I EXECUTED `gemini -p` command** ← THIS IS MANDATORY
- [ ] I received Gemini's response
- [ ] I'm reporting GEMINI'S findings, not my own analysis

If you haven't executed `gemini -p`, STOP and do it now.
If debug_context.txt contains `$(cat ...)` commands, STOP and fix it using the incremental approach.

---

## 📋 **YOUR EXECUTION WORKFLOW**

### Step 1: Assess Current Context

Determine from the conversation/context:
- What's the bug/issue
- What's been tried already
- Your current working theory
- Check if baseline was provided via arguments

**Parse baseline from arguments:**
```bash
# Ensure we have a tmp directory for our debug files
mkdir -p ./tmp

# $ARGUMENTS contains whatever user typed after /debug-gemini-v3
BASELINE_REF="$ARGUMENTS"
if [ -n "$BASELINE_REF" ]; then
    # User provided a baseline ref
    if git rev-parse --verify "$BASELINE_REF" >/dev/null 2>&1; then
        echo "Using provided baseline: $BASELINE_REF"
    else
        echo "Warning: '$BASELINE_REF' is not a valid git ref, will auto-detect baseline"
        BASELINE_REF=""
    fi
else
    echo "No baseline provided, will auto-detect from main/master/HEAD~5"
fi
```

**Only ask the user if:**
- The issue is unclear from context
- No baseline provided and auto-detection fails
- Provided baseline is invalid and you need a valid one

### Step 2: Run Debug Analysis (You Execute This)

**⚠️ CRITICAL ISSUE:** Heredocs with command substitution (`$(...)`) do not work properly in Claude Code's environment. Use the simple sequential approach below that executes each command separately.

**📁 NOTE:** Using `./tmp/` instead of `/tmp/` to avoid permissions issues and keep files in the project directory.

Automatically execute these commands without any user intervention:

```bash
# Ensure tmp directory exists in current working directory (if not already created in Step 1)
mkdir -p ./tmp

# Get recent commit history (NOTE: Use dash separator, not pipe to avoid parsing issues)
git log -n 10 --pretty=format:"%h %ad - %s [%an]" --date=short > ./tmp/debug_git_log.txt

# Get current status
git status --porcelain > ./tmp/debug_git_status.txt

# Use baseline ref from arguments or determine automatically
if [ -n "$BASELINE_REF" ]; then
    # User provided baseline
    BASELINE="$BASELINE_REF"
elif git rev-parse --verify main >/dev/null 2>&1; then
    # Default to main if it exists
    BASELINE="main"
elif git rev-parse --verify master >/dev/null 2>&1; then
    # Fall back to master
    BASELINE="master"
else
    # Use HEAD~5 as last resort
    BASELINE="HEAD~5"
fi

# Get diffs from baseline
git diff "$BASELINE"..HEAD --stat > ./tmp/debug_diff_stat.txt
git diff "$BASELINE"..HEAD --name-status > ./tmp/debug_diff_names.txt
git diff "$BASELINE"..HEAD -- ptycho/ src/ configs/ package.json requirements.txt > ./tmp/debug_diff_details.txt

# CRITICAL: Build debug context file using simple sequential approach
# Heredocs with command substitution don't work in Claude Code's environment
# Each command is executed separately to ensure actual content is written

echo "## RECENT COMMITS" > ./tmp/debug_context.txt
cat ./tmp/debug_git_log.txt >> ./tmp/debug_context.txt
echo -e "\n## CURRENT GIT STATUS" >> ./tmp/debug_context.txt  
cat ./tmp/debug_git_status.txt >> ./tmp/debug_context.txt
echo -e "\n## BASELINE USED: $BASELINE" >> ./tmp/debug_context.txt
echo -e "\n## DIFF STATISTICS (from $BASELINE to HEAD)" >> ./tmp/debug_context.txt
cat ./tmp/debug_diff_stat.txt >> ./tmp/debug_context.txt 2>/dev/null || echo "No baseline diff available" >> ./tmp/debug_context.txt
echo -e "\n## FILES CHANGED" >> ./tmp/debug_context.txt
cat ./tmp/debug_diff_names.txt >> ./tmp/debug_context.txt 2>/dev/null || echo "No file changes detected" >> ./tmp/debug_context.txt
echo -e "\n## DETAILED CODE CHANGES" >> ./tmp/debug_context.txt
head -2000 ./tmp/debug_diff_details.txt >> ./tmp/debug_context.txt 2>/dev/null || echo "No detailed diffs available" >> ./tmp/debug_context.txt

# Python alternative if shell approaches fail:
# BASELINE="$BASELINE" python3 << 'EOF'
# import os
# os.makedirs('./tmp', exist_ok=True)
# baseline = os.environ.get('BASELINE', 'unknown')
# sections = [
#     ('RECENT COMMITS', './tmp/debug_git_log.txt'),
#     ('CURRENT GIT STATUS', './tmp/debug_git_status.txt'),
#     (f'BASELINE USED: {baseline}', None),
#     (f'DIFF STATISTICS (from {baseline} to HEAD)', './tmp/debug_diff_stat.txt'),
#     ('FILES CHANGED', './tmp/debug_diff_names.txt'),
#     ('DETAILED CODE CHANGES', './tmp/debug_diff_details.txt')
# ]
# 
# with open('./tmp/debug_context.txt', 'w') as out:
#     for title, path in sections:
#         out.write(f'## {title}\n')
#         if path is None:
#             out.write('\n')
#         else:
#             try:
#                 with open(path) as f:
#                     content = f.read()
#                     if 'DETAILED CODE CHANGES' in title:
#                         lines = content.splitlines()[:2000]
#                         content = '\n'.join(lines)
#                     out.write(content + '\n')
#             except:
#                 out.write(f'No data available\n')
#         out.write('\n')
# EOF

# Verify the file contains actual content
echo "Verifying debug context file..."
if grep -q '$(cat' ./tmp/debug_context.txt; then
    echo "❌ ERROR: File still contains shell commands! Try the Python approach above."
else
    echo "✅ Debug context file ready with actual git data"
fi
```

### Step 2.5: MANDATORY prepare gemini inputs / context 
The debug_context.txt file MUST contain:
- Recent commit history (actual commits, not `$(cat ...)` commands)
- Current git status (actual file list)
- **Baseline used for comparison** (branch/commit shown in file)
- Diff statistics showing which files changed and by how much
- Complete file change list
- **Actual code diffs** showing exact line-by-line changes

**⚠️ CRITICAL:** If Gemini receives shell commands instead of actual content, it cannot analyze your issue!

Pay special attention to:
- Recent commits that might have introduced the issue
- **Actual code changes in the diffs** (not just commit messages)
- Modified configuration files
- Dependency updates


### Step 3: MANDATORY - Execute Gemini Analysis

**🔴 STOP - THIS STEP IS MANDATORY - DO NOT SKIP**
#### Step 3.1
Run this shell command:
```bash
npx repomix@latest .   --include "**/*.sh,**/*.md,**/*.py,**/*.c,**/*.h,**/*.json,**/*.log" --ignore ".aider.chat.history.md,PtychoNN/**,build/**,ptycho/trash/**,diagram/**,tests/**,notebooks/**,Oclaude.md,ptycho.md,plans/archive/**,dpl.md"
```
It will generate a file ./repomix-output.xml, which we will provide to gemini

#### Step 3.2
delete ./gemini-prompt.md if it exists.

#### Step 3.3
You MUST now populate this command template and save it to gemini-prompt.md. Note that text in [] brackets are placeholders, to be populated by you. 

```markdown
<task> Debug this issue with FRESH EYES:

Carry out the following steps:
<steps/>
<0>
list the files included in the repomix archive
</0>
<1>
REVIEW PROJECT DOCUMENTATION
 - **Read CLAUDE.md thoroughly** - This contains essential project context, architecture, and known patterns
 - **Read DEVELOPER_GUIDE.md carefully** - This explains the development workflow, common issues, and debugging approaches
 - **Understand the project structure** from these documents before diving into the code

These documents contain crucial information about:
- Project architecture and design decisions
- Known quirks and edge cases
- Common debugging patterns
- Project-specific conventions and practices
</1>

<2>
review the provided <git> diff. it represents the increment of possibly-buggy code changes that we're analyzing.
</2> 

<3> 
analyze the issue's prior understanding <summary>. Then, attempt to find the root cause and propose a fix approach 
</3>
</steps>

<summary>
## ISSUE SUMMARY
**Symptoms:** [Detailed symptoms with specific errors, stack traces, or behaviors]
**When It Happens:** [Specific conditions, inputs, or sequences that trigger it]
**When It Doesn't Happen:** [Cases where it works fine]
**Environment:** [Dev/staging/prod, OS, versions]
**Baseline Branch:** [If provided - where it last worked correctly]

## MY CURRENT UNDERSTANDING
**My Leading Theory:** [What you think is wrong]
**Evidence For This Theory:** 
- [Specific observation 1]
- [Specific observation 2]

**Code I'm Focused On:**
- `[file:line]` - [Why you suspect this]
- `[file:line]` - [Why you suspect this]

**What I've Already Analyzed:**
1. [Analysis step 1 and finding]
2. [Analysis step 2 and finding]
3. [Analysis step 3 and finding]

## MY ASSUMPTIONS (PLEASE CHALLENGE THESE)
1. [Assumption about the system]
2. [Assumption about the data flow]
3. [Assumption about dependencies]
4. [Assumption about configuration]
</summary>

<git>
## GIT CONTEXT
[tell gemini about the debug_context.txt that you generated]
</git>


<guidelines>
## FOCUS ON IDENTIFYING ROOT CAUSES
1. **Challenge My Assumptions:** What am I taking for granted that might be wrong?

2. **Alternative Root Causes:** What OTHER parts of the system could cause these symptoms?
   - Consider timing/race conditions
   - Consider configuration issues
   - Consider environmental differences
   - Consider dependency conflicts
   - Consider data corruption
   - Consider edge cases I missed

3. **Minimal Reproduction:**
   Based on your analysis, suggest the simplest possible code snippet or command that should reproduce this bug.

4. **Systemic Issues:**
   - Could this be a design flaw rather than a bug?
   - Are there architectural issues at play?
   - Is this a symptom of technical debt?
5. Do not use tools. All needed context files you need should be included in your context with 
surrounding xml tags. If required information is missing, abort the analysis with an explanataion of the reason you cannot 
successfully continue.
</guidelines>


<output format>
Please provide:
1. **Most Likely Alternative Causes** (ranked by probability)
2. **Minimal Reproduction Case** (simplest way to trigger the bug)
3. **Debug Strategy** (systematic approach)

</output format>
Remember: I might be completely wrong about where the bug is. Look everywhere, not just where I'm pointing.
</task>"
```

#### Step 3.4
You MUST now EXECUTE the following shell commands:
```bash
cat  repomix-output.xml >> gemini-prompt.md
```

```bash
gemini -p "@gemini-prompt.md"
```


**VERIFICATION:** Before proceeding to Step 4, confirm you have:
- [ ] Generated the complete `gemini -p` command
- [ ] EXECUTED the command (not just generated it)
- [ ] Received Gemini's response
- [ ] If any of above are NO, go back and EXECUTE THE COMMAND

### Step 4: Process and Report GEMINI'S Findings (Not Your Own)

After Gemini responds, you:

1. **Analyze GEMINI'S response** (not your own theories)
2. **Identify GEMINI'S key insights**
3. **Create action plan based on GEMINI'S analysis**
4. **Report GEMINI'S findings to user**

Example of your output:
```markdown
## 🎯 Fresh Insights from Gemini

### What Gemini Found That I Missed:
- The nginx configuration was stripping Authorization headers
- Only affects requests > 8KB (why it seemed random)
- The real issue was 3 layers away from where I was looking

### Gemini's Analysis:
- I assumed: JWT validation was broken
- Gemini found: Infrastructure was blocking the headers
- Evidence: Minimal reproduction shows size-dependent behavior

### Action Plan (Based on Gemini's Recommendations):
1. Fix nginx config at /etc/nginx/conf.d/api.conf:23
2. Test with: `curl -X POST https://api/endpoint -H "Authorization: Bearer $TOKEN" -d @large_payload.json`
3. Add monitoring for header stripping
4. Document this infrastructure requirement
```

---

## 🚨 **IF GEMINI COMMAND FAILS**

If the `gemini -p` command fails to execute:
1. Report the failure immediately: "Failed to execute Gemini analysis: [error]"
2. Do NOT provide your own analysis as a substitute
3. Ask user if they want you to try a different approach
4. The command's PURPOSE is to get Gemini's perspective - without it, the command has failed

---

## ⚠️ **PATTERNS TO AVOID**

### ❌ Don't Skip Gemini Execution
**NEVER DO THIS:**
```
You: "Analyzing git history..."
[Run git commands]
"Based on the git diff, I think the issue is..." ❌ WRONG - You didn't run Gemini!
```

**ALWAYS DO THIS:**
```
You: "Analyzing git history and consulting Gemini..."
[Run git commands]
[EXECUTE gemini -p command]
"Based on GEMINI'S analysis..."
```

### ❌ Don't Provide Your Own Analysis Instead
**WRONG:**
```
You: "The git analysis shows changes in auth.py. I believe the issue is..."
```

**RIGHT:**
```
You: "Running Gemini for fresh perspective..."
[EXECUTE gemini -p]
"Gemini identified that the issue is actually..."
```

---

## 📊 **SAMPLE INTERACTIONS**

### ✅ Correct Execution:

```
User: /debug-gemini

You: "I see you're experiencing auth failures with 'Invalid token' errors. 
      Based on our discussion, you suspect the JWT validation at auth.py:45.
      Do you have a baseline branch where auth was working correctly?"

User: "Yes, it works in 'main'"

You: "Perfect. Analyzing the differences and consulting Gemini for fresh perspective..."
     [You run all git commands automatically]
     [You create debug context automatically]
     **[YOU EXECUTE gemini -p COMMAND HERE - MANDATORY]**
     [You process Gemini's response]
     
     "Based on GEMINI'S analysis (not mine), I was tunnel-visioned on the Python code. 
     Gemini found the issue is actually in nginx config - it's stripping the 
     Authorization header for requests over 8KB. Here's Gemini's recommended action plan:
     1. Fix /etc/nginx/conf.d/api.conf:23
     2. Test with the minimal reproduction case Gemini provided
     3. The commit that broke it was a3f2d (3 days ago)"
```

### ❌ What NOT to do:

```
User: /debug-gemini

You: "I see auth failures. Let me analyze..."
     [Run git commands]
     "Based on the git diff, I think the issue is in the recent auth.py changes..."
     ❌ FAILED - You never executed Gemini!
```

---

## 💡 **ENHANCED DEBUG PATTERNS**

### Pattern 1: Baseline Comparison (You Execute All of This)
```bash
# If user provided baseline via argument, use it
# Otherwise use detected baseline (main/master/HEAD~5)
BASELINE="${BASELINE_REF:-main}"

# Ensure tmp directory exists (if not already created)
mkdir -p ./tmp

# Get comprehensive diff from baseline
git diff "$BASELINE"..HEAD --stat > ./tmp/baseline_stat.txt
git diff "$BASELINE"..HEAD --name-status | head -50 > ./tmp/baseline_names.txt
git diff "$BASELINE"..HEAD -- ptycho/ src/ configs/ | head -500 > ./tmp/baseline_diff.txt

# Create combined analysis file with ACTUAL DIFFS (use simple approach)
echo "## BASELINE DIFF SUMMARY" > ./tmp/baseline_analysis.txt
cat ./tmp/baseline_stat.txt >> ./tmp/baseline_analysis.txt
echo -e "\n## FILES CHANGED" >> ./tmp/baseline_analysis.txt
cat ./tmp/baseline_names.txt >> ./tmp/baseline_analysis.txt
echo -e "\n## DETAILED CODE CHANGES (ACTUAL DIFFS)" >> ./tmp/baseline_analysis.txt
cat ./tmp/baseline_diff.txt >> ./tmp/baseline_analysis.txt

# MANDATORY: Execute Gemini analysis
gemini -p "@repomix-output.xml  First review CLAUDE.md and DEVELOPER_GUIDE.md, then analyze regression from baseline $BASELINE..."
```

### Pattern 2: Git Bisect Helper (You Execute All of This)
```bash
# Ensure tmp directory exists (if not already created)
mkdir -p ./tmp

# Get commit history between baseline and HEAD
git log --oneline --graph <baseline>..HEAD > ./tmp/bisect_commits.txt

# MANDATORY: Execute targeted analysis
gemini -p "@repomix-output.xml First review CLAUDE.md and DEVELOPER_GUIDE.md, then identify when bug was introduced..."
```

---

## 🎯 **Why This Approach Works**

1. **Fresh Perspective**: Gemini has no preconceptions about the bug
2. **Comprehensive Context**: Git history + code + configs + logs
3. **Tunnel Vision Breaking**: Explicitly challenges assumptions
4. **Zero Manual Steps**: User runs one command, gets complete analysis
5. **Gemini's Insights**: The whole point is to get an outside perspective

---

## 📈 **Success Metrics**

Track your debugging effectiveness:
- **Gemini Execution Rate**: Must be 100% - if not, the command failed
- **Time to Root Cause**: Usually 2-5 minutes total
- **Tunnel Vision Breaks**: ~80% find issues outside initial focus
- **Minimal Reproduction Success**: ~90% provide working minimal case

---

## 🔧 **TROUBLESHOOTING**

### Debug Context File Issues

**Known Issue:** Heredocs with command substitution don't work in Claude Code's environment. The command would create a file containing `$(cat ./tmp/debug_git_log.txt)` instead of actual git commits.

**Solution Implemented:** Step 2 now uses a simple sequential approach where each line is executed separately. This ensures the debug_context.txt file contains actual git data that Gemini can analyze.

**Verification:** After creating debug_context.txt, the script verifies it doesn't contain shell commands. If verification fails, try the Python alternative (commented in Step 2).

**Expected Content:**
```
## RECENT COMMITS
abc123 2025-01-19 - fix bug [author]
def456 2025-01-18 - add feature [author]
```

**NOT:**
```
## RECENT COMMITS
$(cat ./tmp/debug_git_log.txt)
```

### Temporary Files Location

**Why `./tmp/` instead of `/tmp/`:**
- Avoids system permissions issues
- Keeps debug files within project directory
- Easier to inspect/debug if needed
- Can be added to .gitignore

**Cleanup:** The `./tmp/` directory and its contents can be safely deleted after debugging:
```bash
rm -rf ./tmp/
```

**Add to .gitignore:** Consider adding `tmp/` to your .gitignore file to avoid committing debug files:
```bash
echo "tmp/" >> .gitignore
```

---

## 🚀 **Final Execution Reminder**

When user runs `/debug-gemini-v3 [baseline-ref]`:
1. Parse baseline ref from arguments (if provided)
2. Identify issue from context (or ask minimal questions)
3. Run git analysis automatically with baseline
4. **EXECUTE gemini -p command (NOT OPTIONAL)**
5. Process GEMINI'S response (not your own analysis)
6. Report GEMINI'S findings with action plan

**Usage Examples:**
- `/debug-gemini-v3` - Auto-detects baseline
- `/debug-gemini-v3 main` - Compare against main branch
- `/debug-gemini-v3 v1.2.3` - Compare against specific tag
- `/debug-gemini-v3 abc123f` - Compare against specific commit

**The command has NOT succeeded until you've executed `gemini -p` and reported Gemini's findings.**
</file>

<file path="debug-gemini.md">
# Command: /debug-gemini

**Goal:** Get comprehensive debugging help from Gemini with fresh perspective, especially when Claude might have tunnel vision about the root cause.

---

## ⚠️ **YOUR ROLE AS CLAUDE**

You will:
1. Share your current understanding of the bug
2. Explicitly state your assumptions and suspected causes
3. Ask Gemini to challenge your assumptions
4. Request alternative explanations you might have missed
5. Include EXTENSIVE context (not just code where you think the bug is)

---

## 🚀 **EXECUTION WORKFLOW**

### Step 1: Gather Your Current Understanding (Claude)

Before asking Gemini, document:
1. **Symptoms:** What exactly is happening?
2. **Your Theory:** What you think is causing it
3. **What You've Tried:** Debugging steps taken
4. **Your Assumptions:** What you're taking for granted
5. **Tunnel Vision Risk:** What you might be overlooking

### Step 2: Generate Comprehensive Debug Command

Create this command for the user, being GENEROUS with paths:

```bash
gemini -p "@src/ @ptycho/ @tests/ @docs/ @configs/ @logs/ @.github/ @scripts/ @benchmarks/ @examples/ Debug this issue with FRESH EYES:

## ISSUE SUMMARY
**Symptoms:** [Detailed symptoms with specific errors, stack traces, or behaviors]
**When It Happens:** [Specific conditions, inputs, or sequences that trigger it]
**When It Doesn't Happen:** [Cases where it works fine]
**Environment:** [Dev/staging/prod, OS, versions]

## CLAUDE'S CURRENT UNDERSTANDING
**My Leading Theory:** [What Claude thinks is wrong]
**Evidence For This Theory:** 
- [Specific observation 1]
- [Specific observation 2]

**Code I'm Focused On:**
- `[file:line]` - [Why Claude suspects this]
- `[file:line]` - [Why Claude suspects this]

**What I've Already Tried:**
1. [Debugging step 1 and result]
2. [Debugging step 2 and result]
3. [Debugging step 3 and result]

## MY ASSUMPTIONS (PLEASE CHALLENGE THESE)
1. [Assumption about the system]
2. [Assumption about the data flow]
3. [Assumption about dependencies]
4. [Assumption about configuration]

## GEMINI: PLEASE PROVIDE FRESH PERSPECTIVE

1. **Challenge My Assumptions:** What am I taking for granted that might be wrong?

2. **Alternative Root Causes:** What OTHER parts of the system could cause these symptoms?
   - Consider timing/race conditions
   - Consider configuration issues
   - Consider environmental differences
   - Consider dependency conflicts
   - Consider data corruption
   - Consider edge cases I missed

3. **Check Project Documentation:** 
   - Are there known issues in README.md, CHANGELOG.md, or docs/?
   - Are there migration guides I missed?
   - Are there deprecation warnings?
   - Are there similar fixed issues in closed PRs/issues?

4. **Analyze Wider Context:**
   - What related systems could be involved?
   - What recent changes in OTHER modules could affect this?
   - What implicit dependencies exist?
   - What assumptions does the code make about its environment?

5. **Suggest Non-Obvious Debug Steps:**
   - What diagnostic commands would reveal more?
   - What logging should be added?
   - What state should be inspected?
   - What tools could help (profilers, tracers, etc.)?

6. **Pattern Recognition:**
   - Have you seen similar symptoms in this codebase before?
   - What patterns in the code could lead to this behavior?
   - Are there anti-patterns that match these symptoms?

7. **Systemic Issues:**
   - Could this be a design flaw rather than a bug?
   - Are there architectural issues at play?
   - Is this a symptom of technical debt?

## SPECIFIC AREAS TO INVESTIGATE

Beyond my focus areas, please specifically check:
- Race conditions in async/concurrent code
- State management issues
- Cache invalidation problems
- Off-by-one errors
- Timezone/locale issues
- Memory leaks or resource exhaustion
- Network timeouts or retries
- Permission or security constraints
- Build/compilation issues
- Version mismatches

## OUTPUT FORMAT

Please provide:
1. **Most Likely Alternative Causes** (ranked by probability)
2. **Specific Things to Check** (with exact commands/locations)
3. **Debug Strategy** (systematic approach)
4. **Quick Experiments** (to prove/disprove theories)
5. **Long-term Fixes** (if this reveals systemic issues)

Remember: I might be completely wrong about where the bug is. Look everywhere, not just where I'm pointing."
```

### Step 3: Process Gemini's Fresh Perspective

When Gemini responds:

1. **Highlight Surprising Findings:**
   ```markdown
   ## 🎯 Fresh Insights from Gemini
   
   ### Things Claude Missed:
   - [Unexpected cause 1]
   - [Overlooked connection 2]
   
   ### Challenged Assumptions:
   - Claude assumed [X], but actually [Y]
   - Claude focused on [A], but [B] is more likely
   ```

2. **Create New Debug Plan:**
   Based on Gemini's analysis, create a systematic debug approach

3. **Update Understanding:**
   Document what tunnel vision caused you to miss

---

## 💡 **ENHANCED DEBUG PATTERNS**

### Pattern 1: Performance Degradation
```bash
gemini -p "@src/ @benchmarks/ @profiling/ @logs/ @monitoring/ @configs/ @docs/performance/ 
Performance degraded after [change]. I think it's [cause], but need fresh eyes.
[Include specific metrics, timeline, what changed]
Check for: memory leaks, N+1 queries, cache misses, lock contention, GC pressure"
```

### Pattern 2: Intermittent Failures
```bash
gemini -p "@src/ @tests/ @.github/workflows/ @logs/ @configs/ @infrastructure/
Intermittent test failures. I think it's [race condition in X], but could be wrong.
[Include failure rate, patterns, logs]
Check for: test pollution, timezone issues, external dependencies, resource limits"
```

### Pattern 3: Integration Issues
```bash
gemini -p "@src/ @docs/api/ @examples/ @integration_tests/ @configs/ @docker/
API integration failing. I think it's [auth issue], but customer says it worked before.
[Include request/response, versions, environment]
Check for: API changes, version mismatches, network policies, SSL/TLS issues"
```

### Pattern 4: Data Corruption
```bash
gemini -p "@src/ @migrations/ @docs/data/ @scripts/ @tests/fixtures/ @configs/
Data corruption in [table/field]. I think it's [bad migration], but could be deeper.
[Include samples, timeline, affected records]
Check for: race conditions, transaction issues, encoding problems, precision loss"
```

---

## 🎯 **ANTI-TUNNEL VISION CHECKLIST**

Before sending to Gemini, ask yourself:
- [ ] Am I including areas OUTSIDE where I think the bug is?
- [ ] Have I included all documentation, not just code?
- [ ] Am I sharing what WORKS, not just what's broken?
- [ ] Have I listed my assumptions explicitly?
- [ ] Am I open to being completely wrong?

---

## 📊 **SAMPLE INTERACTION**

```
Claude: "I think the auth bug is in the JWT validation at auth.py:45..."
[Generates Gemini command with full context]

Gemini: "The JWT validation is fine. The issue is actually in the nginx 
config at /etc/nginx/conf.d/api.conf:23 - it's stripping the Authorization 
header for requests over 8KB. This explains why it only fails for users 
with large permission sets."

Claude: "I was completely focused on the Python code and missed the 
infrastructure layer! Here's a new debug plan based on your insight..."
```

---

## 🚨 **COMMON TUNNEL VISION TRAPS**

Share these with Gemini to check:

1. **Looking Where the Error Appears** (not where it originates)
2. **Assuming Recent Changes** (when old code hit new conditions)
3. **Focusing on Code** (when it's config/environment/data)
4. **Debugging Symptoms** (not root causes)
5. **Trusting Error Messages** (when they're misleading)
6. **Assuming Local = Production** (environment differences)
7. **Following Stack Traces** (missing async/timing issues)
8. **Checking Application Layer** (missing infrastructure/OS issues)

---

## 🔄 **ITERATIVE DEBUGGING**

After Gemini's first analysis:

```bash
# If new theory emerges
gemini -p "@[new_relevant_paths]/ Gemini suggested [theory]. 
Let's deep dive into [specific area] to verify...
[Include Gemini's evidence]
Please analyze [specific aspect] in detail."

# If multiple theories exist
gemini -p "@src/ Here are the top 3 theories:
1. [Gemini's top theory]
2. [Alternative theory]
3. [Claude's original theory]
Design experiments to distinguish between these."
```

---

## 💡 **MAXIMIZING GEMINI'S HELP**

### DO Include:
- Error messages AND success cases
- Logs from MULTIPLE sources
- Configuration files (all of them)
- Documentation and comments
- Test files (even passing ones)
- CI/CD configurations
- Monitoring/metrics data
- Example user inputs
- Environment details

### DO Ask For:
- Alternative explanations
- Non-obvious connections
- Historical patterns
- Systemic issues
- Missing safeguards
- Better error handling
- Preventive measures

### DON'T:
- Limit paths to suspected areas
- Hide your wrong assumptions
- Skip "irrelevant" details
- Focus only on recent changes

---

## 🎯 **SUCCESS METRICS**

Track your tunnel vision improvement:
- How often was your initial theory wrong?
- How many fresh insights did Gemini provide?
- How much debugging time was saved?
- What patterns of tunnel vision do you have?

Remember: The goal is to break out of tunnel vision and see the bug from completely new angles!
</file>

<file path="gemdiffs.md">
# Git Diff Analysis Prompt

## Agent Instructions

Consider this list of file paths:
files="`$ARGUMENTS`"

Collect all git diffs and analyze them with Gemini in a single batch.

### Step 1: Collect All Diffs

```bash
# Create a combined diff file
for file in $files; do
    echo "=== FILE: $file ==="
    git diff "$file" || git diff --cached "$file"
    echo -e "\n=== END FILE: $file ===\n"
done > /tmp/all_diffs.txt
```

### Step 2: Send to Gemini for Analysis

```bash
gemini -p "@/tmp/all_diffs.txt Analyze these git diffs as a code reviewer.

For EACH file, determine if the changes are an improvement or worsening based on:
- Code quality and readability
- Performance impact
- Maintainability
- Security implications
- Bug risk
- Best practices

Format your response EXACTLY as:

FILE: [filename]
VERDICT: [IMPROVEMENT|WORSENING|NEUTRAL]
REASON: [1-2 sentence explanation]
---

After analyzing all files, provide:

OVERALL ASSESSMENT:
- Total files: [count]
- Improvements: [count]
- Worsenings: [count]
- Net impact: [POSITIVE|NEGATIVE|MIXED]
- Recommendation: [COMMIT|REVIEW_FIRST|DO_NOT_COMMIT]
- Key concerns: [list any critical issues]"
```

### Step 3: Parse and Display Results

Extract verdicts for each file and color-code the output:
- 🟢 IMPROVEMENT (green)
- 🔴 WORSENING (red)
- 🟡 NEUTRAL (yellow)

### Complete One-Liner Version

```bash
{ for f in $files; do echo "=== $f ==="; git diff "$f" || git diff --cached "$f"; done; } | \
gemini -p "$(cat -) 

Rate each file's diff: IMPROVEMENT/WORSENING/NEUTRAL with reason.
Give overall: COMMIT/REVIEW_FIRST/DO_NOT_COMMIT"
```

### Expected Gemini Output Format

```
FILE: src/main.py
VERDICT: IMPROVEMENT
REASON: Refactored complex function into smaller, testable units.
---

FILE: src/utils.py
VERDICT: WORSENING
REASON: Removed error handling that could cause runtime failures.
---

FILE: tests/test_main.py
VERDICT: IMPROVEMENT
REASON: Added comprehensive test coverage for edge cases.
---

OVERALL ASSESSMENT:
- Total files: 3
- Improvements: 2
- Worsenings: 1
- Net impact: POSITIVE
- Recommendation: REVIEW_FIRST
- Key concerns: Missing error handling in utils.py needs attention before commit.
```
</file>

<file path="geminictx.md">
# Command: /geminictx [query]

**Goal:** Leverage a two-pass AI workflow to provide a comprehensive, context-aware answer to a user's query about the codebase. Pass 1 uses Gemini to identify relevant files, and Pass 2 uses your own (Claude's) synthesis capabilities on the full content of those files.

**Usage:**
- `/geminictx "how does authentication work?"`
- `/geminictx "explain the data loading pipeline"`

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**This command follows a deliberate, non-negotiable two-pass workflow:**
1.  **Context Aggregation:** You MUST first run `repomix` to create a complete snapshot of the codebase.
2.  **Pass 1 (Gemini as Context Locator):** You MUST build a structured prompt file and execute `gemini -p` to identify a list of relevant files based on the user's query and the `repomix` context.
3.  **Pass 2 (Claude as Synthesizer):** You MUST then read the full content of EVERY file Gemini identified to build your own deep context before providing a synthesized answer.

**DO NOT:**
-   ❌ Skip the `repomix` step. The entire workflow depends on this complete context.
-   ❌ Guess which files are relevant. You must delegate this to Gemini.
-   ❌ Only read Gemini's one-sentence justifications. You must read the **full file contents**.
-   ❌ Answer the user's query before you have completed Pass 1 and read all identified files in Pass 2.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### Step 1: Gather Codebase Context with Repomix

First, create a comprehensive and reliable context snapshot of the entire project.

```bash
# The user's query is passed as $ARGUMENTS
USER_QUERY="$ARGUMENTS"

# Use repomix for a complete, single-file context snapshot.
# This is more robust than a long list of @-references.
npx repomix@latest . \
  --include "**/*.{js,py,md,sh,json,c,h,log}" \
  --ignore "build/**,node_modules/**,dist/**,*.lock"

# Verify that the context was created successfully.
if [ ! -s ./repomix-output.xml ]; then
    echo "❌ ERROR: Repomix failed to generate the codebase context. Aborting."
    exit 1
fi

echo "✅ Codebase context aggregated into repomix-output.xml."
```

### Step 2: Build and Execute Pass 1 (Gemini as Context Locator)

Now, build a structured prompt in a file to ask Gemini to find the relevant files.

#### Step 2.1: Build the Prompt File
```bash
# Clean start for the prompt file
rm -f ./gemini-pass1-prompt.md 2>/dev/null

# Create the structured prompt using the v3.0 XML pattern
cat > ./gemini-pass1-prompt.md << 'PROMPT'
<task>
You are a **Context Locator**. Your sole purpose is to analyze the provided codebase context and identify the most relevant files for answering the user's query. Do not answer the query yourself.

<steps>
<1>
Analyze the user's `<query>`.
</1>
<2>
Scan the entire `<codebase_context>` to find all files (source code, documentation, configs) that are relevant to the query.
</2>
<3>
For each relevant file you identify, provide your output in the strict format specified in `<output_format>`.
</3>
</steps>

<context>
<query>
[Placeholder for the user's query]
</query>

<codebase_context>
<!-- Placeholder for content from repomix-output.xml -->
</codebase_context>
</context>

<output_format>
Your output must be a list of entries. Each entry MUST follow this exact format, ending with three dashes on a new line.

FILE: [exact/path/to/file.ext]
RELEVANCE: [A concise, one-sentence explanation of why this file is relevant.]
---

Do not include any other text, conversation, or summaries in your response.
</output_format>
</task>
PROMPT
```

#### Step 2.2: Append Dynamic Context
```bash
# Inject the user's query and the repomix context into the prompt file.
# Using a temp file for the query handles special characters safely.
echo "$USER_QUERY" > ./tmp/user_query.txt
sed -i.bak -e '/\[Placeholder for the user.s query\]/r ./tmp/user_query.txt' -e '//d' ./gemini-pass1-prompt.md

# Append the codebase context
echo -e "\n<codebase_context>" >> ./gemini-pass1-prompt.md
cat ./repomix-output.xml >> ./gemini-pass1-prompt.md
echo -e "\n</codebase_context>" >> ./gemini-pass1-prompt.md

echo "✅ Built structured prompt for Pass 1: ./gemini-pass1-prompt.md"
```

#### Step 2.3: Execute Gemini
```bash
# Execute Gemini with the single, clean prompt file.
gemini -p "@./gemini-pass1-prompt.md"
```

### Step 3: Process Gemini's Response & Prepare for Pass 2

After receiving the list of files from Gemini, parse the output and prepare to read the files.

```bash
# [You will receive Gemini's response, e.g., captured in $GEMINI_RESPONSE]
# For this example, we'll simulate parsing the response to get a file list.

# Parse the output to get a clean list of file paths.
# This is a robust way to extract just the file paths for the next step.
FILE_LIST=$(echo "$GEMINI_RESPONSE" | grep '^FILE: ' | sed 's/^FILE: //')

# Verify that Gemini returned relevant files.
if [ -z "$FILE_LIST" ]; then
    echo "⚠️ Gemini did not identify any specific files for your query. I will attempt to answer based on general project knowledge, but the answer may be incomplete."
    # You might choose to exit here or proceed with caution.
    exit 0
fi

echo "Gemini identified the following relevant files:"
echo "$FILE_LIST"
```

### Step 4: Execute Pass 2 (Claude as Synthesizer)

This is your primary role. Read the full content of the identified files to build deep context.

```bash
# Announce what you are doing for transparency.
echo "Now reading the full content of each identified file to build a deep understanding..."

# You will now iterate through the FILE_LIST and read each one.
# For each file in FILE_LIST:
#   - Verify the file exists (e.g., if [ -f "$file" ]; then ...).
#   - Read its full content into your working memory.
#   - Announce: "Reading: `path/to/file.ext`..."

# After reading all files, you are ready to synthesize the answer.
```

### Step 5: Present Your Synthesized Analysis

Your final output to the user should follow the well-structured format from your original prompt.

```markdown
Based on your query, Gemini identified the following key files, which I have now read and analyzed in their entirety:

-   `path/to/relevant/file1.ext`
-   `path/to/relevant/file2.ext`
-   `docs/relevant_guide.md`

Here is a synthesized analysis of how they work together to address your question.

### Summary
[Provide a 2-3 sentence, high-level answer to the user's query based on your comprehensive analysis of the files.]

### Detailed Breakdown

#### **Core Logic in `path/to/relevant/file1.ext`**
[Explain the role of this file. Reference specific functions or classes you have read.]

**Key Code Snippet:**
\`\`\`[language]
[Quote a critical code block from the file that you have read.]
\`\`\`

#### **Workflow Orchestration in `path/to/relevant/file2.ext`**
[Explain how this file uses or connects to the core logic from the first file.]

**Key Code Snippet:**
\`\`\`[language]
[Quote a relevant snippet showing the interaction.]
\`\`\`

### How It All Connects
[Provide a brief narrative explaining the data flow or call chain between the identified components.]

### Conclusion
[End with a concluding thought or a question to guide the user's next step.]
```
</file>

<file path="generate-agent-checklist-v2.md">
# Command: /generate-agent-checklist-gemini

**Goal:** Autonomously generate and execute a plan to ensure every project module has a high-quality, standardized docstring. This involves creating new docstrings where missing and improving existing ones.

**Usage:**
`/generate-agent-checklist-gemini "I want every .py module in the ptycho library to have a public-interface-focused docstring."`

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**YOUR ROLE IS AN AUTONOMOUS ORCHESTRATOR AND FILE MANAGER. YOU DO NOT PERFORM THE ANALYSIS.**
1.  You MUST gather the user's high-level objective from the command arguments.
2.  You MUST run `repomix` to create a complete, fresh snapshot of the codebase context.
3.  You MUST build a structured prompt file (`doc-plan-prompt.md`) to delegate the analysis and planning to Gemini.
4.  You MUST execute `gemini -p "@doc-plan-prompt.md"`.
5.  You MUST parse Gemini's structured response to create the three critical state files: `modules_prioritized.txt`, `dependency_report.txt`, and `docstring_progress.md`.
6.  You MUST then proceed to Phase 2 (Sub-Agent Orchestration) using these Gemini-generated files as the source of truth.

**DO NOT:**
-   ❌ Run `find`, `pydeps`, or any other manual analysis tool. Gemini is now responsible for all of Phase 1.
-   ❌ Create the plan yourself. Your job is to run the process and manage the state.
-   ❌ Proceed to Phase 2 if Gemini's analysis fails or returns an invalid format.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### **Phase 1: Gemini-Powered Strategic Analysis**

*(You will execute these commands directly to have Gemini generate the plan.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 1.A | **Aggregate Codebase Context** | `[ ]` | **Why:** To provide Gemini with a complete and fresh snapshot of the project for accurate analysis. <br> **How:** Execute the following command now. <br> ```bash <br> # The user's high-level objective is in $ARGUMENTS <br> npx repomix@latest . \ <br>   --include "ptycho/**/*.py,**/*.md" \ <br>   --ignore "build/**,node_modules/**,dist/**,*.lock,scripts/**" <br> <br> if [ ! -s ./repomix-output.xml ]; then <br>     echo "❌ ERROR: Repomix failed to generate the codebase context. Aborting." <br>     exit 1 <br> fi <br> echo "✅ Codebase context aggregated into repomix-output.xml." <br> ``` |
| 1.B | **Build Gemini Prompt** | `[ ]` | **Why:** To create a structured, verifiable prompt that delegates the entire analysis task to Gemini. <br> **How:** Execute the following commands now to create `doc-plan-prompt.md`. <br> ```bash <br> # Create the structured prompt for Gemini <br> cat > ./doc-plan-prompt.md << 'PROMPT' <br> <task> <br> You are an expert Staff Engineer. Your task is to analyze an entire codebase and create a prioritized plan for adding module-level docstrings. <br> <br> <steps> <br> <1> <br> Analyze the `<user_objective>` and the full `<codebase_context>`. <br> </1> <br> <2> <br> Identify all target Python modules (`.py` files in the `ptycho/` directory, excluding `__init__.py`). <br> </2> <br> <3> <br> Analyze the dependencies between these modules to determine a safe documentation order. Foundational modules (those with few dependencies) should come first. <br> </3> <br> <4> <br> Generate a prioritized list of modules and a summary of their dependencies, strictly adhering to the format specified in `<output_format>`. <br> </4> <br> </steps> <br> <br> <context> <br> <user_objective> <br> [Placeholder for the user's objective] <br> </user_objective> <br> <br> <codebase_context> <br> <!-- Placeholder for content from repomix-output.xml --> <br> </codebase_context> <br> </context> <br> <br> <output_format> <br> Your entire response must be a single block. Do not include any conversational text. The format is non-negotiable. <br> <br> ---PRIORITIZED_MODULES_START--- <br> [List of module file paths, one per line, sorted from least to most dependent.] <br> ---PRIORITIZED_MODULES_END--- <br> <br> ---DEPENDENCY_REPORT_START--- <br> [A human-readable summary of key dependencies. For example: "ptycho/loader.py depends on ptycho/raw_data.py, ptycho/tf_helper.py"] <br> ---DEPENDENCY_REPORT_END--- <br> </output_format> <br> </task> <br> PROMPT <br> <br> # Inject the user objective and repomix context into the prompt <br> echo "$ARGUMENTS" > ./tmp/user_objective.txt <br> sed -i.bak -e '/\[Placeholder for the user.s objective\]/r ./tmp/user_objective.txt' -e '//d' ./doc-plan-prompt.md <br> <br> echo -e "\n<codebase_context>" >> ./doc-plan-prompt.md <br> cat ./repomix-output.xml >> ./doc-plan-prompt.md <br> echo -e "\n</codebase_context>" >> ./doc-plan-prompt.md <br> <br> echo "✅ Successfully built structured prompt file: ./doc-plan-prompt.md" <br> ``` |
| 1.C | **Execute Gemini Analysis** | `[ ]` | **Why:** To run the analysis and generate the plan. <br> **How:** Execute the `gemini` command now. <br> ```bash <br> # Execute Gemini with the fully-formed prompt file <br> gemini -p "@./doc-plan-prompt.md" <br> ``` |
| 1.D | **Create State Files from Gemini's Output** | `[ ]` | **Why:** To parse Gemini's structured response and create the state files needed for the subsequent execution phases. <br> **How:** You must now parse the response from the previous command and create the three state files. <br> ```bash <br> # Assume Gemini's response is in a variable $GEMINI_RESPONSE <br> <br> # Create modules_prioritized.txt <br> awk '/---PRIORITIZED_MODULES_START---/,/---PRIORITIZED_MODULES_END---/' <<< "$GEMINI_RESPONSE" | sed '1d;$d' > modules_prioritized.txt <br> <br> # Create dependency_report.txt <br> awk '/---DEPENDENCY_REPORT_START---/,/---DEPENDENCY_REPORT_END---/' <<< "$GEMINI_RESPONSE" | sed '1d;$d' > dependency_report.txt <br> <br> # Verify that the files were created <br> if [ ! -s modules_prioritized.txt ]; then <br>     echo "❌ ERROR: Gemini failed to return a prioritized module list. Aborting." <br>     exit 1 <br> fi <br> <br> # Create the progress tracker from the prioritized list <br> echo "# Docstring Progress Tracker" > docstring_progress.md <br> echo "" >> docstring_progress.md <br> while read -r line; do <br>   echo "- [ ] \`$line\`" >> docstring_progress.md <br> done < modules_prioritized.txt <br> <br> echo "✅ Gemini analysis complete. State files created:" <br> echo "  - modules_prioritized.txt" <br> echo "  - dependency_report.txt" <br> echo "  - docstring_progress.md" <br> ``` |

---

### **Phase 2: Sub-Agent Orchestration**

*(You will now begin the execution loop, delegating tasks to sub-agents with the updated, smarter instructions.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 2.A | **Orchestrate Documentation of Each Module** | `[ ]` | **Why:** To process each module independently by delegating to specialized sub-agents. <br> **How:** Begin a loop. For each file path in `modules_prioritized.txt`: <br> 1. **Invoke a new, single-purpose "Authoring Sub-Agent."** <br> 2. Provide it with the updated instructions from the **"Sub-Agent Instructions: Docstring Authoring (v6)"** section. <br> 3. Pass the specific module's file path and the `dependency_report.txt` file as context. <br> 4. After the sub-agent successfully completes, mark the corresponding item as done in `docstring_progress.md` and proceed to the next module in the loop. |

---

### **Phase 3: Final Verification & Commit**

*(You will execute these final steps after the loop in Phase 2 is complete.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 3.A | **Verify All Modules are Documented** | `[ ]` | **Why:** To ensure no modules were missed by the sub-agents. <br> **How:** Run a script that reads `modules_prioritized.txt` and checks that each file now starts with a `"""` docstring. |
| 3.B | **Invoke Verification Sub-Agent** | `[ ]` | **Why:** To ensure docstrings are consistent and architecturally sound. <br> **How:** Invoke a final "Verification Sub-Agent" with the instructions from the **"Sub-Agent Instructions: Final Verification"** section. |
| 3.C | **Run Automated Style Linting** | `[ ]` | **Why:** To enforce a consistent documentation style. <br> **How:** Install and run `pydocstyle`. <br> ```bash <br> pip install pydocstyle && pydocstyle ptycho/ <br> ``` |
| 3.D | **Final Code Commit** | `[ ]` | **Why:** To save the completed documentation work. <br> **How:** Stage all the modified Python files and commit them with a detailed message reflecting the new capability. <br> ```bash <br> git add ptycho/**/*.py <br> git commit -m "docs: Add or improve module-level docstrings via AI agent" -m "Ensures all core library modules have a high-quality, standardized docstring. Creates new docstrings where missing and refactors existing ones to meet project standards." <br> ``` |

---

### **Sub-Agent Instructions: Docstring Authoring (v6)**

*(Orchestrator: You will provide these new, smarter instructions to each sub-agent you invoke in Phase 2.A.)*

**Your Goal:** Ensure the specified Python module has a single, high-quality, developer-focused module-level docstring that adheres to the project's standards. This involves either **creating a new docstring** if one is missing, or **reviewing and improving an existing one**.

**Your Context:**
*   **Target Module:** `<path/to/module.py>` (Its full content is available in the `repomix` context)
*   **Dependency Report:** `ptycho/dependency_report.txt`

**Your Workflow:**

**1. Assessment & Triage:**
   - **Action:** Examine the source code of the Target Module.
   - **Check:** Does a module-level docstring (one that starts the file, using `"""` or `'''`) already exist?

**2.a. If Docstring is Missing (Creation Workflow):**
   - **Analysis:** Perform dependency analysis to define the module's public API and its consumers.
   - **Drafting:** Write a new docstring from scratch, strictly adhering to the **"Hardened Docstring Template"**.
   - **Verification:** Run a script to ensure your new docstring is under the 15% size limit. Refactor for conciseness if needed.
   - **Finalization:** Insert the new docstring at the top of the target file.

**2.b. If Docstring Exists (Review & Refactor Workflow):**
   - **Analysis:** Critically evaluate the existing docstring against the principles, the **"Hardened Docstring Template"**, and the **"Docstring Anti-Patterns"**.
   - **Identify Gaps:** Determine what is missing or incorrect. Does it lack a usage example? Is the architectural role unclear? Does it fail to mention data contracts?
   - **Refactor:** Create a new, improved version of the docstring.
     - You **MUST** preserve any valuable, accurate information from the original.
     - You **MUST** fix all identified gaps and anti-patterns.
     - The final output **MUST** be 100% compliant with the "Hardened Docstring Template," regardless of the original's structure.
   - **Verification & Replacement:** Ensure the refactored docstring meets the 15% size limit, then replace the old docstring in the file with your new, improved version.

---

### **Hardened Docstring Template (for Authoring Sub-Agent)**

*Your docstring must follow the structure and quality of the examples below. Choose the example that best fits the nature of the module you are documenting.*

---
#### **Example 1: For Modules with Complex Logic & Hidden State (e.g., `raw_data.py`)**

```python
"""
Ptychography data ingestion and scan-point grouping.

This module serves as the primary ingestion layer for the PtychoPINN data pipeline.
It is responsible for taking raw ptychographic data and wrapping it in a `RawData` object.
Its most critical function, `generate_grouped_data()`, assembles individual scan
points into physically coherent groups for training.

Architecture Role:
    Raw NPZ file -> raw_data.py (RawData) -> Grouped Data Dict -> loader.py
"""

"""
Public Interface:
    `RawData.generate_grouped_data(N, K=4, nsamples=1, ...)`
        - Purpose: The core function for sampling and grouping scan points.
        - Critical Behavior (Conditional on `params.get('gridsize')`):
            - **If `gridsize == 1`:** Performs simple sequential slicing.
            - **If `gridsize > 1`:** Implements a robust "group-then-sample"
              strategy to avoid spatial bias.
        - Key Parameters:
            - `nsamples` (int): For `gridsize=1`, this is the number of images.
              For `gridsize>1`, this is the number of *groups*.
"""

"""
Workflow Usage Example:
    ```python
    from ptycho.raw_data import RawData
    from ptycho import params

    # 1. Instantiate RawData from a raw NPZ file's contents.
    raw_data = RawData(xcoords=data['xcoords'], ...)

    # 2. Set the external state that controls the module's behavior.
    params.set('gridsize', 2)

    # 3. Generate the grouped data dictionary.
    grouped_data_dict = raw_data.generate_grouped_data(N=64, nsamples=1000)
    ```
"""

"""
Architectural Notes & Dependencies:
- This module has a critical implicit dependency on the global `params.get('gridsize')`
  value, which completely changes its sampling algorithm.
- It automatically creates a cache file (`*.groups_cache.npz`) to accelerate
  subsequent runs.
"""
```

---
#### **Example 2: For Modules Defined by Data/Tensor Transformations (e.g., `tf_helper.py`)**

```python
"""
Low-level TensorFlow operations for ptychographic data manipulation.

This module provides a suite of high-performance, tensor-based functions for
the core computational tasks in the PtychoPINN pipeline, primarily patch
extraction, reassembly, and tensor format conversions. It is a foundational
library used by the data pipeline, model, and evaluation modules.
"""

"""
Key Tensor Formats:
This module defines and converts between three standard data layouts for batches
of ptychographic patches:

- **Grid Format:** `(B, G, G, N, N, 1)`
  - Represents patches organized in their spatial grid structure.
- **Channel Format:** `(B, N, N, G*G)`
  - Stacks patches in the channel dimension. Required for CNN input.
- **Flat Format:** `(B*G*G, N, N, 1)`
  - Each patch is an independent item in the batch.
"""

"""
Public Interface:
    `reassemble_position(obj_tensor, global_offsets, M=10)`
        - **Purpose:** The primary function for stitching patches back into a full
          object image based on their precise, non-uniform scan coordinates.
        - **Algorithm:** Uses a batched shift-and-sum operation with automatic
          memory management for large datasets.
        - **Parameters:**
            - `obj_tensor` (Tensor): Complex patches in `Flat Format`.
            - `global_offsets` (Tensor): The `(y, x)` scan coordinates for each patch.
            - `M` (int): The size of the central region of each patch to use for
              the reassembly, which helps avoid edge artifacts.
"""

"""
Usage Example:
    This example shows the canonical `Grid -> Channel -> Flat -> Reassembly`
    workflow that this module enables.

    ```python
    import ptycho.tf_helper as hh
    import tensorflow as tf

    # 1. Start with data in Grid Format. Shape: (10, 2, 2, 64, 64, 1)
    patch_grid = tf.random.normal((10, 2, 2, 64, 64, 1))
    
    # 2. Convert to Channel Format for a CNN. Shape: (10, 64, 64, 4)
    patch_channels = hh.grid_to_channel(patch_grid)
    
    # ... (model processing) ...

    # 3. Convert to Flat Format for reassembly. Shape: (40, 64, 64, 1)
    patches_flat = hh.channel_to_flat(patch_channels)

    # 4. Reassemble the flat patches into a final image.
    scan_coords = tf.random.uniform((40, 1, 1, 2), maxval=100)
    reconstructed_image = hh.reassemble_position(patches_flat, scan_coords, M=20)
    ```
"""
```

---

### **Docstring Anti-Patterns (To Be Avoided by Sub-Agents)**

Your generated docstrings will be rejected if they contain the following:

*   **Vague Summaries:** Avoid generic phrases like "This module contains helper functions" or "Utilities for data processing." Be specific about its role.
*   **Marketing Language:** Do not use subjective fluff like "critical," "essential," "high-performance," or specific speedup numbers. Instead, explain *how* it is performant (e.g., "Uses a batched algorithm to manage memory").
*   **Implementation Details:** Do not explain the line-by-line logic of the code. Focus on the public contract: what goes in, what comes out, and what it's for.
*   **Isolated Examples:** Do not provide usage examples that are just a single function call with placeholder variables. The example must show a realistic interaction between modules.
*   **Inaccurate Consumer Lists:** Do not guess which modules use this one. The dependency report is the source of truth.

---

### **Sub-Agent Instructions: Final Verification**

*(Orchestrator: You will provide these instructions to the sub-agent you invoke in Phase 3.B.)*

**Your Goal:** To perform a final consistency and architectural accuracy check on all newly created or updated docstrings.

**Your Context:**
*   The list of all documented modules: `modules_prioritized.txt`
*   The full dependency map: `dependency_report.txt`
*   The PtychoPINN architecture understanding from `docs/DEVELOPER_GUIDE.md` and `docs/architecture.md`

**Your Workflow:**
1.  **Read All Docstrings:** Load the module-level docstring from every file listed in `modules_prioritized.txt`.
2.  **Cross-Reference Architecture Claims:** For each docstring:
    *   Verify "primary consumers" claims against actual dependency data in `ptycho/dependency_report.txt`.
    *   Check that architectural role descriptions align with the actual system design.
    *   Validate that workflow examples show realistic integration patterns.
3.  **Identify Inconsistencies:**
    *   Module claims to be used by X, but dependency report shows no such link.
    *   Usage examples show patterns not actually used in the codebase.
    *   Circular or contradictory architectural role descriptions.
    *   Incorrect data flow or integration claims.
4.  **Generate Report:** Create `docstring_consistency_report.md` with:
    *   **Pass/Fail Summary:** Overall assessment.
    *   **Inconsistencies Found:** Specific issues requiring fixes.
    *   **Architecture Accuracy:** Assessment of architectural claims.
    *   **Recommendations:** Suggested improvements for consistency.
5.  **Report Findings:** Return the path to the generated report. The Orchestrator will decide if fixes are needed before proceeding.
</file>

<file path="generate-agent-checklist.md">
# **Runbook: Comprehensive Module Documentation (v5)**

**CRITICAL META-INSTRUCTION: READ THIS FIRST**

You are the top-level orchestrator. Your task is to execute the steps in this runbook **sequentially, in your current context.**

You **MUST NOT** delegate this entire runbook to a single sub-task (e.g., `Task(Document all modules)`). Delegating the entire process will cause a critical loss of state and context, leading to failure. You are the main thread of execution. You will delegate only when explicitly instructed to do so for small, specific sub-tasks (like documenting a single file).

**Your immediate first action is to begin with Phase 1, Task 1.A.**

---

**Your Role and Objective:**
You are the autonomous Orchestrator for this documentation initiative. You will manage the entire project from start to finish by executing the phases below.

**User's Goal:**
> "I want every `.py` module (not counting scripts) to have a docstring <15 percent of the module size that documents the public interface: i.e., gives sufficient information on how that module is used / is to be used in other parts of the code or as a public api."

---

### **State Management Between Steps**

The files you create in Phase 1 (e.g., `modules_prioritized.txt`, `dependency_report.txt`) are the **state** for this entire execution. You must ensure they are available in your working directory for all subsequent steps. Do not lose track of these files.

---

### **Phase 1: Strategic Analysis & Prioritization**

*(You will execute these commands directly.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 1.A | **Generate List of Target Modules** | `[ ]` | **Why:** To create a master list of all modules that require a docstring. <br> **How:** Execute the following command now. <br> ```bash <br> find ptycho -name "*.py" -not -name "__init__.py" > modules_to_document.txt <br> ``` <br> **Verify:** The file `modules_to_document.txt` must exist before proceeding. |
| 1.B | **Generate Dependency Map** | `[ ]` | **Why:** To provide context for prioritization and for the sub-agents. <br> **How:** Execute the following commands now. If `pydeps` is not installed, install it first. <br> ```bash <br> pip install pydeps <br> pydeps ptycho --cluster -o ptycho/dependency_graph.svg <br> pydeps ptycho --no-output --show-deps > ptycho/dependency_report.txt <br> ``` <br> **Verify:** The file `dependency_report.txt` must exist before proceeding. |
| 1.C | **Prioritize Modules by Dependency** | `[ ]` | **Why:** To create an intelligent execution order. Foundational modules (least dependent) must be documented first. <br> **How:** You must now write and execute a script (e.g., Python) that reads `dependency_report.txt` and `modules_to_document.txt` to produce a new, sorted list. <br> **Output:** A new file, `modules_prioritized.txt`. <br> **Example Logic:** <br> ```python <br> # Pseudocode for the script you will write and run now. <br> dependencies = parse_pydeps_report('dependency_report.txt') <br> modules = read_file_lines('modules_to_document.txt') <br> sorted_modules = sorted(modules, key=lambda m: len(dependencies.get(m, []))) <br> write_lines_to_file('modules_prioritized.txt', sorted_modules) <br> ``` <br> **Verify:** The file `modules_prioritized.txt` must exist before proceeding. |
| 1.D | **Create Progress Tracker** | `[ ]` | **Why:** To track the completion status of each sub-agent's task. <br> **How:** Create a new markdown file named `docstring_progress.md` containing the contents of `modules_prioritized.txt` formatted as a checklist. |

---

### **Phase 2: Sub-Agent Orchestration**

*(You will now begin a loop and delegate tasks one by one.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 2.A | **Orchestrate Documentation of Each Module** | `[ ]` | **Why:** To process each module independently by delegating to specialized sub-agents. <br> **How:** Begin a loop. For each file path in `modules_prioritized.txt`: <br> 1. **Invoke a new, single-purpose "Authoring Sub-Agent."** <br> 2. Provide it with the instructions from the **"Sub-Agent Instructions: Docstring Authoring"** section below. <br> 3. Pass the specific module's file path and the `dependency_report.txt` file as context. <br> 4. After the sub-agent successfully completes, mark the corresponding item as done in `docstring_progress.md` and proceed to the next module in the loop. |

---

### **Phase 3: Final Verification & Commit**

*(After the loop in Phase 2 is complete, you will execute these final steps.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 3.A | **Verify All Modules are Documented** | `[ ]` | **Why:** To ensure no modules were missed by the sub-agents. <br> **How:** Run a script that reads `modules_prioritized.txt` and checks that each file now starts with a `"""` docstring. The script must fail if any module is undocumented. |
| 3.B | **Invoke Verification Sub-Agent** | `[ ]` | **Why:** To ensure docstrings are consistent and architecturally sound. <br> **How:** Invoke a final "Verification Sub-Agent" with the instructions from the **"Sub-Agent Instructions: Final Verification"** section below. Pass it the `dependency_report.txt` file as context. You must review its findings and apply any necessary fixes. |
| 3.C | **Run Automated Style Linting** | `[ ]` | **Why:** To enforce a consistent documentation style across the entire project. <br> **How:** Install `pydocstyle` (`pip install pydocstyle`) and run it on the `ptycho` directory. <br> ```bash <br> pydocstyle ptycho/ <br> ``` <br> **Verify:** The command should report no errors. You must fix any reported issues. |
| 3.D | **Final Code Commit** | `[ ]` | **Why:** To save the completed documentation work to the repository. <br> **How:** Stage all the modified Python files and commit them with a detailed message. <br> ```bash <br> git add ptycho/**/*.py <br> git commit -m "docs: Add comprehensive module-level docstrings" -m "Adds public-interface-focused docstrings to all core library modules, following a consistent format with usage examples and architectural context. Docstring size is constrained to <15% of module size." <br> ``` |

---

### **Sub-Agent Instructions: Docstring Authoring (v5)**

*(Orchestrator: You will provide these instructions to each sub-agent you invoke in Phase 2.A.)*

**Your Goal:** Write a single, high-quality, developer-focused module-level docstring for the specified Python module.

**Your Context:**
*   **Target Module:** `<path/to/module.py>`
*   **Dependency Report:** `ptycho/dependency_report.txt`

**Your Guiding Principles (Non-Negotiable):**
1.  **Adapt to the Module's Nature:** You MUST analyze the module and determine its primary characteristic. Is it defined by complex conditional logic (like the `raw_data.py` example) or by its data transformations and tensor shape contracts (like the `tf_helper.py` example)? Your docstring's focus MUST reflect this.
2.  **Data Contracts are King:** If the module's primary purpose is to transform data shapes, you MUST explicitly document the input and output tensor formats and shapes.
3.  **Explain Parameter *Effects*:** For public functions, explain the *effect* of critical parameters on the behavior of the system.
4.  **Realistic Workflow Examples:** Your usage example MUST be a practical, multi-step snippet that shows how the module interacts with its primary consumers and dependencies.

**Your Workflow:**
1.  **Analysis:** Perform the dependency analysis to define the module's exact public API and its consumers. You MUST also scan the target module's source code for any imports from or calls to the legacy `ptycho.params` module. If found, you MUST investigate how this external state alters the module's behavior.
2.  **Drafting:** Write the docstring, strictly adhering to the **"Hardened Docstring Template"** below. You MUST choose the most appropriate style based on the two provided examples and fill out every section. If you identified any hidden dependencies, you MUST document them in the **"Architectural Notes & Dependencies"** section.
3.  **Constraint Verification:** Run a script to ensure your docstring is under the 15% size limit. Refactor for conciseness if it fails.
4.  **Anti-Pattern Review:** Before finalizing, you MUST review the **"Docstring Anti-Patterns"** section below and ensure your docstring does not violate any of them.
5.  **Finalization:** Insert the docstring into the target file.

---

### **Hardened Docstring Template (for Authoring Sub-Agent)**

*Your docstring must follow the structure and quality of the examples below. Choose the example that best fits the nature of the module you are documenting.*

---
#### **Example 1: For Modules with Complex Logic & Hidden State (e.g., `raw_data.py`)**

```python
"""
Ptychography data ingestion and scan-point grouping.

This module serves as the primary ingestion layer for the PtychoPINN data pipeline.
It is responsible for taking raw ptychographic data and wrapping it in a `RawData` object.
Its most critical function, `generate_grouped_data()`, assembles individual scan
points into physically coherent groups for training.

Architecture Role:
    Raw NPZ file -> raw_data.py (RawData) -> Grouped Data Dict -> loader.py
"""

"""
Public Interface:
    `RawData.generate_grouped_data(N, K=4, nsamples=1, ...)`
        - Purpose: The core function for sampling and grouping scan points.
        - Critical Behavior (Conditional on `params.get('gridsize')`):
            - **If `gridsize == 1`:** Performs simple sequential slicing.
            - **If `gridsize > 1`:** Implements a robust "group-then-sample"
              strategy to avoid spatial bias.
        - Key Parameters:
            - `nsamples` (int): For `gridsize=1`, this is the number of images.
              For `gridsize>1`, this is the number of *groups*.
"""

"""
Workflow Usage Example:
    ```python
    from ptycho.raw_data import RawData
    from ptycho import params

    # 1. Instantiate RawData from a raw NPZ file's contents.
    raw_data = RawData(xcoords=data['xcoords'], ...)

    # 2. Set the external state that controls the module's behavior.
    params.set('gridsize', 2)

    # 3. Generate the grouped data dictionary.
    grouped_data_dict = raw_data.generate_grouped_data(N=64, nsamples=1000)
    ```
"""

"""
Architectural Notes & Dependencies:
- This module has a critical implicit dependency on the global `params.get('gridsize')`
  value, which completely changes its sampling algorithm.
- It automatically creates a cache file (`*.groups_cache.npz`) to accelerate
  subsequent runs.
"""
```

---
#### **Example 2: For Modules Defined by Data/Tensor Transformations (e.g., `tf_helper.py`)**

```python
"""
Low-level TensorFlow operations for ptychographic data manipulation.

This module provides a suite of high-performance, tensor-based functions for
the core computational tasks in the PtychoPINN pipeline, primarily patch
extraction, reassembly, and tensor format conversions. It is a foundational
library used by the data pipeline, model, and evaluation modules.
"""

"""
Key Tensor Formats:
This module defines and converts between three standard data layouts for batches
of ptychographic patches:

- **Grid Format:** `(B, G, G, N, N, 1)`
  - Represents patches organized in their spatial grid structure.
- **Channel Format:** `(B, N, N, G*G)`
  - Stacks patches in the channel dimension. Required for CNN input.
- **Flat Format:** `(B*G*G, N, N, 1)`
  - Each patch is an independent item in the batch.
"""

"""
Public Interface:
    `reassemble_position(obj_tensor, global_offsets, M=10)`
        - **Purpose:** The primary function for stitching patches back into a full
          object image based on their precise, non-uniform scan coordinates.
        - **Algorithm:** Uses a batched shift-and-sum operation with automatic
          memory management for large datasets.
        - **Parameters:**
            - `obj_tensor` (Tensor): Complex patches in `Flat Format`.
            - `global_offsets` (Tensor): The `(y, x)` scan coordinates for each patch.
            - `M` (int): The size of the central region of each patch to use for
              the reassembly, which helps avoid edge artifacts.
"""

"""
Usage Example:
    This example shows the canonical `Grid -> Channel -> Flat -> Reassembly`
    workflow that this module enables.

    ```python
    import ptycho.tf_helper as hh
    import tensorflow as tf

    # 1. Start with data in Grid Format. Shape: (10, 2, 2, 64, 64, 1)
    patch_grid = tf.random.normal((10, 2, 2, 64, 64, 1))
    
    # 2. Convert to Channel Format for a CNN. Shape: (10, 64, 64, 4)
    patch_channels = hh.grid_to_channel(patch_grid)
    
    # ... (model processing) ...

    # 3. Convert to Flat Format for reassembly. Shape: (40, 64, 64, 1)
    patches_flat = hh.channel_to_flat(patch_channels)

    # 4. Reassemble the flat patches into a final image.
    scan_coords = tf.random.uniform((40, 1, 1, 2), maxval=100)
    reconstructed_image = hh.reassemble_position(patches_flat, scan_coords, M=20)
    ```
"""
```

---

### **Docstring Anti-Patterns (To Be Avoided by Sub-Agents)**

Your generated docstrings will be rejected if they contain the following:

*   **Vague Summaries:** Avoid generic phrases like "This module contains helper functions" or "Utilities for data processing." Be specific about its role.
*   **Marketing Language:** Do not use subjective fluff like "critical," "essential," "high-performance," or specific speedup numbers. Instead, explain *how* it is performant (e.g., "Uses a batched algorithm to manage memory").
*   **Implementation Details:** Do not explain the line-by-line logic of the code. Focus on the public contract: what goes in, what comes out, and what it's for.
*   **Isolated Examples:** Do not provide usage examples that are just a single function call with placeholder variables. The example must show a realistic interaction between modules.
*   **Inaccurate Consumer Lists:** Do not guess which modules use this one. The dependency report is the source of truth.

---

### **Sub-Agent Instructions: Final Verification**

*(Orchestrator: You will provide these instructions to the sub-agent you invoke in Phase 3.B.)*

**Your Goal:** To perform a final consistency and architectural accuracy check on all newly created docstrings.

**Your Context:**
*   The list of all documented modules: `modules_prioritized.txt`
*   The full dependency map: `ptycho/dependency_report.txt`
*   The PtychoPINN architecture understanding from `docs/DEVELOPER_GUIDE.md` and `docs/architecture.md`

**Your Workflow:**
1.  **Read All Docstrings:** Load the module-level docstring from every file listed in `modules_prioritized.txt`.
2.  **Cross-Reference Architecture Claims:** For each docstring:
    *   Verify "primary consumers" claims against actual dependency data in `ptycho/dependency_report.txt`.
    *   Check that architectural role descriptions align with the actual system design.
    *   Validate that workflow examples show realistic integration patterns.
3.  **Identify Inconsistencies:**
    *   Module claims to be used by X, but dependency report shows no such link.
    *   Usage examples show patterns not actually used in the codebase.
    *   Circular or contradictory architectural role descriptions.
    *   Incorrect data flow or integration claims.
4.  **Generate Report:** Create `docstring_consistency_report.md` with:
    *   **Pass/Fail Summary:** Overall assessment.
    *   **Inconsistencies Found:** Specific issues requiring fixes.
    *   **Architecture Accuracy:** Assessment of architectural claims.
    *   **Recommendations:** Suggested improvements for consistency.
5.  **Report Findings:** Return the path to the generated report. The Orchestrator will decide if fixes are needed before proceeding.
</file>

<file path="generate-doc-context.md">
# Command: /generate-doc-context <maskset-file-or-patterns>

**Goal:** Create an isolated, "documentation-only" view of the codebase in a temporary `git worktree` for AI context priming or architectural review.

**Usage:**
- `/generate-doc-context doc_context.maskset` (using a maskset file)
- `/generate-doc-context 'ptycho/**/*.py' 'scripts/tools/*.py'` (using direct glob patterns)

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**YOUR ROLE IS AN AUTONOMOUS ORCHESTRATOR.**
1.  You MUST parse the maskset (either from a file or from the command arguments).
2.  You MUST create a new, clean `git worktree` in a temporary directory.
3.  You MUST run the `strip_code.py` script inside that worktree with the specified masks.
4.  You MUST report the path to the new worktree and provide instructions for cleanup.

**DO NOT:**
-   ❌ Modify the user's current working directory. All modifications must happen inside the worktree.
-   ❌ Skip the `git worktree` step. Isolation is critical.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### Step 1: Parse Maskset and Define Paths

```bash
# The user's input is in $ARGUMENTS
MASK_INPUT="$ARGUMENTS"
WORKTREE_DIR="doc_context_worktree_$(date +%s)"
MASK_PATTERNS=()

# Check if the input is a file or direct patterns
if [ -f "$MASK_INPUT" ]; then
    echo "Reading glob patterns from maskset file: $MASK_INPUT"
    # Read non-empty, non-comment lines from the file into the array
    while IFS= read -r line; do
        [[ -n "$line" && ! "$line" =~ ^\s*# ]] && MASK_PATTERNS+=("$line")
    done < "$MASK_INPUT"
else
    echo "Using glob patterns provided directly as arguments."
    MASK_PATTERNS=($ARGUMENTS)
fi

if [ ${#MASK_PATTERNS[@]} -eq 0 ]; then
    echo "❌ ERROR: No valid patterns found in the maskset or arguments."
    exit 1
fi

echo "Patterns to be processed:"
printf " - %s\n" "${MASK_PATTERNS[@]}"
```

### Step 2: Create the Isolated Git Worktree

```bash
# Check if the worktree directory already exists
if [ -d "$WORKTREE_DIR" ]; then
    echo "⚠️ Warning: Worktree directory '$WORKTREE_DIR' already exists. Removing it first."
    git worktree remove --force "$WORKTREE_DIR"
fi

echo "Creating a new, clean worktree at: ./$WORKTREE_DIR"
# Create a worktree based on the current HEAD
git worktree add "$WORKTREE_DIR" HEAD

if [ $? -ne 0 ]; then
    echo "❌ ERROR: Failed to create git worktree. Please ensure you are in a git repository."
    exit 1
fi
```

### Step 3: Run the Code Stripping Script Inside the Worktree

```bash
echo "Running the code stripping script inside the worktree..."

# The script to run is at its original location, but we execute it from
# within the worktree directory to ensure paths are resolved correctly.
(cd "$WORKTREE_DIR" && python ../scripts/tools/strip_code.py "${MASK_PATTERNS[@]}")

if [ $? -ne 0 ]; then
    echo "❌ ERROR: The strip_code.py script failed. The worktree may be in a partial state."
    echo "You can inspect it at: ./$WORKTREE_DIR"
    exit 1
fi
```

### Step 4: Report Success and Provide Cleanup Instructions

```bash
echo ""
echo "✅ Success! A documentation-only view of the codebase has been created."
echo ""
echo "You can now inspect the result at:"
echo "  $WORKTREE_DIR"
echo ""
echo "This directory contains a full copy of the repository, but with the code stripped from the files matching your maskset, leaving only the module-level docstrings."
echo ""
echo "---"
echo "🧹 **Cleanup Instructions**"
echo "When you are finished, you can remove the worktree with the following command:"
echo "  git worktree remove --force $WORKTREE_DIR"
echo ""
echo "You can also remove the worktree's administrative files from the .git directory with:"
echo "  git worktree prune"
```
</file>

<file path="implementation-gemini-full.md">
# Command: /implementation-gemini-full <initiative-path>

**Goal:** Autonomously generate a complete, code-aware, phased implementation plan by delegating the analysis and authoring to Gemini, then saving the resulting artifacts to the project structure.

**Usage:**
- `/implementation-gemini-full plans/active/real-time-notifications`

**Prerequisites:**
- An R&D plan (`plan.md`) must exist at the specified `<initiative-path>`.

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**YOUR ROLE IS AN AUTONOMOUS ORCHESTRATOR AND FILE MANAGER.**
1.  You MUST parse the R&D plan from the specified `<initiative-path>/plan.md`.
2.  You MUST run `repomix` to create a complete, fresh snapshot of the codebase context.
3.  You MUST build a structured prompt file (`tmp/impl-prompt.md`) using the XML format.
4.  You MUST execute `gemini -p "@tmp/impl-prompt.md"` to delegate the implementation plan generation.
5.  You MUST save Gemini's response **exactly as provided** to the correct output file.
6.  You MUST update `PROJECT_STATUS.md` with the new phase information.

**DO NOT:**
-   ❌ Modify, interpret, or enhance Gemini's output in any way.
-   ❌ Create the implementation plan yourself. Your job is to run the process.
-   ❌ Skip any step. The workflow is non-negotiable.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### Step 1: Prepare Context from the R&D Plan

```bash
# Parse arguments
INITIATIVE_PATH="$1"
RD_PLAN_PATH="$INITIATIVE_PATH/plan.md"

# Verify the R&D plan exists
if [ ! -f "$RD_PLAN_PATH" ]; then
    echo "❌ ERROR: R&D plan not found at '$RD_PLAN_PATH'."
    echo "Please run /customplan-gemini-full first."
    exit 1
fi

# Read the entire content of the R&D plan.
RD_PLAN_CONTENT=$(cat "$RD_PLAN_PATH")
echo "✅ Successfully loaded R&D plan from '$RD_PLAN_PATH'."
```

### Step 2: Aggregate Codebase Context with Repomix

```bash
# Use repomix for a complete, single-file context snapshot.
npx repomix@latest . \
  --include "**/*.{js,py,md,sh,json,c,h,log,yml,toml}" \
  --ignore "build/**,node_modules/**,dist/**,*.lock,tmp/**"

# Verify that the context was created successfully.
if [ ! -s ./repomix-output.xml ]; then
    echo "❌ ERROR: Repomix failed to generate the codebase context. Aborting."
    exit 1
fi
echo "✅ Codebase context aggregated into repomix-output.xml."
```

### Step 3: MANDATORY - Build the Prompt File

```bash
# Clean start for the prompt file
mkdir -p ./tmp
rm -f ./tmp/impl-prompt.md 2>/dev/null

# Create the structured prompt with placeholders
cat > ./tmp/impl-prompt.md << 'PROMPT'
<task>
You are an expert Lead Software Engineer. Your task is to create a complete, phased implementation plan based on a high-level R&D plan.

Your implementation plan must be deeply informed by an analysis of the provided codebase. You will break the project down into logical, testable phases, and for each phase, you will define the goals, tasks, and potential risks.

<steps>
<1>
Analyze the `<rd_plan_context>` to understand the project's overall objectives, scope, and technical specifications.
</1>
<2>
Thoroughly analyze the entire `<codebase_context>` to identify natural boundaries for phasing, dependencies, existing code patterns, and potential risks. This analysis is critical for creating a safe and effective plan.
</2>
<3>
Generate the complete, phased implementation plan. The plan must strictly adhere to the format specified in `<output_format>`. All sections, especially "Key Modules & APIs" and "Potential Gotchas," must be filled out based on your analysis.
</3>
</steps>

<context>
<rd_plan_context>
[Placeholder for the content of plan.md]
</rd_plan_context>

<codebase_context>
<!-- Placeholder for content from repomix-output.xml -->
</codebase_context>
</context>

<output_format>
Your entire response must be a single Markdown block containing the implementation plan. Do not include any conversational text before or after the plan. The format is non-negotiable.

<!-- ACTIVE IMPLEMENTATION PLAN -->
<!-- DO NOT MISTAKE THIS FOR A TEMPLATE. THIS IS THE OFFICIAL SOURCE OF TRUTH FOR THE PROJECT'S PHASED PLAN. -->

# Phased Implementation Plan

**Project:** [Name from R&D Plan]
**Initiative Path:** `[Initiative Path]`

---
## Git Workflow Information
**Feature Branch:** [Value of current git branch]
**Baseline Branch:** [Value of baseline branch from PROJECT_STATUS.md]
**Baseline Commit Hash:** [Commit hash of baseline branch]
**Last Phase Commit Hash:** [Commit hash of baseline branch]
---

**Created:** [Current Date, e.g., 2025-08-03]
**Core Technologies:** [List of technologies from your analysis]

---

## 📄 **DOCUMENT HIERARCHY**
... (Standard hierarchy section) ...

---

## 🎯 **PHASE-BASED IMPLEMENTATION**

**Overall Goal:** [Synthesize a one-sentence summary from the R&D Plan's objective.]

**Total Estimated Duration:** [Sum of phase estimates, e.g., 3 days]

---

## 📋 **IMPLEMENTATION PHASES**

### **Phase 1: [Descriptive Phase Name]**

**Goal:** [Clear, concise goal for this phase.]
**Deliverable:** [A specific, verifiable artifact, e.g., "A new module `src/core/new_feature.py` with passing unit tests."]
**Estimated Duration:** [e.g., 1 day]

**Key Modules & APIs to Touch:**
- `[path/to/module1.py]`: [Brief reason]
- `[path/to/module2.py]`: [Brief reason]

**Potential Gotchas & Critical Conventions:**
- [A specific, code-aware warning, e.g., "The `offsets_f` tensor in `raw_data.py` stores coordinates in `[y, x]` order."]
- [Another warning, e.g., "The `hh.translate` function expects `[dx, dy]` order. A coordinate swap will be necessary."]

**Implementation Checklist:** `phase_1_checklist.md`
**Success Test:** [A specific command to run to verify completion, e.g., `pytest tests/core/test_new_feature.py` completes with 100% pass rate.]

---

### **Phase 2: [Descriptive Phase Name]**
... (Repeat for all necessary phases) ...

---

### **Final Phase: Validation & Documentation**
... (Standard final phase section) ...

---

## 📊 **PROGRESS TRACKING**
... (Standard progress tracking section) ...

---

## 🚀 **GETTING STARTED**
... (Standard getting started section) ...

---

## ⚠️ **RISK MITIGATION**
... (Standard risk mitigation section) ...

</output_format>
</task>
PROMPT

# Inject the dynamic context
echo "$RD_PLAN_CONTENT" > ./tmp/rd_plan.txt
sed -i.bak -e '/\[Placeholder for the content of plan.md\]/r ./tmp/rd_plan.txt' -e '//d' ./tmp/impl-prompt.md

echo -e "\n<codebase_context>" >> ./tmp/impl-prompt.md
cat ./repomix-output.xml >> ./tmp/impl-prompt.md
echo -e "\n</codebase_context>" >> ./tmp/impl-prompt.md

echo "✅ Successfully built structured prompt file: ./tmp/impl-prompt.md"
```

### Step 4: MANDATORY - Execute Gemini Analysis

```bash
# Execute Gemini with the fully-formed prompt file
GEMINI_RESPONSE=$(gemini -p "@./tmp/impl-prompt.md")
```

### Step 5: Save Implementation Plan and Update Project Status

```bash
# Define the output path
OUTPUT_PATH="$INITIATIVE_PATH/implementation.md"

# Save the plan exactly as received.
echo "$GEMINI_RESPONSE" > "$OUTPUT_PATH"

# Verify the file was saved
if [ ! -s "$OUTPUT_PATH" ]; then
    echo "❌ ERROR: Failed to save Gemini's output to '$OUTPUT_PATH'."
    exit 1
fi
echo "✅ Saved Gemini's implementation plan to: $OUTPUT_PATH"

# Update PROJECT_STATUS.md
echo "Updating PROJECT_STATUS.md with new phase information..."
# (Logic to parse $GEMINI_RESPONSE for phase count, duration, etc., and update PROJECT_STATUS.md would go here)
echo "✅ Updated PROJECT_STATUS.md"

# Announce completion to the user
echo ""
echo "Next step: Run \`/phase-checklist-gemini-full 1 $INITIATIVE_PATH\` to have Gemini create the detailed Phase 1 checklist."
```
</file>

<file path="implementation.md">
# Command: /implementation

**Goal:** Generate and save a phased implementation plan document, including Git state tracking, based on the most recent R&D plan and established decomposition principles. This enhanced version requires the identification of key modules and potential risks for each phase.

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**THIS COMMAND MUST FOLLOW THIS EXACT SEQUENCE:**
1.  You MUST read `PROJECT_STATUS.md` to identify the current initiative.
2.  You MUST read the corresponding `plan.md`.
3.  You MUST execute the Git commands in the "State Persistence Logic" section to capture the feature branch, baseline branch, and baseline commit hash.
4.  You MUST decompose the work into logical phases, following the "PHASE DECOMPOSITION GUIDELINES".
5.  **NEW:** For each phase, you MUST analyze the codebase to identify the **"Key Modules & APIs to Touch"** and any **"Potential Gotchas & Critical Conventions"**.
6.  You MUST generate the `implementation.md` file using the provided template, populating all fields correctly.
7.  You MUST update `PROJECT_STATUS.md` to advance the initiative to Phase 1.

**DO NOT:**
-   ❌ Generate the implementation plan without first capturing the Git state.
-   ❌ Leave the "Key Modules" or "Potential Gotchas" sections empty. This indicates insufficient analysis.
-   ❌ Use complex or untested shell commands.

**EXECUTION CHECKPOINT:** Before saving the `implementation.md` file, you must verify that the `Baseline Commit Hash` field contains a valid Git commit hash and that the new analysis sections are populated.

---

## 🤖 **CONTEXT: YOU ARE CLAUDE CODE**

You are Claude Code, an autonomous command-line tool. You will execute the Git commands and file operations described below directly and without human intervention to create the implementation plan.

---

## 📋 **YOUR EXECUTION WORKFLOW**

### Step 1: Read Context
-   Read `PROJECT_STATUS.md` to get the current initiative path.
-   Read `<path>/plan.md` to understand the project goals and technical details.

### Step 2: 🔴 MANDATORY - Capture Git State
-   Execute the shell commands provided in the "State Persistence Logic" section below to determine the feature branch, baseline branch, and baseline commit hash.

### Step 3: Decompose Work into Phases & Perform Deeper Analysis
-   Analyze the "Core Capabilities" and "Technical Implementation Details" from the `plan.md`.
-   Using the **"PHASE DECOMPOSITION GUIDELINES"** below, break the work down into a sequence of 2-5 logical phases.
-   **For each phase you define, perform a targeted analysis of the codebase to identify and document:**
    1.  The specific files, functions, and classes that will be modified or created.
    2.  Any non-obvious technical details, API contracts, or data conventions that a developer must know to implement the phase correctly (e.g., "the offset tensor uses [y, x] order").

### Step 4: Generate and Save Implementation Plan
-   Generate the full content for the implementation plan using the "IMPLEMENTATION PLAN TEMPLATE" below.
-   Populate all sections, including the new analysis sections for each phase, with the information you gathered.
-   Save the content to `<initiative-path>/implementation.md`.

### Step 5: Update Project Status
-   Update the `PROJECT_STATUS.md` file using the "PROJECT STATUS UPDATE" section as a guide.

### Step 6: Confirm and Present
-   Announce that the implementation plan has been created and the project status has been updated.
-   Present the full content of the generated `implementation.md` for the user's review.

---

## 🔒 **State Persistence Logic (Revised for Robustness)**

You must execute the following shell commands. This robust, sequential approach is tested to work in your environment.

```bash
# 1. Get the current feature branch name
feature_branch=$(git rev-parse --abbrev-ref HEAD)

# 2. Extract baseline branch from PROJECT_STATUS.md
# Format: **Branch:** `feature/name` (baseline: branch-name)
baseline_branch=$(grep "Branch:" PROJECT_STATUS.md | sed 's/.*baseline: \(.*\))/\1/')
if [ -z "$baseline_branch" ]; then
    echo "❌ ERROR: Could not determine baseline branch from PROJECT_STATUS.md"
    exit 1
fi
echo "Baseline branch determined as: $baseline_branch"

# 3. Get the commit hash of that baseline branch
baseline_hash=$(git rev-parse "$baseline_branch")
echo "Baseline commit hash: $baseline_hash"
```

---

## 💡 **PHASE DECOMPOSITION GUIDELINES**

When breaking work into phases, you **MUST** follow these principles:

1.  **Each phase must produce a verifiable deliverable.**
2.  **Phases should be logically independent when possible.**
3.  **Consider natural boundaries in the work:** Data First, Foundation First, Backend then Frontend.
4.  **Size phases appropriately:** Aim for phases that represent approximately 1-2 days of focused work.
5.  **The final phase is always "Validation & Documentation".**

---

## 템플릿 & 가이드라인 (Templates & Guidelines)

### **IMPLEMENTATION PLAN TEMPLATE (Updated)**
*This is the template for the content of `implementation.md`.*
```markdown
<!-- ACTIVE IMPLEMENTATION PLAN -->
<!-- DO NOT MISTAKE THIS FOR A TEMPLATE. THIS IS THE OFFICIAL SOURCE OF TRUTH FOR THE PROJECT'S PHASED PLAN. -->

# Phased Implementation Plan

**Project:** <Name from R&D Plan>
**Initiative Path:** `plans/active/<initiative-name>/`

---
## Git Workflow Information
**Feature Branch:** <Value of $feature_branch from logic above>
**Baseline Branch:** <Value of $baseline_branch from logic above>
**Baseline Commit Hash:** <Value of $baseline_hash from logic above>
**Last Phase Commit Hash:** <Value of $baseline_hash from logic above>
---

**Created:** <Current Date, e.g., 2025-07-20>
**Core Technologies:** Python, NumPy, TensorFlow, scikit-image

---

## 📄 **DOCUMENT HIERARCHY**

This document orchestrates the implementation of the objective defined in the main R&D plan. The full set of documents for this initiative is:

- **`plan.md`** - The high-level R&D Plan
  - **`implementation.md`** - This file - The Phased Implementation Plan
    - `phase_1_checklist.md` - Detailed checklist for Phase 1
    - `phase_2_checklist.md` - Detailed checklist for Phase 2
    - `phase_final_checklist.md` - Checklist for the Final Phase

---

## 🎯 **PHASE-BASED IMPLEMENTATION**

**Overall Goal:** <Synthesize a one-sentence summary from the R&D Plan's objective.>

**Total Estimated Duration:** <Sum of phase estimates, e.g., 3 days>

---

## 📋 **IMPLEMENTATION PHASES**

### **Phase 1: Core Logic Implementation**

**Goal:** To implement the foundational data structures and core algorithms for the new feature.
**Deliverable:** A new module `src/core/new_feature.py` with passing unit tests for all public functions.
**Estimated Duration:** 1 day

**Key Modules & APIs to Touch:**
- `ptycho/raw_data.py`: `get_image_patches`
- `ptycho/tf_helper.py`: `translate`
- `ptycho/config/config.py`: `ModelConfig`

**Potential Gotchas & Critical Conventions:**
- The `offsets_f` tensor in `raw_data.py` stores coordinates in `[y, x]` order.
- The `hh.translate` function in `tf_helper.py` expects its `translations` argument in `[dx, dy]` order.
- A coordinate swap will be necessary to ensure correctness.

**Implementation Checklist:** `phase_1_checklist.md`
**Success Test:** `pytest tests/core/test_new_feature.py` completes with 100% pass rate.

---

### **Phase 2: Integration with Main Application**

**Goal:** To integrate the new core logic into the main application workflow.
**Deliverable:** An updated `src/main.py` that correctly calls the new module and produces the expected output.
**Estimated Duration:** 1 day

**Key Modules & APIs to Touch:**
- `src/main.py`: Main application loop
- `src/config.py`: Add new feature flag

**Potential Gotchas & Critical Conventions:**
- Ensure the new feature flag is `False` by default to maintain backward compatibility.
- The main application expects data in a specific format; a data transformation step may be needed.

**Implementation Checklist:** `phase_2_checklist.md`
**Success Test:** Running `python src/main.py --enable-new-feature --input data.txt` produces a valid output file.

---

### **Final Phase: Validation & Documentation**

**Goal:** Validate the complete implementation, update all relevant documentation, and ensure performance meets requirements.
**Deliverable:** A fully tested and documented feature, ready for production use.
**Estimated Duration:** 1 day

**Key Modules & APIs to Touch:**
- `README.md`: User-facing documentation
- `docs/features.md`: Detailed feature documentation

**Potential Gotchas & Critical Conventions:**
- All public-facing documentation must be updated with clear examples.
- Performance benchmarks must be run and documented before the feature is considered complete.

**Implementation Checklist:** `phase_final_checklist.md`
**Success Test:** All R&D plan success criteria are verified as complete.

---

## 📊 **PROGRESS TRACKING**

### Phase Status:
- [ ] **Phase 1:** Core Logic Implementation - 0% complete
- [ ] **Phase 2:** Integration with Main Application - 0% complete
- [ ] **Final Phase:** Validation & Documentation - 0% complete

**Current Phase:** Phase 1: Core Logic Implementation
**Overall Progress:** ░░░░░░░░░░░░░░░░ 0%

---

## 🚀 **GETTING STARTED**

1.  **Generate Phase 1 Checklist:** Run `/phase-checklist 1` to create the detailed checklist.
2.  **Begin Implementation:** Follow the checklist tasks in order.
3.  **Track Progress:** Update task states in the checklist as you work.
4.  **Request Review:** Run `/complete-phase` when all Phase 1 tasks are done to generate a review request.

---

## ⚠️ **RISK MITIGATION**

**Potential Blockers:**
- **Risk:** The external API dependency might have a lower rate limit than expected.
  - **Mitigation:** Implement client-side caching and exponential backoff for retries.
- **Risk:** The new algorithm may be too computationally expensive.
  - **Mitigation:** Profile the code early in Phase 1 and identify optimization opportunities.

**Rollback Plan:**
- **Git:** Each phase will be a separate, reviewed commit on the feature branch, allowing for easy reverts.
- **Feature Flag:** The `--enable-new-feature` flag allows the new code to be disabled in production if issues arise.
```

### **PROJECT STATUS UPDATE**
*Update these fields in `PROJECT_STATUS.md`.*
```markdown
**Current Phase:** Phase 1: Core Logic Implementation
**Progress:** ░░░░░░░░░░░░░░░░ 0%
**Next Milestone:** A new module `src/core/new_feature.py` with passing unit tests.
**Implementation Plan:** `plans/active/<initiative-name>/implementation.md`
```
</file>

<file path="mkclaudes.md">
for each subdir with .md file(s), split the docs into 1 or more .md files within the same subdir: (1) zero or one file, named CLAUDE.MD, containing ALL information useful to an agent working in a  
subdir; and (2) zero or more other .md files (generally a subset of the original files, with any sections that moved to CLAUDE.MD removed. Not that the new CLAUDE.md, if created, should contain          
sufficient references to the (2)nd set of .mds
the plan should specify that the max allowed CLAUDE.MD size is 500 lines
the plan is to specify that readmes should be left intact (this is an exception to the no-duplication rule)
</file>

<file path="phase-checklist-gemini-full.md">
# Command: /phase-checklist-gemini-full <phase-number> <initiative-path>

**Goal:** Autonomously generate a complete, highly-detailed, code-aware implementation checklist for a specific project phase using Gemini for analysis and generation.

**Usage:**
- `/phase-checklist-gemini-full 2 plans/active/my-project`

**Prerequisites:**
- An `implementation.md` file must exist at the specified `<initiative-path>`.
- The file must contain a clearly defined section for the given `<phase-number>`.

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**YOUR ROLE IS TO ORCHESTRATE, NOT TO AUTHOR.**
1.  You MUST parse the phase information from the specified `implementation.md` file.
2.  You MUST run `repomix` to create a complete, fresh snapshot of the codebase context.
3.  You MUST build a structured prompt file (`tmp/checklist-prompt.md`) using the XML format.
4.  You MUST execute `gemini -p "@tmp/checklist-prompt.md"` to delegate the checklist generation.
5.  You MUST save Gemini's response **exactly as provided** to the correct output file.

**DO NOT:**
-   ❌ Modify, interpret, or add comments to Gemini's output. You are a file manager.
-   ❌ Create the checklist yourself. Your job is to run the process.
-   ❌ Skip any step.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### Step 1: Prepare Context from Local Files

```bash
# Parse arguments
PHASE_NUMBER="$1"
INITIATIVE_PATH="$2"
IMPLEMENTATION_PLAN_PATH="$INITIATIVE_PATH/implementation.md"

# Verify the implementation plan exists
if [ ! -f "$IMPLEMENTATION_PLAN_PATH" ]; then
    echo "❌ ERROR: Implementation plan not found at '$IMPLEMENTATION_PLAN_PATH'."
    exit 1
fi

# Extract the entire section for the specified phase.
PHASE_INFO_CONTENT=$(awk "/### \*\*Phase $PHASE_NUMBER:/{f=1} f && /^### \*\*Phase|## 📊 PROGRESS TRACKING/{if (!/Phase $PHASE_NUMBER/) f=0} f" "$IMPLEMENTATION_PLAN_PATH")

if [ -z "$PHASE_INFO_CONTENT" ]; then
    echo "❌ ERROR: Could not find or extract content for Phase $PHASE_NUMBER in '$IMPLEMENTATION_PLAN_PATH'."
    exit 1
fi
echo "✅ Successfully extracted info for Phase $PHASE_NUMBER."
```

### Step 2: Aggregate Codebase Context with Repomix

```bash
# Use repomix for a complete, single-file context snapshot.
npx repomix@latest . \
  --include "**/*.{js,py,md,sh,json,c,h,log,yml,toml}" \
  --ignore "build/**,node_modules/**,dist/**,*.lock,tmp/**"

if [ ! -s ./repomix-output.xml ]; then
    echo "❌ ERROR: Repomix failed to generate the codebase context. Aborting."
    exit 1
fi
echo "✅ Codebase context aggregated into repomix-output.xml."
```

### Step 3: MANDATORY - Build the Prompt File

```bash
# Clean start for the prompt file
mkdir -p ./tmp
rm -f ./tmp/checklist-prompt.md 2>/dev/null

# Create the structured prompt
cat > ./tmp/checklist-prompt.md << 'PROMPT'
<task>
You are an expert software engineer and project manager. Your task is to create a complete, ultra-detailed, step-by-step implementation checklist for a given project phase.

The checklist must be so detailed that a developer can execute it by copying and pasting code and commands directly. You must analyze the provided codebase context to inform your guidance, referencing existing patterns, APIs, and potential gotchas.

<steps>
<1>
Analyze the `<phase_info>` to understand the goals, deliverables, and critical context (especially "Potential Gotchas") for this phase.
</1>
<2>
Thoroughly analyze the entire `<codebase_context>` to find the exact files, functions, and code patterns relevant to the phase goal.
</2>
<3>
Generate the complete checklist. The final output must strictly adhere to the Markdown table format specified in `<output_format>`. The "How/Why & API Guidance" column is the most important part and must contain specific, actionable details.
</3>
</steps>

<context>
<phase_info>
[Placeholder for the Phase N section from implementation.md]
</phase_info>

<codebase_context>
<!-- Placeholder for content from repomix-output.xml -->
</codebase_context>
</context>

<output_format>
Your entire response must be a single Markdown block containing the checklist. Do not include any conversational text before or after the checklist. The format is non-negotiable.

# Phase [N]: [Phase Name] Checklist

**Initiative:** [Initiative Name]
**Created:** [Current Date]
**Phase Goal:** [Goal from phase_info]
**Deliverable:** [Deliverable from phase_info]

## ✅ Task List

### Instructions:
1.  Work through tasks in order. Dependencies are noted in the guidance column.
2.  The **"How/Why & API Guidance"** column contains all necessary details for implementation.
3.  Update the `State` column as you progress: `[ ]` (Open) -> `[P]` (In Progress) -> `[D]` (Done).

---

| ID  | Task Description                                   | State | How/Why & API Guidance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            -
| :-- | :------------------------------------------------- | :---- | :-------------------------------------------------
| **Section 0: Preparation & Analysis**
| 0.A | **Review Critical Context**                        | `[ ]` | **Why:** To prevent common errors by understanding the specific challenges of this phase. <br> **Action:** [Gemini: Based on the 'Potential Gotchas' section of the phase_info, write a specific instruction here. e.g., "Carefully review the coordinate system mismatch between `raw_data.py` and `tf_helper.py`."]
| 0.B | **Analyze Source Code**                            | `[ ]` | **Why:** To understand the existing code before modification. <br> **Action:** [Gemini: Based on the 'Key Modules & APIs' section, list the exact files to be read. e.g., "Open and read `ptycho/raw_data.py` and `ptycho/tf_helper.py`."]
| **Section 1: Implementation Tasks**
| 1.A | **Implement: [Task 1 from phase_info]**            | `[ ]` | **Why:** [Gemini: Synthesize the reason from the phase goal.] <br> **File:** `[Gemini: Identify the exact file path]` <br> **API Guidance:** [Gemini: Provide a specific, actionable code snippet or API call, incorporating any necessary fixes for the 'Potential Gotchas'. e.g., "**CRITICAL:** You must swap the coordinates... Use this code: `offsets_xy = tf.gather(offsets_yx, [1, 0], axis=1)`"]
| ... | ...                                                | ...   | ...
| **Section 2: Testing & Validation**
| 2.A | **Write Unit/Integration Tests**                   | `[ ]` | **Why:** To verify the new implementation is correct. <br> **File:** `[Gemini: Suggest a new or existing test file]` <br> **Guidance:** [Gemini: Describe the specific test cases needed to validate the changes and the success criteria from the phase_info.]
| ... | ...                                                | ...   | ...

---

## 🎯 Success Criteria
... (Standard success criteria section) ...

</output_format>
</task>
PROMPT

# Inject the dynamic context
echo "$PHASE_INFO_CONTENT" > ./tmp/phase_info.txt
sed -i.bak -e '/\[Placeholder for the Phase N section from implementation.md\]/r ./tmp/phase_info.txt' -e '//d' ./tmp/checklist-prompt.md

echo -e "\n<codebase_context>" >> ./tmp/checklist-prompt.md
cat ./repomix-output.xml >> ./tmp/checklist-prompt.md
echo -e "\n</codebase_context>" >> ./tmp/checklist-prompt.md

echo "✅ Successfully built structured prompt file: ./tmp/checklist-prompt.md"
```

### Step 4: MANDATORY - Execute Gemini Analysis

```bash
# Execute Gemini with the fully-formed prompt file
GEMINI_RESPONSE=$(gemini -p "@./tmp/checklist-prompt.md")
```

### Step 5: Save Gemini's Checklist

```bash
# Define the output path
OUTPUT_PATH="$INITIATIVE_PATH/phase_${PHASE_NUMBER}_checklist.md"

# Save the checklist exactly as received.
echo "$GEMINI_RESPONSE" > "$OUTPUT_PATH"

# Verify the file was saved
if [ ! -s "$OUTPUT_PATH" ]; then
    echo "❌ ERROR: Failed to save Gemini's output to '$OUTPUT_PATH'."
    exit 1
fi

# Announce completion to the user
echo "✅ Saved Gemini's complete Phase $PHASE_NUMBER checklist to: $OUTPUT_PATH"
echo ""
echo "The checklist is ready for execution and contains highly detailed, code-aware tasks."
</file>

<file path="phase-checklist.md">
# Command: /phase-checklist <phase-number>

**Goal:** Generate a detailed, context-aware checklist for a specific phase by parsing the implementation plan and providing actionable guidance.

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**THIS COMMAND MUST FOLLOW THIS EXACT SEQUENCE:**
1.  You MUST parse the phase number from the command arguments.
2.  You MUST read `PROJECT_STATUS.md` to identify the current initiative path.
3.  You MUST read the corresponding `<path>/implementation.md` file.
4.  You MUST extract the **Goal**, **Deliverable**, **Key Tasks**, **Key Modules & APIs**, and **Potential Gotchas** for the specified phase number.
5.  You MUST use this extracted information to populate the new, enhanced checklist template.
6.  You MUST save the generated checklist to `<path>/phase_<n>_checklist.md`.
7.  You MUST present the saved file path and its full content to the user.

**DO NOT:**
-   ❌ Generate a generic checklist. You must use the specific context from `implementation.md`.
-   ❌ Leave the "Critical Context" section empty. If the `implementation.md` is missing this information, you must report an error.
-   ❌ Execute any of the tasks in the checklist you generate. Your role is to create the plan, not execute it.

---

## 🤖 **CONTEXT: YOU ARE CLAUDE CODE**

You are Claude Code, an autonomous command-line tool. You will execute the file operations described below to create a detailed and context-aware checklist for the user or another AI agent to follow.

---

## 📋 **YOUR EXECUTION WORKFLOW**

### Step 1: Parse Arguments and Read Context
-   Parse the `<phase-number>` from the command.
-   Read `PROJECT_STATUS.md` to get the current initiative path.
-   Read `<path>/implementation.md`.

### Step 2: Extract Phase-Specific Information
-   Using a robust parsing method (like `awk` or a Python script), extract all the relevant sections for the specified phase from `implementation.md`:
    -   Phase Name, Goal, Deliverable
    -   The list of "Key Tasks"
    -   The content of "Key Modules & APIs to Touch"
    -   The content of "Potential Gotchas & Critical Conventions"

### Step 3: Generate and Save the Checklist
-   Generate the full content for the checklist using the "ENHANCED CHECKLIST TEMPLATE" below.
-   Populate all sections with the information you extracted in Step 2.
-   For the main task list, create a row for each "Key Task" you extracted and use the context to generate specific, actionable prompts in the "How/Why & API Guidance" column.
-   Save the final content to `<initiative-path>/phase_<phase-number>_checklist.md`.

### Step 4: Confirm and Present
-   Announce that the detailed checklist has been created.
-   Present the full content of the generated checklist file for the user's review.

---

## 템플릿 & 가이드라인 (Templates & Guidelines)

### **ENHANCED CHECKLIST TEMPLATE (for non-Gemini `/phase-checklist`)**
*This is the template for the content of `phase_<n>_checklist.md`.*
```markdown
# Phase <N>: <Phase Name> Checklist

**Initiative:** <Initiative name from plan>
**Created:** <Today's date YYYY-MM-DD>
**Phase Goal:** <Extracted from implementation.md>
**Deliverable:** <Extracted from implementation.md>

---
## 🧠 **Critical Context for This Phase**

**Key Modules & APIs Involved:**
<*Content from "Key Modules & APIs" in implementation.md is inserted here*>

**⚠️ Potential Gotchas & Conventions to Respect:**
<*Content from "Potential Gotchas" in implementation.md is inserted here*>
---

## ✅ Task List

### Instructions:
1.  Work through tasks in order. Dependencies are noted in the guidance column.
2.  The **"How/Why & API Guidance"** column contains all necessary details for implementation.
3.  Update the `State` column as you progress: `[ ]` (Open) -> `[P]` (In Progress) -> `[D]` (Done).

---

| ID  | Task Description                                   | State | How/Why & API Guidance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       -
| :-- | :------------------------------------------------- | :---- | :-------------------------------------------------
| **Section 0: Preparation & Analysis**
| 0.A | **Review Critical Context**                        | `[ ]` | **Why:** To prevent common errors by understanding the specific challenges of this phase. <br> **Action:** Carefully read the "Critical Context for This Phase" section above. Acknowledge that you understand the potential gotchas before proceeding.
| 0.B | **Analyze Source Code**                            | `[ ]` | **Why:** To understand the existing code before modification. <br> **Action:** Open and read the files listed in the "Key Modules & APIs" section. Pay close attention to the function signatures, data flow, and any existing comments.
| **Section 1: Implementation Tasks**
| 1.A | **Implement: <Task 1 from implementation.md>**     | `[ ]` | **Why:** [Agent fills in based on goal] <br> **File:** `[Agent fills in based on context]` <br> **API Guidance:** **[Agent: Based on your analysis of the source code and the gotchas, what is the exact code snippet or API call needed here? Be specific.]**
| 1.B | **Implement: <Task 2 from implementation.md>**     | `[ ]` | **Why:** [Agent fills in based on goal] <br> **File:** `[Agent fills in based on context]` <br> **API Guidance:** **[Agent: Based on your analysis of the source code and the gotchas, what is the exact code snippet or API call needed here? Be specific.]**
| ... | ...                                                | ...   | ...
| **Section 2: Testing & Validation**
| 2.A | **Write Unit/Integration Tests**                   | `[ ]` | **Why:** To verify the new implementation is correct and does not introduce regressions. <br> **File:** `[Agent: Suggest a new or existing test file]` <br> **Guidance:** Write tests that specifically cover the changes made in this phase. Refer to the "Success Test" for this phase in `implementation.md`.
| 2.B | **Run All Tests**                                  | `[ ]` | **Why:** To confirm the changes are working and have not broken other parts of the application. <br> **Command:** `python -m unittest discover -s tests -p "test_*.py"` <br> **Verify:** All tests must pass.
| **Section 3: Finalization**
| 3.A | **Code Formatting & Linting**                      | `[ ]` | **Why:** To maintain code quality and project standards. <br> **How:** Review code for consistent indentation, remove any debug prints, ensure proper docstrings for new functions.
| 3.B | **Update Function Docstrings**                     | `[ ]` | **Why:** To document new parameters and functionality. <br> **How:** Update docstrings for any modified functions to reflect the changes made in this phase.

---

## 🎯 Success Criteria

**This phase is complete when:**
1.  All tasks in the table above are marked `[D]` (Done).
2.  The phase success test passes: `<specific command from implementation.md>`
3.  No regressions are introduced in the existing test suite.
</file>

<file path="priming.md">
### **Agent Context Priming 

You are an expert AI software engineer assigned to the PtychoPINN project. Your context has been pre-loaded with the project's master directives file, `CLAUDE.md`.

Your primary directive is to **never execute a task directly**. You must always follow a structured **"Analyze -> Plan -> Execute"** workflow.

### Your Current Objective

You have been given a specific task by the user.

> **User's Request:**
> `$ARGUMENTS`

### **Your Immediate and Only Task**

Your **only** task right now is to produce a **"Task Analysis and Action Plan"**. You are forbidden from executing any code or modifying any files until this plan has been presented and approved.

To create this plan, you must follow these steps precisely:

**Step 1: Keyword Extraction**
-   Analyze the user's request and extract a list of key technical terms and concepts (e.g., "generalization study," "experimental dataset," "evaluation").

**Step 2: Documentation Discovery**
-   Using your keyword list, search your pre-loaded context (`CLAUDE.md`) to find relevant `<doc-ref>` and `<code-ref>` tags.
-   Read the documents referenced by these tags.
-   If necessary, use `grep` to find your keywords in the `docs/` and `scripts/` directories to discover unlinked but relevant documentation.

**Step 3: Plan Formulation**
-   Based on your research from Step 2, formulate a step-by-step action plan.
-   Your plan **must** begin with a "Relevant Documentation" section that lists the files you read to inform your plan.
-   If your plan involves modifying a script, you **must** include a `diff` of the proposed changes.
-   If your plan involves executing a command, you **must** state the exact command you intend to run.

**Step 4: Output the Plan**
-   Present the complete "Task Analysis and Action Plan" to the user.
-   **STOP** and await approval. Do not proceed further.

---

### **Example of a Correct Output**

Your output for this turn should look like this:

> **Task Analysis and Action Plan**
>
> **Objective:** Run the complete generalization study using a new experimental dataset.
>
> **Relevant Documentation:**
> 1.  `CLAUDE.md`
> 2.  `scripts/studies/QUICK_REFERENCE.md`
> 3.  `scripts/studies/run_complete_generalization_study.sh` (source code)
>
> **Analysis:**
> The user wants to run the main generalization study workflow but substitute their own dataset. The primary tool, `run_complete_generalization_study.sh`, has a `--test-data` flag but lacks a `--train-data` flag, making this impossible without modification.
>
> **Action Plan:**
> 1.  **Modify Script:** I will edit `scripts/studies/run_complete_generalization_study.sh` to add a `--train-data` argument.
>     ```diff
>     --- a/scripts/studies/run_complete_generalization_study.sh
>     +++ b/scripts/studies/run_complete_generalization_study.sh
>     ... (diff content) ...
>     ```
> 2.  **Execute Study:** After the script is modified, I will run the following command:
>     ```bash
>     ./scripts/studies/run_complete_generalization_study.sh \
>         --skip-data-prep \
>         --train-data "datasets/fly64/fly001_64_train.npz" \
>         --test-data "datasets/fly64/fly001_64_train.npz" \
>         --output-dir "fly64_generalization_study"
>     ```
>
> I will now await your approval before proceeding.
</file>

<file path="pyrefly-fix.md">
# Command: /pyrefly-fix-local <target-directory>

**Goal:** Autonomously run `pyrefly check` on a specified directory, analyze the reported errors, and fix each identified issue sequentially.

**Usage:**
- `/pyrefly-fix-local src/nanobrag_torch/models/`
- `/pyrefly-fix-local tests/`

---

## 🔴 **CRITICAL: MANDATORY EXECUTION FLOW**

**YOUR ROLE IS AN AUTONOMOUS DEVELOPER.**
1.  You MUST run `pyrefly check` on the user-provided target directory and capture its output.
2.  You MUST parse the `pyrefly` output to create a structured list of errors to fix.
3.  You MUST enter a loop, fixing **one error at a time**.
4.  After each fix, you MUST run `pyrefly check` again on the *modified file* to verify that the specific error was resolved and no new errors were introduced. If the fix is not perfect, you must revert it and try again.
5.  After the loop completes, you MUST run `pyrefly check` a final time on the entire target directory to confirm all issues are resolved.
6.  You MUST then commit the changes with a detailed message.

**DO NOT:**
-   ❌ Attempt to fix all errors at once. You must work sequentially, one error at a time.
-   ❌ Commit any changes until the final verification scan passes with zero errors.
-   ❌ Modify any code unrelated to the specific error you are currently fixing.

---

## 🤖 **YOUR EXECUTION WORKFLOW**

### **Phase 1: Analysis & Task Generation**

*(You will execute these commands directly to analyze the code and generate the task list.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 1.A | **Run `pyrefly check`** | `[ ]` | **Why:** To get the ground-truth list of all static analysis errors that need to be fixed. <br> **How:** Execute the following command now. <br> ```bash <br> # The user's target directory is in $ARGUMENTS <br> TARGET_DIR="$ARGUMENTS" <br> <br> # Ensure the target directory exists <br> if [ ! -d "$TARGET_DIR" ]; then <br>     echo "❌ ERROR: Target directory '$TARGET_DIR' not found. Aborting." <br>     exit 1 <br> fi <br> <br> # Run pyrefly and save the output to a file <br> echo "Running pyrefly check on '$TARGET_DIR'..." <br> pyrefly check "$TARGET_DIR" > ./tmp/pyrefly_errors.log <br> <br> # Check if there were any errors <br> if [ ! -s ./tmp/pyrefly_errors.log ]; then <br>     echo "✅ No pyrefly errors found in '$TARGET_DIR'. Nothing to do." <br>     exit 0 <br> fi <br> <br> echo "✅ Found pyrefly errors. Report saved to ./tmp/pyrefly_errors.log." <br> ``` |
| 1.B | **Parse Errors into a Task List** | `[ ]` | **Why:** To create a structured, machine-readable list of tasks for the orchestration loop. <br> **How:** Write and execute a script (e.g., Python) to parse `pyrefly_errors.log` into a JSON file `tmp/pyrefly_tasks.json`. Each entry should contain the file path, line number, error code, and error message. <br> **Example JSON entry:** <br> ```json <br> { <br>   "file_path": "src/nanobrag_torch/models/crystal.py", <br>   "line_number": 38, <br>   "error_code": "bad-function-definition", <br>   "error_message": "Default `None` is not assignable to parameter `config` with type `CrystalConfig`" <br> } <br> ``` |

---

### **Phase 2: Sequential Fixing Loop**

*(You will now begin the execution loop, fixing one error at a time.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 2.A | **Iterate Through Each Error** | `[ ]` | **Why:** To process each error independently, ensuring each fix is correct and isolated. <br> **How:** Begin a loop. For each error object in `tmp/pyrefly_tasks.json`: <br> 1. **Read the File:** Read the content of the `file_path` for the current error. <br> 2. **Formulate the Fix:** Based on the `error_message` and `line_number`, determine the minimal code change required. <br> 3. **Apply the Fix:** Modify the file with your change. <br> 4. **Verify the Fix:** Run `pyrefly check` **only on the modified file**. Check that the original error is gone and no new errors have appeared in that file. <br> 5. **Revert if Necessary:** If the verification fails, revert the changes to that file (`git checkout -- <file_path>`) and log the failure before moving to the next error. |

---

### **Phase 3: Final Verification & Commit**

*(You will execute these final steps after the loop in Phase 2 is complete.)*

| ID | Task Description | State | How/Why & API Guidance |
| :-- | :--- | :--- | :--- |
| 3.A | **Run Final Verification Scan** | `[ ]` | **Why:** To ensure that all fixes have been applied correctly and no new errors were introduced across the entire target directory. <br> **How:** Run `pyrefly check` one last time on the original target directory. <br> ```bash <br> echo "Running final verification scan on '$TARGET_DIR'..." <br> pyrefly check "$TARGET_DIR" > ./tmp/pyrefly_final_check.log <br> <br> if [ -s ./tmp/pyrefly_final_check.log ]; then <br>     echo "❌ ERROR: Final verification failed. Some errors remain:" <br>     cat ./tmp/pyrefly_final_check.log <br>     echo "Please review the failed fixes and remaining errors." <br>     exit 1 <br> fi <br> echo "✅ Final verification passed. All pyrefly errors have been resolved." <br> ``` |
| 3.B | **Final Code Commit** | `[ ]` | **Why:** To save the completed refactoring work. <br> **How:** Stage all the modified Python files and commit them with a detailed message. <br> ```bash <br> git add "$TARGET_DIR/**/*.py" <br> git commit -m "refactor(typing): Resolve pyrefly static analysis errors" -m "Ran pyrefly check and fixed all identified type safety and code quality issues in the '$TARGET_DIR' directory. This improves code correctness and maintainability." <br> ``` |
</file>

<file path="taskctx.md">
Run this shell command:

`gemini -p "<user query> $ARGUMENTS </user query> <task> The user wants context information about the <user query>:  First, review the project documentation, starting from core docs such as CLAUDE.md and then expanding to the others. Then, review each file (including documentation, code and configuration) and for each, think about whether or not that file is relevant to the user task. Second, for each relevant file that you identified in the previous step, think about why that file is relevant and quote any key sections. Return your answer in the form of <return format> a list of files with a justification for each included file and with quotes of key sections that you identified </return format>.</task> "`

Do not do any analysis until AFTER reading the output of the shell command.
The command will return file paths and additional text, per <return format>. Given the <user query>, read each of those files. 
Then, return a literal quote of the shell command output, followed by your own analysis of the <user query>.
</file>

<file path="update-docs.md">
# Command: /update-docs

**Goal:** Review the most recent code changes and update all relevant project documentation to reflect them, incorporating user-provided suggestions.

**Usage:** `/update-docs [optional suggested edits]`

**Example:**
`/update-docs The main change was adding the --n-train-images flag to run_comparison.sh. We should update its docstring and also create a new README in the scripts/studies/ directory to explain the new generalization study workflow.`

**User's suggested changes and edits:**
$ARGUMENTS

---
## 🚀 **EXECUTION STRATEGY**

**As the AI agent, follow this documentation update checklist precisely:**

1.  **Identify Recent Changes & User Suggestions:**
    *   First, check if the user provided any suggestions above (these came from the `/update-docs` command arguments).
    *   If no arguments were provided, review the most recent code changes or ask the user: "What specific feature or change do I need to document?"
2.  **Analyze Impact:** Based on the recent changes and the user's suggestions, analyze which parts of the project are affected.
3.  **Systematically Review and Update Documentation:** Go through the checklist in the "DOCUMENTATION REVIEW CHECKLIST" section below. For each item, check if an update is needed, paying close attention to the user's suggestions. If an update is needed, perform it. If not, state that you've checked it and no changes were required.

---

## **DOCUMENTATION REVIEW CHECKLIST**

### **1. High-Level Project Guides**

*   [ ] **`README.md` (Root Level):**
    *   **Check:** Does the new feature change the primary installation or usage instructions? Is it a major new capability that should be highlighted in the overview?
    *   **Action (if needed):** Update the "Usage" or "Features" sections.

*   [ ] **`docs/DEVELOPER_GUIDE.md`:**
    *   **Check:** Does the change introduce a new architectural principle, a critical workflow, or a lesson learned (an "anti-pattern")? Does it affect the data pipeline or evaluation methods?
    *   **Action (if needed):** Add a new section or update an existing one to reflect the new best practices or architectural components.

*   [ ] **`CLAUDE.md`:**
    *   **Check:** Does the change introduce a new core directive for the AI? Does it change how the AI should interact with the codebase or data formats?
    *   **Action (if needed):** Add or update a directive. _e.g., "You MUST now use the `new_function()` for all evaluations."_

### **2. Tool and Script Documentation**

*   [ ] **Script `README.md` Files:**
    *   **Check:** Was a new script directory created (e.g., `scripts/studies/`)? Does it have a `README.md` explaining its purpose and workflow?
    *   **Action (if needed):** Create or update the `README.md` for the relevant directory (e.g., `scripts/tools/README.md`, `scripts/studies/README.md`). Provide clear usage examples.

*   [ ] **Shell Script Header Comments:**
    *   **Check:** Were any shell scripts (`.sh`) created or modified?
    *   **Action (if needed):** Update the header comments in the script to explain its purpose, arguments, and provide up-to-date usage examples.

### **3. Code-Level Documentation (Docstrings)**

*   [ ] **Python Module Docstrings:**
    *   **Check:** Were any new Python files (`.py`) created?
    *   **Action (if needed):** Add a module-level docstring at the top of the new file explaining its overall purpose and the tools it provides.

*   [ ] **Function and Class Docstrings:**
    *   **Check:** Were any new public functions or classes added or modified?
    *   **Action (if needed):** Add or update the docstrings to clearly explain the purpose, arguments (`Args:`), return values (`Returns:`), and any errors raised (`Raises:`). Include a simple usage example if helpful.

### **4. Data Format Documentation**

*   [ ] **`docs/data_contracts.md`:**
    *   **Check:** Did the changes introduce a new data format, add/remove keys from an NPZ file, or change the shape/type of any arrays?
    *   **Action (if needed):** Update the data contracts document to reflect the new canonical format. This is a critical step.

### **5. Final Review**

*   [ ] **Consistency Check:**
    *   **Check:** Read through all the changes you've made. Is the terminology consistent? Do the examples in different documents align with each other?
    *   **Action (if needed):** Correct any inconsistencies.
</file>

</files>
