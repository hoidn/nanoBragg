### Summary of Adaptations for Differentiability

| Feature | C Implementation (Non-Differentiable) | PyTorch Implementation (Differentiable) | Rationale for Change |
| :--- | :--- | :--- | :--- |
| **Parameter Handling** | Scalar variables (`double a;`) are used directly in calculations. They are static values with no concept of a computational history. | Parameters intended for optimization are defined as tensors with `requires_grad=True` (`self.cell_a = torch.tensor(..., requires_grad=True)`). | This is the fundamental requirement for PyTorch's autograd system to track operations on a parameter and compute gradients with respect to it. |
| **Structure Factor Lookup** | **Discrete Array Indexing:** `F_cell` is found by rounding fractional `h,k,l` to the nearest integers (`h0,k0,l0`) and performing a direct, non-differentiable array lookup: `Fhkl[h0][k0][l0]`. | **Differentiable Interpolation:** `F_cell` is calculated as a smooth function of the *fractional* `h,k,l` values using differentiable interpolation methods (e.g., `torch.nn.functional.grid_sample`) based on the surrounding integer points in the HKL grid. | A discrete lookup has a zero derivative almost everywhere, which provides no useful gradient for optimization. Interpolation creates a smooth, continuous function, allowing meaningful gradients to flow from the loss back to `h,k,l` and thus to the underlying crystal parameters. **This is the most critical change for scientific utility.** |
| **Conditional Logic** | `if/else` statements are used to handle special cases, such as when a denominator is zero (`if (x == 0) ... else ...`). | Conditional logic is implemented using differentiable operators like `torch.where(condition, x, y)`. | Standard `if/else` statements create "dead ends" in the computational graph. `torch.where` computes both branches but selects the output based on the condition, ensuring a continuous gradient path is maintained for both possibilities. |
| **State Management** | **Pre-computation and Storage:** Derived values (e.g., `a_star` from `cell_a`) are calculated once at the start and stored in variables for later use. | **On-the-Fly Calculation:** Derived values must be re-calculated from their base parameters *inside the forward pass* as part of the differentiable graph. They cannot be stored as simple pre-computed attributes if their inputs require gradients. | Pre-computing and storing a derived value "detaches" it from its original inputs in the computational graph, breaking the path for gradients. Re-calculating it during the forward pass ensures the entire sequence of operations is tracked by autograd. |
| **Data Modification** | **In-place Operations:** Functions frequently modify their inputs directly via pointers for memory efficiency (e.g., `unitize(vector, vector)`). | **Functional Programming:** Operations return a *new* tensor instead of modifying an existing one (e.g., `new_vector = unitize(vector)`). | PyTorch's autograd system can fail or produce incorrect results if a tensor that requires a gradient is modified in-place. The functional approach of creating a new output tensor for each operation is required to correctly build the computational graph. |
| **Looping vs. Vectorization** | The algorithm is built on nested `for` loops that iterate over pixels, sources, mosaic domains, etc., accumulating a sum. | All loops are replaced by **tensor broadcasting**. A single, vectorized operation is performed across expanded tensor dimensions, and `torch.sum()` is used at the end to perform the integration. | While not strictly a differentiability requirement, this is the core architectural change that enables PyTorch's autograd to work efficiently on the entire problem at once, rather than trying to differentiate through a complex, stateful loop. |
