{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Parameter Refinement with nanoBragg PyTorch\n",
    "\n",
    "This tutorial demonstrates how to use the differentiable cell parameters in nanoBragg PyTorch to refine unit cell parameters from diffraction data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "With the new general triclinic cell parameter support, you can now:\n",
    "- Define arbitrary unit cells (not just cubic)\n",
    "- Make cell parameters differentiable\n",
    "- Use gradient-based optimization to refine parameters\n",
    "- Handle any crystal system from triclinic to cubic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set environment variable for MKL compatibility\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Import nanoBragg PyTorch components\n",
    "from nanobrag_torch.config import CrystalConfig\n",
    "from nanobrag_torch.models.crystal import Crystal\n",
    "from nanobrag_torch.models.detector import Detector\n",
    "from nanobrag_torch.simulator import Simulator\n",
    "\n",
    "# Set device and precision\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float64\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Triclinic Crystal Data\n",
    "\n",
    "Let's start by creating a target triclinic crystal that we'll try to recover through optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our \"true\" crystal parameters (triclinic)\n",
    "true_params = {\n",
    "    'cell_a': 281.0,      # Angstroms\n",
    "    'cell_b': 281.0,      # Angstroms\n",
    "    'cell_c': 165.2,      # Angstroms\n",
    "    'cell_alpha': 90.0,   # degrees\n",
    "    'cell_beta': 90.0,    # degrees\n",
    "    'cell_gamma': 120.0   # degrees (hexagonal-like)\n",
    "}\n",
    "\n",
    "# Create the true crystal\n",
    "true_config = CrystalConfig(\n",
    "    space_group_name='P1',\n",
    "    **true_params,\n",
    "    mosaic_spread_deg=0.0,\n",
    "    mosaic_domains=1,\n",
    "    N_cells=(5, 5, 5)\n",
    ")\n",
    "\n",
    "true_crystal = Crystal(config=true_config, device=device, dtype=dtype)\n",
    "detector = Detector(device=device, dtype=dtype)\n",
    "\n",
    "print(\"True crystal parameters:\")\n",
    "for param, value in true_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Generate \"observed\" diffraction pattern\n",
    "simulator = Simulator(true_crystal, detector, crystal_config=true_config, device=device, dtype=dtype)\n",
    "observed_image = simulator.run().detach()\n",
    "\n",
    "# Display the diffraction pattern\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(observed_image.cpu().numpy(), cmap='viridis', origin='lower')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title('Observed Diffraction Pattern')\n",
    "plt.xlabel('Fast axis (pixels)')\n",
    "plt.ylabel('Slow axis (pixels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Differentiable Parameters\n",
    "\n",
    "Now let's create initial guess parameters that are differentiable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a perturbed initial guess (10% error)\n",
    "initial_params = torch.tensor([\n",
    "    true_params['cell_a'] * 0.9,     # 10% too small\n",
    "    true_params['cell_b'] * 1.1,     # 10% too large\n",
    "    true_params['cell_c'] * 0.95,    # 5% too small\n",
    "    true_params['cell_alpha'] * 1.05, # 5% too large\n",
    "    true_params['cell_beta'] * 0.95,  # 5% too small\n",
    "    true_params['cell_gamma'] * 1.02  # 2% too large\n",
    "], device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "print(\"Initial guess parameters:\")\n",
    "param_names = ['cell_a', 'cell_b', 'cell_c', 'cell_alpha', 'cell_beta', 'cell_gamma']\n",
    "for i, name in enumerate(param_names):\n",
    "    error = (initial_params[i].item() - list(true_params.values())[i]) / list(true_params.values())[i] * 100\n",
    "    print(f\"  {name}: {initial_params[i].item():.1f} (error: {error:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining a Loss Function\n",
    "\n",
    "We'll use mean squared error between the observed and simulated diffraction patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(params, observed_image, detector, device, dtype):\n",
    "    \"\"\"Compute MSE loss between simulated and observed diffraction patterns.\"\"\"\n",
    "    # Unpack parameters\n",
    "    cell_a, cell_b, cell_c, cell_alpha, cell_beta, cell_gamma = params\n",
    "    \n",
    "    # Create crystal with current parameters\n",
    "    config = CrystalConfig(\n",
    "        space_group_name='P1',\n",
    "        cell_a=cell_a,\n",
    "        cell_b=cell_b,\n",
    "        cell_c=cell_c,\n",
    "        cell_alpha=cell_alpha,\n",
    "        cell_beta=cell_beta,\n",
    "        cell_gamma=cell_gamma,\n",
    "        mosaic_spread_deg=0.0,\n",
    "        mosaic_domains=1,\n",
    "        N_cells=(5, 5, 5)\n",
    "    )\n",
    "    \n",
    "    crystal = Crystal(config=config, device=device, dtype=dtype)\n",
    "    \n",
    "    # Simulate diffraction pattern\n",
    "    simulator = Simulator(crystal, detector, crystal_config=config, device=device, dtype=dtype)\n",
    "    simulated_image = simulator.run()\n",
    "    \n",
    "    # Compute MSE loss\n",
    "    loss = torch.nn.functional.mse_loss(simulated_image, observed_image)\n",
    "    \n",
    "    return loss, simulated_image\n",
    "\n",
    "# Test the loss function\n",
    "initial_loss, initial_image = compute_loss(initial_params, observed_image, detector, device, dtype)\n",
    "print(f\"Initial loss: {initial_loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Optimization\n",
    "\n",
    "Now let's use gradient-based optimization to refine the cell parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam([initial_params], lr=0.01)\n",
    "\n",
    "# Track optimization history\n",
    "loss_history = []\n",
    "param_history = []\n",
    "\n",
    "# Optimization loop\n",
    "n_iterations = 50\n",
    "for iteration in range(n_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute loss\n",
    "    loss, simulated_image = compute_loss(initial_params, observed_image, detector, device, dtype)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Store history\n",
    "    loss_history.append(loss.item())\n",
    "    param_history.append(initial_params.detach().clone().cpu().numpy())\n",
    "    \n",
    "    # Optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if iteration % 10 == 0:\n",
    "        print(f\"Iteration {iteration:3d}: Loss = {loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\nFinal loss: {loss_history[-1]:.6f}\")\n",
    "print(f\"Loss reduction: {(1 - loss_history[-1]/loss_history[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Convergence\n",
    "\n",
    "Let's visualize how the optimization progressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogy(loss_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Optimization Loss Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot parameter convergence\n",
    "plt.subplot(1, 2, 2)\n",
    "param_history = np.array(param_history)\n",
    "true_values = list(true_params.values())\n",
    "\n",
    "for i, name in enumerate(param_names):\n",
    "    relative_error = (param_history[:, i] - true_values[i]) / true_values[i] * 100\n",
    "    plt.plot(relative_error, label=name)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative Error (%)')\n",
    "plt.title('Parameter Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final parameters\n",
    "print(\"\\nFinal refined parameters:\")\n",
    "final_params = initial_params.detach().cpu().numpy()\n",
    "for i, name in enumerate(param_names):\n",
    "    true_val = true_values[i]\n",
    "    refined_val = final_params[i]\n",
    "    error = (refined_val - true_val) / true_val * 100\n",
    "    print(f\"  {name}: {refined_val:.3f} (true: {true_val:.3f}, error: {error:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Diffraction Patterns\n",
    "\n",
    "Let's visualize the difference between initial, refined, and true diffraction patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate diffraction pattern with refined parameters\n",
    "with torch.no_grad():\n",
    "    _, refined_image = compute_loss(initial_params, observed_image, detector, device, dtype)\n",
    "    refined_image = refined_image.cpu().numpy()\n",
    "\n",
    "# Also get the initial guess image\n",
    "initial_params_copy = torch.tensor(param_history[0], device=device, dtype=dtype)\n",
    "with torch.no_grad():\n",
    "    _, initial_guess_image = compute_loss(initial_params_copy, observed_image, detector, device, dtype)\n",
    "    initial_guess_image = initial_guess_image.cpu().numpy()\n",
    "\n",
    "observed_np = observed_image.cpu().numpy()\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Row 1: Images\n",
    "im1 = axes[0, 0].imshow(initial_guess_image, cmap='viridis', origin='lower')\n",
    "axes[0, 0].set_title('Initial Guess')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "im2 = axes[0, 1].imshow(refined_image, cmap='viridis', origin='lower')\n",
    "axes[0, 1].set_title('Refined')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "im3 = axes[0, 2].imshow(observed_np, cmap='viridis', origin='lower')\n",
    "axes[0, 2].set_title('True (Observed)')\n",
    "plt.colorbar(im3, ax=axes[0, 2])\n",
    "\n",
    "# Row 2: Differences\n",
    "diff1 = axes[1, 0].imshow(initial_guess_image - observed_np, cmap='RdBu_r', origin='lower')\n",
    "axes[1, 0].set_title('Initial - True')\n",
    "plt.colorbar(diff1, ax=axes[1, 0])\n",
    "\n",
    "diff2 = axes[1, 1].imshow(refined_image - observed_np, cmap='RdBu_r', origin='lower')\n",
    "axes[1, 1].set_title('Refined - True')\n",
    "plt.colorbar(diff2, ax=axes[1, 1])\n",
    "\n",
    "# Hide the last subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    if ax.get_visible():\n",
    "        ax.set_xlabel('Fast axis (pixels)')\n",
    "        ax.set_ylabel('Slow axis (pixels)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print RMS errors\n",
    "initial_rms = np.sqrt(np.mean((initial_guess_image - observed_np)**2))\n",
    "refined_rms = np.sqrt(np.mean((refined_image - observed_np)**2))\n",
    "print(f\"\\nRMS errors:\")\n",
    "print(f\"  Initial guess: {initial_rms:.6f}\")\n",
    "print(f\"  Refined:       {refined_rms:.6f}\")\n",
    "print(f\"  Improvement:   {(1 - refined_rms/initial_rms)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "### Constrained Optimization\n",
    "\n",
    "In practice, you might want to add constraints to keep parameters in physically reasonable ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_optimization(initial_params, observed_image, detector, device, dtype, n_iterations=50):\n",
    "    \"\"\"Optimization with parameter constraints.\"\"\"\n",
    "    params = initial_params.clone()\n",
    "    optimizer = torch.optim.Adam([params], lr=0.01)\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply constraints (e.g., positive lengths, angles between 20-160 degrees)\n",
    "        constrained_params = params.clone()\n",
    "        constrained_params[:3] = torch.nn.functional.relu(params[:3]) + 1.0  # Lengths > 1 Å\n",
    "        constrained_params[3:] = torch.clamp(params[3:], min=20.0, max=160.0)  # Angles in range\n",
    "        \n",
    "        loss, _ = compute_loss(constrained_params, observed_image, detector, device, dtype)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: Loss = {loss.item():.6f}\")\n",
    "    \n",
    "    return constrained_params\n",
    "\n",
    "# Example usage (not run to save computation)\n",
    "# constrained_params = constrained_optimization(initial_params.clone(), observed_image, detector, device, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Different Optimizers\n",
    "\n",
    "You can experiment with different PyTorch optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of different optimizers\n",
    "optimizers = {\n",
    "    'Adam': lambda p: torch.optim.Adam([p], lr=0.01),\n",
    "    'SGD': lambda p: torch.optim.SGD([p], lr=0.1, momentum=0.9),\n",
    "    'LBFGS': lambda p: torch.optim.LBFGS([p], lr=1, max_iter=20)\n",
    "}\n",
    "\n",
    "# LBFGS requires a closure\n",
    "def lbfgs_optimization(params, observed_image, detector, device, dtype, n_iterations=10):\n",
    "    optimizer = torch.optim.LBFGS([params], lr=1, max_iter=20)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = compute_loss(params, observed_image, detector, device, dtype)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        loss = optimizer.step(closure)\n",
    "        print(f\"LBFGS step {i}: Loss = {loss.item():.6f}\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Example usage (not run to save computation)\n",
    "# lbfgs_params = lbfgs_optimization(initial_params.clone(), observed_image, detector, device, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we demonstrated:\n",
    "\n",
    "1. **Loading triclinic crystal data** with arbitrary unit cell parameters\n",
    "2. **Setting up differentiable parameters** using PyTorch tensors with `requires_grad=True`\n",
    "3. **Defining a loss function** that compares simulated and observed diffraction patterns\n",
    "4. **Running gradient-based optimization** to refine cell parameters\n",
    "5. **Visualizing convergence** and comparing results\n",
    "\n",
    "The key advantages of this approach are:\n",
    "- **Automatic differentiation**: No need to derive gradients manually\n",
    "- **General crystal systems**: Works for any unit cell from triclinic to cubic\n",
    "- **GPU acceleration**: Can leverage CUDA for faster computation\n",
    "- **Integration with ML**: Can be combined with neural networks for advanced applications\n",
    "\n",
    "This differentiable simulation capability opens up many possibilities for crystallographic refinement, including joint refinement of multiple parameters, uncertainty quantification, and integration with machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}