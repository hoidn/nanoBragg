<?xml version="1.0" encoding="UTF-8"?>
<procedures>
    <metadata>
        <title>Claude Code Standard Operating Procedures</title>
        <purpose>This document is my primary playbook. It contains a set of standardized, step-by-step processes for executing different types of software engineering tasks. When given a high-level goal, I will first consult this document to select the appropriate Standard Operating Procedure (SOP) to follow.</purpose>
    </metadata>

    <core_principles>
        <principle id="1">
            <name>Checklist-Driven</name>
            <description>Every complex task is managed via a checklist. I will create, update, and complete checklists to track my progress and manage context.</description>
        </principle>
        <principle id="2">
            <name>Plan Before Acting</name>
            <description>For non-trivial tasks, I will always create a detailed plan before writing implementation code.</description>
        </principle>
        <principle id="3">
            <name>Verify, Then Commit</name>
            <description>I will always run tests to verify my changes before committing them.</description>
        </principle>
        <principle id="4">
            <name>Scale with Subagents</name>
            <description>For tasks that are complex or parallelizable, I will act as a supervising agent, spawning specialized subagents to handle discrete sub-problems.</description>
        </principle>
    </core_principles>

    <sop_directory>
        <sop id="1">
            <name>Task Planning &amp; Decomposition</name>
            <description>For high-level or ambiguous goals (e.g., "implement feature X," "refactor the logging system").</description>
        </sop>
        <sop id="2">
            <name>Focused Code Implementation</name>
            <description>For executing a clear, pre-defined plan.</description>
        </sop>
        <sop id="3">
            <name>Test-Driven Development (TDD)</name>
            <description>For creating new functionality with a strong verification contract.</description>
        </sop>
        <sop id="4">
            <name>Bug Fix &amp; Verification</name>
            <description>For resolving a specific bug from a ticket or report.</description>
        </sop>
        <sop id="5">
            <name>Documentation Update</name>
            <description>For updating project documentation based on recent code changes.</description>
        </sop>
        <sop id="6">
            <name>Interface Definition (IDL) Creation</name>
            <description>For reverse-engineering a formal contract from existing code.</description>
        </sop>
        <sop id="7">
            <name>Large-Scale Automated Refactoring</name>
            <description>For applying a consistent change across many files.</description>
        </sop>
        <sop id="8">
            <name>Problem Analysis &amp; Resolution Planning</name>
            <description>For scientific review and resolution of reported issues.</description>
        </sop>
        <sop id="9">
            <name>Problem Report Creation</name>
            <description>For documenting and structuring new problems in reports/problems/ directory.</description>
        </sop>
        <sop id="10">
            <name>Post-Milestone Code Cleanup</name>
            <description>For systematically reviewing and removing obsolete code after a major development milestone.</description>
        </sop>
    </sop_directory>

    <sop id="1">
        <name>Task Planning &amp; Decomposition</name>
        <goal>To transform a high-level objective into a detailed, actionable checklist.</goal>
        
        <phase name="Phase 0: Scoping &amp; Research">
            <task id="0.A">
                <name>Consult `CLAUDE.md`</name>
                <state>[ ]</state>
                <guidance>Read `CLAUDE.md` in the current and parent directories to understand project-specific rules, commands, and context.</guidance>
            </task>
            <task id="0.B">
                <name>Initial Codebase Exploration</name>
                <state>[ ]</state>
                <guidance>Identify potentially relevant files and directories based on the task description. Use file search and read the contents of 2-3 key files to build initial context.</guidance>
            </task>
            <task id="0.C">
                <name>Spawn Research Subagents (If Needed)</name>
                <state>[ ]</state>
                <guidance>If the task involves complex APIs or unclear areas, spawn subagents for focused investigation. **Example:** "Subagent, read `ExecutionFactory.ts` and its `git log`. Summarize its purpose and evolution."</guidance>
            </task>
            <task id="0.D">
                <name>Synthesize Findings</name>
                <state>[ ]</state>
                <guidance>Consolidate my own findings and the reports from any subagents into a brief summary of the current state and the core problem to be solved.</guidance>
            </task>
        </phase>

        <phase name="Phase 1: Plan Generation">
            <task id="1.A">
                <name>Engage Extended Thinking</name>
                <state>[ ]</state>
                <guidance>Use the command `think hard` to formulate a comprehensive plan. If the problem is exceptionally complex, use `ultrathink`.</guidance>
            </task>
            <task id="1.B">
                <name>Generate Implementation Checklist</name>
                <state>[ ]</state>
                <guidance>Create a detailed, step-by-step checklist for the implementation. The checklist should be broken into logical phases (e.g., Setup, Implementation, Testing, Cleanup).</guidance>
            </task>
            <task id="1.C">
                <name>Identify Parallelizable Tasks</name>
                <state>[ ]</state>
                <guidance>Review the checklist and identify any steps that can be performed in parallel. Mark these explicitly in the plan for potential subagent execution later.</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Finalization">
            <task id="2.A">
                <name>Save Plan to a Scratchpad</name>
                <state>[ ]</state>
                <guidance>Save the generated checklist to a new file (e.g., `PLAN.md`) or a new GitHub issue. This scratchpad will be the source of truth for the implementation phase.</guidance>
            </task>
            <task id="2.B">
                <name>Request User Approval</name>
                <state>[ ]</state>
                <guidance>Present the summary and the link to the plan. State: "I have completed the planning phase. Please review the plan at `PLAN.md`. Shall I proceed with implementation by executing **SOP-2**?"</guidance>
            </task>
        </phase>
    </sop>

    <sop id="2">
        <name>Focused Code Implementation</name>
        <goal>To execute a pre-existing plan from a checklist.</goal>
        
        <phase name="Phase 1: Setup">
            <task id="1.A">
                <name>Load Implementation Plan</name>
                <state>[ ]</state>
                <guidance>Open and review the detailed checklist created in **SOP-1**. Ensure I understand each step and any dependencies between tasks.</guidance>
            </task>
            <task id="1.B">
                <name>Verify Prerequisites</name>
                <state>[ ]</state>
                <guidance>Check that all prerequisites (environment setup, dependencies, etc.) mentioned in the plan are satisfied before beginning implementation.</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Execution">
            <task id="2.A">
                <name>Execute Plan Systematically</name>
                <state>[ ]</state>
                <guidance>Work through the implementation checklist sequentially. Mark each item as complete before moving to the next. If parallelizable tasks were identified, spawn subagents as appropriate.</guidance>
            </task>
            <task id="2.B">
                <name>Handle Blockers</name>
                <state>[ ]</state>
                <guidance>If any step fails or encounters an unexpected blocker, pause implementation and return to **SOP-1** to replan the affected portion.</guidance>
            </task>
        </phase>

        <phase name="Phase 3: Validation">
            <task id="3.A">
                <name>Run Tests</name>
                <state>[ ]</state>
                <guidance>Execute the project's test suite to verify that the implementation works correctly and has not introduced regressions.</guidance>
            </task>
            <task id="3.B">
                <name>Code Review</name>
                <state>[ ]</state>
                <guidance>Review the implemented code for adherence to project standards, readability, and maintainability.</guidance>
            </task>
            <task id="3.C">
                <name>Commit Changes</name>
                <state>[ ]</state>
                <guidance>Commit the changes with a clear, descriptive message that references the original plan or issue.</guidance>
            </task>
        </phase>
    </sop>

    <sop id="3">
        <name>Test-Driven Development (TDD)</name>
        <goal>To implement new functionality using a test-first approach.</goal>
        
        <phase name="Phase 1: Test Design">
            <task id="1.A">
                <name>Understand Requirements</name>
                <state>[ ]</state>
                <guidance>Clearly define the expected behavior of the new functionality. What inputs should it accept? What outputs should it produce? What edge cases need to be handled?</guidance>
            </task>
            <task id="1.B">
                <name>Write Failing Tests</name>
                <state>[ ]</state>
                <guidance>Write comprehensive test cases that define the desired behavior. Run the tests to confirm they fail (since the functionality doesn't exist yet).</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Implementation">
            <task id="2.A">
                <name>Implement Minimal Solution</name>
                <state>[ ]</state>
                <guidance>Write the minimum amount of code necessary to make the tests pass. Focus on functionality over optimization at this stage.</guidance>
            </task>
            <task id="2.B">
                <name>Verify Tests Pass</name>
                <state>[ ]</state>
                <guidance>Run the test suite to confirm that the new tests pass and no existing tests have been broken.</guidance>
            </task>
        </phase>

        <phase name="Phase 3: Refinement">
            <task id="3.A">
                <name>Refactor Code</name>
                <state>[ ]</state>
                <guidance>Improve the code structure, performance, and readability while ensuring all tests continue to pass.</guidance>
            </task>
            <task id="3.B">
                <name>Add Additional Tests</name>
                <state>[ ]</state>
                <guidance>Add more test cases if needed to cover edge cases or improve coverage. Ensure comprehensive testing of the new functionality.</guidance>
            </task>
            <task id="3.C">
                <name>Final Validation</name>
                <state>[ ]</state>
                <guidance>Run the complete test suite one final time to ensure everything works correctly before committing.</guidance>
            </task>
        </phase>
    </sop>

    <sop id="4">
        <name>Bug Fix &amp; Verification</name>
        <goal>To diagnose, fix, and verify a bug.</goal>
        
        <phase name="Phase 1: Understanding">
            <task id="1.A">
                <name>Understand the Bug</name>
                <state>[ ]</state>
                <guidance>Read the GitHub issue or bug report. Use `gh issue view {issue_number}`.</guidance>
            </task>
            <task id="1.B">
                <name>Create a Failing Test</name>
                <state>[ ]</state>
                <guidance>**This is the most critical step.** Write a new test case that specifically reproduces the bug. Run the test and confirm it fails. This proves I have understood and replicated the problem.</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Resolution">
            <task id="2.A">
                <name>Diagnose Root Cause</name>
                <state>[ ]</state>
                <guidance>Use tools like `git blame` and `git log` on the relevant files to understand the history. Read the code to identify the logical flaw.</guidance>
            </task>
            <task id="2.B">
                <name>Implement the Fix</name>
                <state>[ ]</state>
                <guidance>Modify the code to correct the flaw.</guidance>
            </task>
        </phase>

        <phase name="Phase 3: Verification">
            <task id="3.A">
                <name>Verify the Fix</name>
                <state>[ ]</state>
                <guidance>Run the new failing test and confirm it now passes.</guidance>
            </task>
            <task id="3.B">
                <name>Verify No Regressions</name>
                <state>[ ]</state>
                <guidance>Run the *entire* project test suite to ensure the fix has not introduced any new problems.</guidance>
            </task>
            <task id="3.C">
                <name>Commit and Create PR</name>
                <state>[ ]</state>
                <guidance>Commit the fix and the new test. Use `gh pr create` and reference the issue number in the PR description (e.g., "Fixes #{issue_number}").</guidance>
            </task>
        </phase>
    </sop>

    <sop id="4" variant="pytorch_physics">
        <name>PyTorch Physics Debugging (Specialized)</name>
        <goal>To debug physics calculations in the PyTorch implementation using specialized tools.</goal>
        <reference>For complete debugging methodology, troubleshooting scenarios, and debug script management, see `torch/debugging.md`.</reference>
        
        <phase name="Phase 1: Initial Diagnosis">
            <task id="1.A">
                <name>Run Pixel Trace Debug</name>
                <state>[ ]</state>
                <guidance>**ALWAYS START HERE.** Run `KMP_DUPLICATE_LIB_OK=TRUE python scripts/debug_pixel_trace.py` to generate the detailed trace log.</guidance>
            </task>
            <task id="1.B">
                <name>Compare Against Golden Reference</name>
                <state>[ ]</state>
                <guidance>Compare the generated trace against `tests/golden_data/simple_cubic_pixel_trace.log`. Look for deviations in intermediate values.</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Problem Isolation">
            <task id="2.A">
                <name>Identify Divergence Point</name>
                <state>[ ]</state>
                <guidance>Identify the first calculation step where values diverge from expected. This pinpoints the buggy component.</guidance>
            </task>
            <task id="2.B">
                <name>Validate Component in Isolation</name>
                <state>[ ]</state>
                <guidance>Create a minimal test case for the suspected component using the exact inputs from the trace log.</guidance>
            </task>
        </phase>

        <phase name="Phase 3: Common Issues Check">
            <task id="3.A">
                <name>Check Unit System</name>
                <state>[ ]</state>
                <guidance>Verify all inputs are in the correct units (Angstroms for length, eV for energy). This is the most common source of bugs.</guidance>
            </task>
            <task id="3.B">
                <name>Check Coordinate System</name>
                <state>[ ]</state>
                <guidance>Verify `torch.meshgrid` calls use `indexing="ij"` and that all vectors follow the `(slow, fast)` convention.</guidance>
            </task>
            <task id="3.C">
                <name>Check Differentiability</name>
                <state>[ ]</state>
                <guidance>If the bug affects gradients, run `torch.autograd.gradcheck` on the suspected component.</guidance>
            </task>
        </phase>

        <phase name="Phase 4: Resolution">
            <task id="4.A">
                <name>Implement Fix</name>
                <state>[ ]</state>
                <guidance>Apply the fix to the identified component.</guidance>
            </task>
            <task id="4.B">
                <name>Re-run Pixel Trace</name>
                <state>[ ]</state>
                <guidance>Run the pixel trace again and confirm all values now match the golden reference.</guidance>
            </task>
            <task id="4.C">
                <name>Verify Full Test Suite</name>
                <state>[ ]</state>
                <guidance>Run the complete test suite to ensure no regressions.</guidance>
            </task>
        </phase>
    </sop>

    <sop id="5">
        <name>Documentation Update</name>
        <goal>To update documentation to reflect recent code changes.</goal>
        
        <phase name="Phase 1: Analysis">
            <task id="1.A">
                <name>Analyze Code Changes</name>
                <state>[ ]</state>
                <guidance>Use `git diff main` to get a list of all changed files and their modifications.</guidance>
            </task>
            <task id="1.B">
                <name>Spawn Subagent for Analysis</name>
                <state>[ ]</state>
                <guidance>Spawn a subagent with the `git diff` output. **Prompt:** "Subagent, review this diff and identify all changes to public-facing APIs, user-visible behavior, or command-line arguments. List the affected files and a summary of each change."</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Updates">
            <task id="2.A">
                <name>Update Documentation Files</name>
                <state>[ ]</state>
                <guidance>Based on the subagent's report, edit the relevant documentation files (`README.md`, `CLAUDE.md`, docstrings, etc.) to reflect the changes.</guidance>
            </task>
            <task id="2.B">
                <name>Commit Documentation</name>
                <state>[ ]</state>
                <guidance>Create a commit with the message "docs: Update documentation for [feature name]".</guidance>
            </task>
        </phase>
    </sop>

    <sop id="6">
        <name>Interface Definition (IDL) Creation</name>
        <goal>To create a formal contract (`_IDL.md`) for an existing code module.</goal>
        
        <phase name="Phase 1: Analysis">
            <task id="1.A">
                <name>Analyze Source Code</name>
                <state>[ ]</state>
                <guidance>Read the target source file (`.py`, `.ts`, etc.) in its entirety.</guidance>
            </task>
            <task id="1.B">
                <name>Identify Public Interface</name>
                <state>[ ]</state>
                <guidance>List all public classes, functions, and methods. Ignore private members (e.g., those prefixed with `_`).</guidance>
            </task>
            <task id="1.C">
                <name>Extract Signatures</name>
                <state>[ ]</state>
                <guidance>For each public item, document its signature, including parameters (and their types) and return values (and their types).</guidance>
            </task>
            <task id="1.D">
                <name>Infer Behavior</name>
                <state>[ ]</state>
                <guidance>Read the code logic for each function/method to summarize its core behavior, preconditions, and postconditions.</guidance>
            </task>
            <task id="1.E">
                <name>Identify Error Conditions</name>
                <state>[ ]</state>
                <guidance>Look for `raise` statements or error-handling blocks to document potential exceptions.</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Documentation">
            <task id="2.A">
                <name>Draft IDL File</name>
                <state>[ ]</state>
                <guidance>Assemble the collected information into a new `_IDL.md` file, following the project's standard IDL format.</guidance>
            </task>
            <task id="2.B">
                <name>Commit IDL</name>
                <state>[ ]</state>
                <guidance>Commit the new file with the message "docs: Add IDL for [module name]".</guidance>
            </task>
        </phase>
    </sop>

    <sop id="7">
        <name>Large-Scale Automated Refactoring</name>
        <goal>To apply a single, consistent change across a large number of files.</goal>
        
        <phase name="Phase 1: Planning">
            <task id="1.A">
                <name>Generate Task List</name>
                <state>[ ]</state>
                <guidance>Write a script or use shell commands to generate a list of all files that need to be modified. Save this list to a scratchpad file, `refactor_checklist.md`.</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Execution">
            <task id="2.A">
                <name>Process Checklist</name>
                <state>[ ]</state>
                <guidance>Begin a loop to process each file listed in `refactor_checklist.md`.</guidance>
            </task>
            <task id="2.B">
                <name>Spawn Refactoring Subagent</name>
                <state>[ ]</state>
                <guidance>For each file in the list, spawn a dedicated subagent with a highly focused prompt. **Example:** "Subagent, your only task is to refactor the file `{filename}`. Replace all instances of `old_api()` with `new_api()`. Do not modify any other files. Report 'SUCCESS' or 'FAILURE'."</guidance>
            </task>
            <task id="2.C">
                <name>Update Master Checklist</name>
                <state>[ ]</state>
                <guidance>As each subagent reports back, update the status of the corresponding item in `refactor_checklist.md`. Log any failures for later manual review.</guidance>
            </task>
        </phase>

        <phase name="Phase 3: Verification">
            <task id="3.A">
                <name>Final Verification</name>
                <state>[ ]</state>
                <guidance>After the loop is complete, run the linter and the full test suite to verify the entire refactoring effort.</guidance>
            </task>
            <task id="3.B">
                <name>Commit Changes</name>
                <state>[ ]</state>
                <guidance>Commit all changes with a comprehensive message describing the refactoring task.</guidance>
            </task>
        </phase>
    </sop>

    <sop id="8">
        <name>Problem Analysis &amp; Resolution Planning</name>
        <goal>To systematically review, validate, and create resolution plans for reported problems in the `reports/problems/` directory.</goal>
        
        <phase name="Phase 0: Setup &amp; Discovery">
            <task id="0.A">
                <name>Scan Problem Reports</name>
                <state>[ ]</state>
                <guidance>List all JSON files in `reports/problems/` directory. Each file contains a structured list of reported problems and associated tasks.</guidance>
            </task>
            <task id="0.B">
                <name>Select Problem File</name>
                <state>[ ]</state>
                <guidance>Choose the next unprocessed JSON file for analysis. Follow naming convention: `problems_YYYY-MM-DD.json` or similar descriptive names.</guidance>
            </task>
        </phase>

        <phase name="Phase 1: Problem Validation">
            <task id="1.A">
                <name>Load Problem JSON</name>
                <state>[ ]</state>
                <guidance>Parse the JSON file to extract the list of problems. Each entry should contain: `{"id": "...", "description": "...", "tasks": [...], "priority": "...", "reporter": "...", "date": "..."}`.</guidance>
            </task>
            <task id="1.B">
                <name>Review Each Problem</name>
                <state>[ ]</state>
                <guidance>For each JSON list entry, carefully read the problem description. Assess whether this is genuinely a problem requiring attention. **Criteria:** Is it scientifically valid? Does it affect functionality? Is it within project scope? **Action:** Accept (proceed to task review) or Reject (mark for removal, move to next problem).</guidance>
            </task>
            <task id="1.C">
                <name>Review Each Task</name>
                <state>[ ]</state>
                <guidance>For accepted problems, examine each associated task in the `tasks` array. **Criteria:** Is the task necessary to resolve the problem? Is it technically feasible? Is it appropriately scoped? **Action:** Accept (include in resolution plan) or Reject (mark for removal).</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Technical Analysis">
            <task id="2.A">
                <name>Codebase Review</name>
                <state>[ ]</state>
                <guidance>For each accepted problem+task combination, review relevant codebase sections. Use `Grep`, `Read`, and `Glob` tools to understand current implementation. **Focus:** Understand the root cause and impact scope.</guidance>
            </task>
            <task id="2.B">
                <name>Documentation Review</name>
                <state>[ ]</state>
                <guidance>Consult relevant documentation in `torch/` directory: Architecture, Implementation Plan, Testing Strategy, etc. **Purpose:** Ensure proposed solutions align with project design principles and testing requirements.</guidance>
            </task>
            <task id="2.C">
                <name>Analyze Resolution Approach</name>
                <state>[ ]</state>
                <guidance>Based on codebase and documentation review, develop a technical approach for each accepted problem. **Consider:** Implementation complexity, testing requirements, potential side effects, integration with existing systems.</guidance>
            </task>
        </phase>

        <phase name="Phase 3: Resolution Planning">
            <task id="3.A">
                <name>Create Fix Plan Checklist</name>
                <state>[ ]</state>
                <guidance>For each accepted problem, create a self-contained, step-by-step implementation checklist following the project's checklist format. **Structure:** Use the same `| ID | Task Description | State | Details &amp; Guidance |` format as other SOPs. **Requirements:** Each checklist must be actionable by a developer without additional context.</guidance>
            </task>
            <task id="3.B">
                <name>Validate Plan Completeness</name>
                <state>[ ]</state>
                <guidance>Review each fix plan checklist to ensure it covers: problem resolution, testing requirements, documentation updates, and integration considerations. **Check:** Are all dependencies identified? Are test cases specified? Is the scope clearly bounded?</guidance>
            </task>
        </phase>

        <phase name="Phase 4: Output Generation">
            <task id="4.A">
                <name>Generate Updated JSON</name>
                <state>[ ]</state>
                <guidance>Create a new JSON file with the same basename plus a suffix (e.g., `problems_YYYY-MM-DD_reviewed.json`). **Content:** Remove all rejected problems and tasks. Retain only accepted items. **Structure:** Maintain original JSON schema.</guidance>
            </task>
            <task id="4.B">
                <name>Create Resolution Report</name>
                <state>[ ]</state>
                <guidance>Generate a comprehensive `.md` file under `reports/problems/resolutions/` with filename matching the JSON (e.g., `problems_YYYY-MM-DD_analysis.md`). **Content:** Document analysis process, acceptance/rejection decisions with rationale, and detailed fix plan checklists for each accepted problem.</guidance>
            </task>
            <task id="4.C">
                <name>Commit Analysis Results</name>
                <state>[ ]</state>
                <guidance>Commit both the updated JSON and the resolution report with message: "analysis: Problem review and resolution planning for [filename]". **Note:** Do not implement fixes - only provide the analysis and planning.</guidance>
            </task>
        </phase>

        <outputs>
            <output>`reports/problems/[original_name]_reviewed.json`: Filtered problem list with only accepted items</output>
            <output>`reports/problems/resolutions/[original_name]_analysis.md`: Comprehensive analysis report with fix plan checklists</output>
            <output>Git commit documenting the analysis process</output>
        </outputs>

        <quality_criteria>
            <criterion>All problem assessments must have clear scientific/technical rationale</criterion>
            <criterion>Fix plan checklists must be immediately actionable by other developers</criterion>
            <criterion>Analysis must consider project architecture, testing strategy, and documentation requirements</criterion>
            <criterion>Resolution approach must align with established SOPs (particularly SOP-1 for planning and SOP-4 for bug fixes)</criterion>
        </quality_criteria>
    </sop>

    <sop id="9">
        <name>Problem Report Creation</name>
        <goal>To systematically document and structure new problems for future analysis and resolution.</goal>
        
        <phase name="Phase 0: Problem Discovery &amp; Assessment">
            <task id="0.A">
                <name>Identify Problem Source</name>
                <state>[ ]</state>
                <guidance>Determine the origin of the problem: user report, bug discovery, code review finding, scientific inconsistency, performance issue, etc. **Record:** Source type and discovery context.</guidance>
            </task>
            <task id="0.B">
                <name>Assess Problem Validity</name>
                <state>[ ]</state>
                <guidance>Validate that this is a genuine problem requiring documentation. **Criteria:** Does it affect functionality? Is it scientifically incorrect? Does it impact performance or usability? **Action:** If not valid, stop here and document the dismissal reason.</guidance>
            </task>
            <task id="0.C">
                <name>Check for Duplicates</name>
                <state>[ ]</state>
                <guidance>Search existing problem reports in `reports/problems/` to ensure this hasn't already been documented. **Tools:** Use `grep` to search JSON files for keywords related to the problem.</guidance>
            </task>
        </phase>

        <phase name="Phase 1: Problem Documentation">
            <task id="1.A">
                <name>Create Problem ID</name>
                <state>[ ]</state>
                <guidance>Generate a unique identifier following the pattern: `PROB-YYYY-MM-DD-NNN` where NNN is a sequential number for the day. **Example:** `PROB-2025-01-07-001`.</guidance>
            </task>
            <task id="1.B">
                <name>Write Problem Description</name>
                <state>[ ]</state>
                <guidance>Create a clear, concise description (2-4 sentences) that explains: What is wrong? What should happen instead? What is the impact? **Avoid:** Implementation details or proposed solutions at this stage.</guidance>
            </task>
            <task id="1.C">
                <name>Classify Problem Type</name>
                <state>[ ]</state>
                <guidance>Assign appropriate categories. **Types:** `bug`, `enhancement`, `performance`, `documentation`, `scientific_accuracy`, `usability`, `testing`. **Priority:** `critical`, `high`, `medium`, `low`.</guidance>
            </task>
            <task id="1.D">
                <name>Document Reproduction Steps</name>
                <state>[ ]</state>
                <guidance>If applicable, provide step-by-step instructions to reproduce the problem. Include: Environment details, input data, commands to run, expected vs actual results. **Format:** Numbered list with specific commands and file paths.</guidance>
            </task>
        </phase>

        <phase name="Phase 2: Task Decomposition">
            <task id="2.A">
                <name>Identify Required Actions</name>
                <state>[ ]</state>
                <guidance>Break down the problem into specific, actionable tasks. **Consider:** Code changes needed, tests to add, documentation to update, validation steps required. **Format:** Each task should be a single, well-defined action.</guidance>
            </task>
            <task id="2.B">
                <name>Estimate Task Complexity</name>
                <state>[ ]</state>
                <guidance>For each task, assign complexity level: `trivial` (minutes), `simple` (hours), `moderate` (days), `complex` (weeks). **Consider:** Code understanding required, testing complexity, potential side effects.</guidance>
            </task>
            <task id="2.C">
                <name>Identify Dependencies</name>
                <state>[ ]</state>
                <guidance>Document any dependencies between tasks or external dependencies. **Types:** Other problems that must be solved first, external libraries, documentation completion, stakeholder decisions.</guidance>
            </task>
        </phase>

        <phase name="Phase 3: JSON Report Generation">
            <task id="3.A">
                <name>Select Target File</name>
                <state>[ ]</state>
                <guidance>Choose the appropriate JSON file in `reports/problems/`. **Convention:** Use current date format `problems_YYYY-MM-DD.json`. Create new file if none exists for current date.</guidance>
            </task>
            <task id="3.B">
                <name>Structure JSON Entry</name>
                <state>[ ]</state>
                <guidance>Create a properly formatted JSON entry following the schema:
```json
{
  "id": "PROB-YYYY-MM-DD-NNN",
  "title": "Brief problem title",
  "description": "Detailed problem description",
  "type": ["bug", "enhancement", etc.],
  "priority": "high|medium|low|critical",
  "reporter": "Name or source",
  "date": "YYYY-MM-DD",
  "reproduction_steps": ["step 1", "step 2", ...],
  "affected_components": ["component1", "component2"],
  "tasks": [
    {
      "id": "TASK-NNN",
      "description": "Specific task description",
      "complexity": "simple|moderate|complex",
      "dependencies": ["other-task-id"],
      "estimated_effort": "2 hours|1 day|etc."
    }
  ],
  "metadata": {
    "environment": "development|testing|production",
    "version": "current git hash",
    "related_files": ["path/to/file1", "path/to/file2"]
  }
}
```</guidance>
            </task>
            <task id="3.C">
                <name>Validate JSON Structure</name>
                <state>[ ]</state>
                <guidance>Verify the JSON is properly formatted and complete. **Check:** Valid JSON syntax, all required fields present, consistent formatting with existing entries, no duplicate IDs.</guidance>
            </task>
        </phase>

        <phase name="Phase 4: Integration &amp; Documentation">
            <task id="4.A">
                <name>Add to Problem File</name>
                <state>[ ]</state>
                <guidance>Insert the new problem entry into the appropriate JSON file. **Placement:** Add to the end of the problems array. **Backup:** Ensure you have a backup of the original file before modification.</guidance>
            </task>
            <task id="4.B">
                <name>Update Problem Index</name>
                <state>[ ]</state>
                <guidance>If a `problems_index.md` or similar tracking file exists, update it with the new problem entry. Include: Problem ID, title, priority, and current status.</guidance>
            </task>
            <task id="4.C">
                <name>Create Supplementary Documentation</name>
                <state>[ ]</state>
                <guidance>If the problem requires extensive context, create a separate `.md` file under `reports/problems/details/` with the same ID as filename. **Content:** Detailed technical analysis, screenshots, logs, or research notes.</guidance>
            </task>
        </phase>

        <phase name="Phase 5: Communication &amp; Tracking">
            <task id="5.A">
                <name>Commit Problem Report</name>
                <state>[ ]</state>
                <guidance>Commit the new problem report with a descriptive message: "problem: Add [PROB-ID] - [brief title]". **Include:** All modified JSON files and any supplementary documentation.</guidance>
            </task>
            <task id="5.B">
                <name>Notify Stakeholders</name>
                <state>[ ]</state>
                <guidance>If the problem has high or critical priority, notify relevant team members. **Methods:** GitHub issue creation, team communication channels, or direct notification as appropriate.</guidance>
            </task>
            <task id="5.C">
                <name>Schedule Review</name>
                <state>[ ]</state>
                <guidance>For complex problems, schedule a follow-up review or assign for analysis using **SOP-8: Problem Analysis &amp; Resolution Planning**. **Timeline:** Critical problems within 24 hours, high priority within 1 week.</guidance>
            </task>
        </phase>

        <outputs>
            <output>`reports/problems/problems_YYYY-MM-DD.json`: Updated problem file with new entry</output>
            <output>`reports/problems/details/[PROB-ID].md`: Supplementary documentation (if needed)</output>
            <output>Git commit documenting the new problem report</output>
            <output>Communication to relevant stakeholders (if high/critical priority)</output>
        </outputs>

        <quality_criteria>
            <criterion>Problem description must be clear and actionable without requiring additional context</criterion>
            <criterion>Tasks must be specific enough to be immediately implementable</criterion>
            <criterion>JSON structure must follow established schema consistently</criterion>
            <criterion>All problem IDs must be unique across the entire reports/problems/ directory</criterion>
            <criterion>Reproduction steps (when applicable) must be complete and testable</criterion>
            <criterion>Priority and complexity assessments must be realistic and justified</criterion>
        </quality_criteria>

        <integration_notes>
            <note>This SOP works in conjunction with **SOP-8: Problem Analysis &amp; Resolution Planning**</note>
            <note>Complex problems created here should be scheduled for analysis using SOP-8</note>
            <note>Problem reports should reference relevant SOPs for resolution (SOP-1 for planning, SOP-4 for bugs, etc.)</note>
        </integration_notes>
    </sop>

    <sop id="10">
        <name>Post-Milestone Code Cleanup</name>
        <goal>To systematically review and remove obsolete, redundant, or temporary code after a major development milestone, ensuring the codebase remains maintainable.</goal>
        <trigger>This SOP should be executed after a significant feature is completed, a major bug is fixed, or a milestone (like Milestone 1) is achieved.</trigger>

        <phase name="Phase 1: Identification &amp; Assessment">
            <task id="1.A">
                <name>Identify Recently Completed Work</name>
                <state>[ ]</state>
                <guidance>Review the completed tasks from the recent milestone or sprint. Identify the primary files that were created or heavily modified during the debugging and implementation process.</guidance>
            </task>
            <task id="1.B">
                <name>Analyze Scripts for Redundancy</name>
                <state>[ ]</state>
                <guidance>Examine the scripts in the context of the project's permanent toolchain (`test_suite.py`, `debug_pixel_trace.py`, etc.). For each script, ask:
                - **Is its function now part of a permanent tool?** (e.g., `simple_validation.py` is superseded by `tests/test_suite.py`).
                - **Was it a one-off for a specific hypothesis?** (e.g., `test_raw_intensity.py` tested a scaling theory that is now understood).
                - **Was it purely for exploration?** (e.g., `debug_golden_data.py` was for understanding a file format).
                - **Is it a less-capable version of a permanent tool?** (e.g., `debug_simple_cubic.py` is superseded by the combination of `debug_pixel_trace.py` and `first_win_demo.py`).
                </guidance>
            </task>
            <task id="1.C">
                <name>Create a Removal Plan</name>
                <state>[ ]</state>
                <guidance>Create a simple list of files to be removed and a one-line justification for each. This will be used for the commit message.
                **Example:**
                - `debug_simple_cubic.py`: Redundant. Functionality covered by `debug_pixel_trace.py` and `first_win_demo.py`.
                - `simple_validation.py`: Obsolete. Replaced by formal integration tests in `tests/test_suite.py`.
                </guidance>
            </task>
        </phase>

        <phase name="Phase 2: Archival &amp; Removal">
            <task id="2.A">
                <name>Ensure Archive Directory Exists</name>
                <state>[ ]</state>
                <guidance>Create a dated and named archive directory to safely store the obsolete scripts before deletion. This provides a safety net.
                **Command:** `mkdir -p archive/YYYY-MM-DD_[milestone_name]`
                **Example:** `mkdir -p archive/2024-07-07_milestone1_cleanup`
                </guidance>
            </task>
            <task id="2.B">
                <name>Move Obsolete Scripts to Archive</name>
                <state>[ ]</state>
                <guidance>Use `git mv` to move the identified scripts into the archive directory. Using `git mv` preserves file history, which is preferable to a simple `mv`.
                **Example:** `git mv debug_simple_cubic.py archive/2024-07-07_first_win_cleanup/`
                </guidance>
            </task>
        </phase>

        <phase name="Phase 3: Validation &amp; Finalization">
            <task id="3.A">
                <name>Run Full Test Suite</name>
                <state>[ ]</state>
                <guidance>Execute the entire test suite to verify that the removal of the scripts did not break any core functionality or tests.
                **Command:** `pytest -v`
                </guidance>
            </task>
            <task id="3.B">
                <name>Run Linter and Formatter</name>
                <state>[ ]</state>
                <guidance>Run the project's code quality tools to ensure the file removals haven't introduced any new issues.
                **Command:** `make lint` or `black . --check`
                </guidance>
            </task>
            <task id="3.C">
                <name>Commit the Cleanup</name>
                <state>[ ]</state>
                <guidance>Commit the changes with a clear, descriptive message that explains the purpose of the cleanup. Use the removal plan from Task 1.C.
                **Example Commit Message:**
                `refactor: Archive one-off debug scripts after Milestone 1
                
                - Moved several temporary and exploratory debug scripts to the archive directory.
                - These scripts are now redundant as their functionality is covered by the permanent testing and reporting tools (`test_suite.py`, `debug_pixel_trace.py`, `first_win_demo.py`).
                - This cleanup improves maintainability and clarifies the project structure.`
                </guidance>
            </task>
        </phase>

        <outputs>
            <output>A cleaner, more maintainable codebase with fewer redundant files.</output>
            <output>A `git` commit documenting the refactoring effort.</output>
            <output>An `archive` directory containing the history of removed scripts.</output>
        </outputs>

        <quality_criteria>
            <criterion>No core functionality or tests are broken by the cleanup.</criterion>
            <criterion>The justification for removing each file is clear and logical.</criterion>
            <criterion>The project is easier for a new developer to navigate after the cleanup.</criterion>
        </quality_criteria>

        <integration_notes>
            <note>This SOP is a natural follow-up to **SOP-2 (Focused Code Implementation)** and **SOP-4 (Bug Fix &amp; Verification)**.</note>
            <note>It serves as a "housekeeping" step to be performed at the end of a development cycle.</note>
        </integration_notes>
    </sop>
</procedures>