[W1009 20:09:09.924805010 CPUAllocator.cpp:245] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event
================================================================================
Detailed nanoBragg Performance Analysis
================================================================================

Outputs will be saved to: reports/benchmarks/20251009-200904

System Information:
CPU cores: 32
Total RAM: 125.7 GB
PyTorch version: 2.7.1+cu126
CUDA available: True
GPU: NVIDIA GeForce RTX 3090

Benchmark Configuration:
Test sizes: [4096]
Iterations: 1
Device: cpu
Dtype: float32
Profiling: enabled
Artifacts: keeping profiler traces

C binary: ./golden_suite_generator/nanoBragg

================================================================================
Benchmarking Different Detector Sizes
================================================================================

PERF-PYTORCH-005: Testing simulator cache reuse
Each size will be run twice: cold (first run) and warm (cached)

==================================================
Detector Size: 4096x4096 (16,777,216 pixels)
--------------------------------------------------
Running C implementation...
Running PyTorch COLD (first run) on CPU...
  Setup time (cold): 0.174s
  Simulation time: 2.320s
  Cache hit: False
Running PyTorch WARM (cached) on CPU...
  Profiling enabled: reports/benchmarks/20251009-200904/profile_4096x4096
    Saved trace to: reports/benchmarks/20251009-200904/profile_4096x4096/trace.json
  Setup time (warm): 0.000s
  Simulation time: 0.634s
  Cache hit: True
  Profiler trace saved: reports/benchmarks/20251009-200904/profile_4096x4096/trace.json
  Setup speedup: 81279.4x faster

Results:
  C time: 0.757s (memory: 0.0 MB)
  PyTorch COLD total: 2.533s (memory: 437.3 MB)
  PyTorch WARM total: 0.673s (memory: 14.4 MB)
  Speedup (cold): 0.30x (C faster)
  Speedup (warm): 1.13x (PyTorch faster)
  Correlation (cold): 0.721175
  Correlation (warm): 0.721175

  ✓ Cache effectiveness: 81279.4x faster setup
  ✓ PERF-PYTORCH-005 EXIT CRITERIA MET: Warm setup 0.0ms < 50ms

================================================================================
PERFORMANCE SUMMARY (PERF-PYTORCH-005: Cache Reuse)
================================================================================

Size             Pixels    C (s)    PyTorch    PyTorch      Setup       Corr
                                   COLD (s)   WARM (s)    Speedup     (warm)
----------------------------------------------------------------------------------
4096x4096    16,777,216    0.757      2.533      0.673   81279.4x   0.721175

================================================================================
CACHE EFFECTIVENESS (PERF-PYTORCH-005)
================================================================================

Size       Setup COLD (ms) Setup WARM (ms)      Speedup    Exit Crit
----------------------------------------------------------------------
4096x4096            174.4             0.0     81279.4x            ✓

PERF-PYTORCH-005 Exit Criteria: Setup time < 50ms for cached runs
  ✓ ALL SIZES MEET EXIT CRITERIA

Detailed results saved to reports/benchmarks/20251009-200904/benchmark_results.json

================================================================================
ANALYSIS
================================================================================

COLD runs (first time, includes compilation):
  ✓ C is on average 3.34x faster than PyTorch

WARM runs (cached simulator):
  ✓ PyTorch is on average 1.13x faster than C

Cache effectiveness: 81279.4x faster setup on average
✓ All correlations > 0.99, indicating excellent numerical agreement

================================================================================
PROFILING ARTIFACTS
================================================================================

Profiler traces saved to subdirectories:
  4096x4096: reports/benchmarks/20251009-200904/profile_4096x4096/trace.json

To view profiler traces:
  Chrome: Open chrome://tracing and load trace.json
  TensorBoard: Run 'tensorboard --logdir reports/benchmarks/20251009-200904'


All outputs saved to: reports/benchmarks/20251009-200904
