# PERF-PYTORCH-004 P3.3 — CUDA Benchmark Blocker

**Date:** 2025-09-30
**Status:** BLOCKED
**Blocker:** CUDA graphs compatibility issue (tracked as PERF-PYTORCH-005-CUDAGRAPHS)

## Summary

Attempted to run P3.3 CUDA benchmarks using the same configuration that succeeded for P3.2 CPU benchmarks (Attempt #17). All CUDA runs failed immediately with CUDAGraphs RuntimeError, preventing any performance measurement.

## Command Executed

```bash
env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/benchmark_detailed.py \
  --sizes 256,512,1024 --device cuda --iterations 2
```

## Error Details

**Error Type:** `RuntimeError` from torch CUDAGraphs
**Location:** `src/nanobrag_torch/simulator.py:345` in `compute_physics_for_position`
**Message:** "accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run"

**Stack Trace Excerpt:**
```
File "/home/ollie/Documents/nanoBragg/src/nanobrag_torch/simulator.py", line 345, in compute_physics_for_position
    incident_flat = incident_beam_direction.unsqueeze(0).unsqueeze(0).expand(
        diffracted_beam_unit.shape[0], diffracted_beam_unit.shape[1], -1
    ).reshape(-1, 3).contiguous()
```

**Suggested Fix:** "To prevent overwriting, clone the tensor outside of torch.compile() or call torch.compiler.cudagraph_mark_step_begin() before each model invocation."

## Root Cause Analysis

The `incident_beam_direction` tensor (shape `[3]`) is being repeatedly expanded and reshaped inside the torch.compile compiled graph without cloning. This creates aliased tensor views that violate CUDA graphs memory safety requirements:

1. `incident_beam_direction` is a parameter passed into `compute_physics_for_position`
2. Inside the compiled function, it's expanded via `unsqueeze().expand().reshape()`
3. CUDA graphs detects this creates unsafe aliasing across multiple invocations
4. CPU execution doesn't enforce these constraints, so the issue only manifests on CUDA

## Impact

- **Blocks:** PERF-PYTORCH-004 P3.3 (CUDA benchmarks)
- **Affects:** All CUDA execution of the simulator
- **Severity:** High - prevents any CUDA performance validation
- **CPU:** Not affected - CPU benchmarks (P3.2) completed successfully

## Proposed Solutions

### Option A: Add tensor clone (RECOMMENDED)
- **Action:** Add `.clone()` to `incident_beam_direction` before the expansion
- **Pros:** Minimal change, preserves gradients, device-neutral
- **Cons:** Small memory/performance overhead
- **Verification:** Run CUDA benchmarks, verify CPU unchanged, check gradients

### Option B: Restructure tensor flow
- **Action:** Redesign to avoid aliasing entirely
- **Pros:** No clone overhead
- **Cons:** Larger code change, risk of regressions

### Option C: Disable CUDA graphs
- **Action:** Add torch.compiler.disable_cudagraphs decorator
- **Pros:** Quick fix
- **Cons:** Defeats performance optimization goal

## Next Actions

1. Implement Option A (clone fix) in next debugging loop
2. Verify:
   - CUDA benchmarks run without errors
   - CPU performance unchanged (±5% tolerance)
   - Gradient flow preserved (torch.autograd.gradcheck passes)
3. Re-run P3.3 CUDA benchmarks after fix
4. Proceed to P3.5 (performance summary and go/no-go decision)

## Related Items

- **fix_plan.md:** [PERF-PYTORCH-005-CUDAGRAPHS] — new item tracking this blocker
- **fix_plan.md:** [PERF-PYTORCH-004] Attempt #18 — documents discovery
- **Core Rule #16:** PyTorch Device & Dtype Neutrality
- **Testing Strategy §1.4:** PyTorch Device & Dtype Discipline

## References

- Plan: `plans/active/perf-pytorch-compile-refactor/plan.md` (P3.3 blocked)
- Error log: Terminal output from 2025-09-30 21:41:18 UTC
- Code location: `src/nanobrag_torch/simulator.py:345`
