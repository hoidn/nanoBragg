Already up to date.
Waiting for expected_actor=galph...
Already up to date.
[sync] state: actor=galph status=complete iter=63
To github.com:hoidn/nanoBragg.git
   0e92dd8..73498df  HEAD -> feature/spec-based-2
$ codex exec -m gpt-5-codex -c model_reasoning_effort=high --dangerously-bypass-approvals-and-sandbox
[2025-10-07T02:31:17] OpenAI Codex v0.38.0 (research preview)
--------
workdir: /Users/ollie/Documents/nanoBragg3
model: gpt-5-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
--------
[2025-10-07T02:31:17] User instructions:
<role>
planning, review and analysis. do not make code changes.
</role>
<current long-term goals>
in order of decreasing priority:
<1>
run a successful parallel test of pytorch nanobragg against nanobragg c with this command:

nanoBragg  -mat A.mat -floatfile img.bin -hkl scaled.hkl  -nonoise  -nointerpolate -oversample 1  -exposure 1  -flux 1e18 -beamsize 1.0  -spindle_axis -1 0 0 -Xbeam 217.742295 -Ybeam 213.907080  -distance 231.274660 -lambda 0.976800 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527 -odet_vector -0.000088 0.004914 -0.999988 -sdet_vector -0.005998 -0.999970 -0.004913 -fdet_vector 0.999982 -0.005998 -0.000118 -pix0_vector_mm -216.336293 215.205512 -230.200866  -beam_vector 0.00051387949 0.0 -0.99999986  -Na 36  -Nb 47 -Nc 29 -osc 0.1 -phi 0 -phisteps 10 -detector_rotx 0 -detector_roty 0 -detector_rotz 0 -twotheta 0

this will require first adding support for the following cli params to pytorch nanobragg:
-nonoise
-pix0_vector_mm

</1>
<2>
review and firm up if needed plans/active/vectorization.md. then implement it (or rather, delegate its implementation to ralph), starting with the tricubic interpolation vectorization 
</2>
</current long-term goals>
</past long term goals>
(just for archival / reference purposes):
- error-correcting the engineer agent 
- profiling pytorch nanobragg to find performance bottlenecks. analyzing to understand the CRITICAL slowness of the pytorch implementation compared to reference C, which persists after our recent improvement attempts
- finding performance issues in the PyTorch reimplementation of nanobragg (originally a C code) and speeding it up. It should be efficiently vectorized and faster than C nanobragg, but currently it's slower 
- ensuring that the pytorch implementation uses fp32 (not fp64) by default 
- understanding why pytorch is slower than C in the warm condition at 4096x4096 resolution and fixing the underlying performance issue, if one is found. THIS IS A CRITICAL PERFORMANCE ISSUE.
- once all the above are taken care of: building a user-facing showcase of autodiff-based parameter refinement, with plots / visuals

</past long-term goals>
<task>
You are galph, a planner / supervisor agent. you are overseeing the work of an agent (ralph) that is running prompts/main.md in a loop, using docs/fix_plan.md as its instruction set and long term memory. 

You will get invoked repeatedly. Use galph_memory.md to communicate with your future self. You'll plans under plans/, when needed, to help steer multi-turn efforts by the coder agent (ralph). Those plans will be cross referenced from docs/fix_plan.md so that ralph can find / read them. 

At the start of every invocation:
- Run `timeout 30 git pull --rebase` with a hard 30-second timeout  to sync with origin before reviewing context. If the command times out, immediately abort any partial rebase (`git rebase --abort`) and fall back to a normal merge via `git pull --no-rebase`. Whatever path you take, resolve resulting conflicts (docs/fix_plan.md is a frequent hotspot) and document key decisions in galph_memory.md. If the pull reports conflicts:
  * Run `git status --short` to list conflicted files.
  * Fix each file (remove conflict markers, keep the intended content) and stage it with `git add`.
  * When the index is clean, resume with `timeout 30 git rebase --continue --no-edit`. IMPORTANT: NEVER RUN THIS COMMAND WITHOUT TIMEOUT
- After resolving any conflicts, read the latest galph_memory.md entry (and any linked plan files) so you do not lose past context.
- Prune or summarize older entries when they become stale or redundant.

Before concluding each invocation:
- Append a concise update to galph_memory.md capturing key findings, decisions, and open questions (reference file paths or plan names).
- Note any follow-up actions you expect Ralph to take.
- If no substantial update is needed, explicitly log that fact so future runs know context was reviewed.
</task>

<instructions>
<0>
READ the following files:
- Index of project documentation: `./docs/index.md`
- $SPECS: `./specs/spec-a.md`
- $ARCH: `./arch.md` (ADR-backed implementation architecture; reconcile design with spec, surface conflicts)
- docs/development/c_to_pytorch_config_map.md — C↔Py config parity and implicit rules
-- docs/debugging/debugging.md — Parallel trace-driven debugging SOP
- $PLAN: `./fix_plan.md` (living, prioritized to‑do; keep it up to date)
- $TESTS: `./docs/development/testing_strategy.md` (testing philosophy, tiers, seeds/tolerances, commands)
- Set `AUTHORITATIVE_CMDS_DOC=./docs/development/testing_strategy.md` (or project‑specific path) and consult it for authoritative reference commands and test discovery.
<0>
<1>
do a deep analysis of the codebase in light of the <current long term goals>. What are some current issues / gaps and possible approaches to resolving them? Review docs/fix_plan.md and plans/active/, as previous iterations of you may have already done some legwork.
 


Debugging Items — Hypothesis + Triage (initial pass):
- Only when the selected item is a debugging or parity analysis task, formulate 1–3 plausible hypotheses for the observed gap/issue.
- Triage each with checks appropriate to scope using existing artifacts or quick, documented reproductions; this triage may constitute the primary work for the iteration. Record outcomes succinctly in the evidence.
- For the top hypothesis, state confidence and the single confirming step to run next. Deliver 1–3 ranked hypotheses with supporting evidence, artifact paths, and the next confirming step.
</1>
<2>
flip a coin using python. if it comes up <heads>:
review ralph's work over the last N (~10 but can be more or less - you decide) iterations. Check the commit history. Has the work been productive? Have there been regressions? Do we need to provide any feedback / course-correction?
</heads>
if it comes up <tails>: proceed to step <3>
</2>
<3>
Given your findings in <1> and <2>, think about whether there's any need for a multi-turn planning effort -- i.e. ralph can't see the forest for the trees and may struggle with major refactorings and multi-turn implementation efforts unless they are coordinated by you. Is there a need for such planning *right now*? If such a plan is already spoken for (plans/active/ or wherever else past galph might have saved to), does that plan require updates or is it up to date / high quality and simply pending? IFF you determine the need for a new plan or modification of an existing one:
<yes case>
- we will be calling the plan topic the <focus issue> of this turn.
- based on which long term <goal> and sub-goal is that effort / plan? 
- Which existing docs/fix_plan.md items does it (i.e. the <focus issue>) relate to? 
- Documentation review for <focus issue>: Using `docs/index.md` as the map, identify and read the specific documents relevant to the chosen <focus issue> (e.g., component contracts, architecture ADRs, parity/testing strategy). List the file paths you will rely on (with a one‑line rationale each) before drafting or updating the plan.
- think deeply. draft / redraft the plan and save it to a .md under plans/active/. Structure the write-up as a phased implementation document (see `plans/archive/general-detector-geometry/implementation.md` for tone/shape): begin with context + phase overviews, then outline each phase’s intent, prerequisites, and exit criteria. When a phase benefits from explicit tracking, embed a checklist table using the `ID | Task Description | State | How/Why & Guidance` format (with `[ ]`, `[P]`, `[D]` markers) inside that phase section.
  • Include reproduction commands, owners (if known), and decision rules in the guidance column.
  • Favor narrative flow first; layer checklists only where they clarify verification steps or deliverables.
  • Mini-template to crib when drafting:
    ```md
    ## Context
    - Initiative: <initiative>
    - Phase Goal: <outcome>
    - Dependencies: <docs/tests>

    ### Phase A — <short title>
    Goal: <what this phase proves or delivers>
    Prereqs: <artifacts or measurements required before starting>
    Exit Criteria: <verifiable completion signal>

    | ID | Task Description | State | How/Why & Guidance |
    | --- | --- | --- | --- |
    | A1 | <Key diagnostic or implementation step> | [ ] | Run `<command>`; capture results under `reports/<date>/...`. |
    | A2 | <Follow-up validation> | [ ] | Compare against `<artifact>`; stop if deviation > threshold. |
    ```
- When refreshing an existing plan, retrofit it to this phased format before adding or editing tasks.
- review docs/fix_plan.md. edit if needed. cross reference the new plans .md so that ralph can find it.
</yes case>
</no case>
- Since you decided there's no need for planning, you will instead focus on review / housekeeping. 
- Once the above is done (or deemed unneeded): review and evaluate ralph's work. Scrutinize the commit history. Look at the diffs. 
- Are the docs/fix_plan.md contents and priorities sane? things to consider:
  - if docs/fix_plan.md is longer than 1000 lines it should be housecleaned. If it's disorganized and / or internally inconsistent, consider how this could be addressed. 
- after considering the above, you have enough information to choose a <focus issue> for this turn. do so. Consider the nature of the <focus issue>:
    - Documentation review for <focus issue>: From `docs/index.md`, enumerate and read the documents that are most relevant to the chosen <focus issue>`; note the key file paths you will rely on for this turn.
    - Do we need a new docs/fix_plan item to put ralph back on course, fix one of his mistakes, or instruct him to do something that he overlooked? If so, draft it and add it to docs/fix_plans
    - does the <focus issue> involve identified issues in docs/fix_plan.md? If so, fix them. If you decide to shorten docs/fix_plan.md, the least relevant portions should be moved to archive/fix_plan_archive.md (with summary + archive cross reference if appropriate)
</no case>
</3>
<3.5>
Render and write ./input.md (supervisor→engineer steering memo). Overwrite the entire file every invocation. Keep length ~100–200 lines. Include these sections in order:
Header:
- Summary: <one‑sentence goal for this loop>
 - Mode: <TDD | Parity | Perf | Docs | none>
- Focus: <plan item ID/title from docs/fix_plan.md>
- Branch: <expected branch>
- Mapped tests: <validated list | none — evidence‑only>
- Artifacts: <key paths to produce under reports/>
- Do Now: 1 line naming the exact docs/fix_plan.md item (ID and title) to execute this loop, plus the exact pytest command/env to reproduce it (only when an authoritative mapping exists). If no such test exists, the Do Now MUST be to author the minimal targeted test first and then run it. If you intend to delegate the choice, write “Do Now: delegate” and provide decision guidance below.
  - Note: If operating in supervisor evidence mode (internal-only), do not run the full suite. Allowed: pytest --collect-only to validate selectors; in TDD mode only, run one targeted selector to confirm a new failing test. All other test execution is deferred to Ralph. Do not include any Phase label in input.md.
<Do Now guidelines>
Supervisor Evidence Gate (internal-only): No full pytest runs. Allowed: non-mutating probes and CLI validation tools (e.g., scripts/validation/*, nb-compare, pixel trace); pytest --collect-only to validate selectors; and in TDD mode only, a single targeted pytest run to confirm the newly authored test fails. Do not change production code. This gate applies only to galph. Ralph is exempt. Do not include any Phase label in input.md.
Verification scripts: You may run nb-compare and scripts/validation/* to collect metrics and artifacts (no code changes). Record outputs under reports/.
Mapped tests under supervisor evidence mode: Include exact selectors in input.md; validate with --collect-only; do not run them (except the TDD failure confirmation). Do not include any Phase label in input.md.
Mode flags: Mode flags are combinable and refine execution; do not include any Phase label in input.md.
TDD mode (supervisor-scoped): When operating in supervisor evidence mode, author/update a single minimal failing test that encodes the acceptance criterion. Confirm it fails via a targeted pytest selector; record the selector and expected failure text in input.md. Do not change production code, and do not include any Phase label in input.md.
Command Sourcing (tests): Only include an exact pytest command in Do Now when sourced from an authoritative mapping (e.g., docs/development/testing_strategy.md) or an existing, known test file/identifier. If no authoritative mapping exists, set the Do Now task to author the minimal targeted test first; do not guess at a pytest selector here.
</Do Now guidelines>
- If Blocked: fallback action (what to run/capture, how to record it in Attempts History).
- Priorities & Rationale: 3–6 bullets with file pointers (specs/arch/tests/plan) justifying the Do Now.
- How-To Map: exact commands (pytest, nb-compare), env vars, ROI/thresholds, and where to store artifacts; prefer using `scripts/validation/*` when available.
- Pitfalls To Avoid: 5–10 crisp do/don’t reminders (device/dtype neutrality, vectorization, Protected Assets, two‑message loop policy, no ad‑hoc scripts).
- Pointers: file paths with line anchors to the most relevant spec/arch/testing_strategy/fix_plan entries for this loop.
- Next Up (optional): 1–2 candidates Ralph may choose next loop if he finishes early (still one thing per loop).

 Rules:
 - Ralph must not edit input.md. You (galph) are the single writer.
 - If input.md’s Do Now conflicts with an existing in_progress selection in fix_plan.md, Ralph is allowed to switch; he must record the change in Attempts History.
- Do not instruct Ralph to run the full test suite more than once per turn. Require targeted tests first; run the full suite once at the end only if code changed. For prompt/docs‑only loops, require `pytest --collect-only -q` instead.
- Prefer referencing reusable scripts under `scripts/validation/` from Do Now when available.
- Commit input.md each run as part of step <4> (commit only if the content changed).
</3.5>
<4>
Before finishing the loop, enforce git hygiene:
- Run `git status --short` to inspect new or modified files that appeared during this invocation.
- Review diffs for each change; revert only if you created it this loop and it was accidental.
- Stage intentional updates with `git add -A` and commit via `git commit -m "SUPERVISOR: <scope> - <tests or rationale>"`, noting any tests run (use `tests: not run` when you skip them).
- After committing, run `git push` to share the updates. If the push is rejected, `timeout 30 git pull --rebase`, resolve conflicts (capture decisions—especially for docs/fix_plan.md—in galph_memory.md), then push again.
- If you deliberately leave the tree dirty, document the rationale in galph_memory.md so the next invocation knows why.
</4>
</instructions>
<notes>
- ignore 'routing violations'. these are out of scope.
- ignore AT parallel 012-related items for now
</notes>
Now carefully and exhaustively follow your <instructions>.


[2025-10-07T02:31:23] thinking

**Preparing environment and update plan**
[2025-10-07T02:31:24] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && timeout 30 git pull --rebase' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:26] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && timeout 30 git pull --rebase' succeeded in 2.10s:
Already up to date.
[2025-10-07T02:31:26] tokens used: 13,212

[2025-10-07T02:31:29] thinking

**Preparing to read galph_memory.md**
[2025-10-07T02:31:30] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:30] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat galph_memory.md' succeeded in 681ms:
## 2025-09-30 (loops 1-4 summary)
- Established parity backlog: AT-020/021/022/024 failing; created first supervisor plan for AT-021 and enforced prompts/debug.md routing.
- Diagnosed AT-024 random misset cap mismatch (π/2 vs 90 radians) and opened dedicated plan; insisted Ralph stop verification-only loops.
- Flagged torch.compile warm-up overhead (per-instance compilation, inline tensor factories) and opened AT-012 rotation parity plan (Phases A–E).
- Documented performance hotspots (`_compute_physics_for_position` guard tensors, `Crystal.compute_cell_tensors` clamps) and chronic dirty worktree artifacts blocking `git pull --rebase`.

## 2025-09-30 (loops 5-10 summary)
- Authored repo hygiene checklist (REPO-HYGIENE-002) to undo accidental `golden_suite_generator/nanoBragg.c` churn and purge stray reports.
- Created PERF-PYTORCH-004 plan to hoist constants, introduce compile caching, and remove Dynamo graph breaks; reiterated need for prompts/debug.md while parity fails.
- Repeatedly documented routing violations from Ralph and emphasized restoring Core Rule #13 metric duality after WIP commit 058986f swapped `V_actual` for formula volume.
- Noted that `_compute_physics_for_position` still recompiles each Simulator and that benchmarks in `reports/benchmarks/20250930-004916/` show ≥200× slowdown vs C for small detectors.

## 2025-09-30 (ralph debugging loop — AT-012 float32 regression)
- **Context**: AT-PARALLEL-012 test_simple_cubic_correlation failing (43/50 peaks matched, need ≥95%) after DTYPE-DEFAULT-001 (commit 8c2ceb4) switched simulator to native float32.
- **Investigation**:
  * Reproduced: 43/50 peaks matched (86%), corr≈1.0 (perfect physics) ✅
  * Parity matrix test: PASSES perfectly ✅ (live C↔Py comparison with current float32)
  * **Root cause identified**: PyTorch float32 produces 7× more unique values (4901 vs C's 669) in hot pixels, fragmenting intensity plateaus. Peak detection algorithm (scipy.ndimage.maximum_filter) is sensitive to plateau structure.
  * Parallel comparison: C float32 beam center (20×20 ROI) has 91 unique values; PyTorch has 331 (3.6× fragmentation). Max absolute diff tiny (0.0017).
  * **First Divergence**: Numerical accumulation patterns differ (likely FMA, compiler opts, vectorized accumulation order, or torch.compile kernel fusion) → perfect correlation but incompatible plateau structure.
[2025-10-07T02:31:30] tokens used: 14,414

[2025-10-07T02:31:35] thinking

**Preparing to review key documentation files**
[2025-10-07T02:31:36] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/index.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:36] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/index.md" succeeded in 700ms:
# Project Documentation Index

Welcome to the central index for all `nanoBragg-PyTorch` project documentation. This guide provides a map to the key technical specifications, development processes, and historical records that define the project.

---

## 🏛️ Permanent Documentation

These are the core, living documents that guide the project.

### Core Project Guides
### PyTorch Guardrails
* **[PyTorch Runtime Checklist](./development/pytorch_runtime_checklist.md)** – quick do/don’t list for vectorization and device placement. Always review before editing simulator code.
* **[PyTorch Design](./architecture/pytorch_design.md#vectorization-strategy)** – detailed explanation of the required broadcast shapes and batched flows.
* **[Testing Strategy §1.4](./development/testing_strategy.md#14-pytorch-device--dtype-discipline)** – required CPU/GPU smoke tests and compile checks.

* **[CLAUDE.md](../CLAUDE.md)** - The primary instruction set for the AI agent.
* **[README.md](../README.md)** - The main project entry point.
* **[README_PYTORCH.md](../README_PYTORCH.md)** - Comprehensive user guide for the PyTorch implementation, including CLI usage, parallel comparison tools, and visualization.
* **[PROJECT_STATUS.md](../PROJECT_STATUS.md)** - Tracks the current active development initiative.
[2025-10-07T02:31:36] tokens used: 17,422

[2025-10-07T02:31:39] thinking

**Previewing spec-a.md**
[2025-10-07T02:31:40] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' specs/spec-a.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:40] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' specs/spec-a.md" succeeded in 721ms:
# nanoBragg Spec A — Sharded Specification

Spec A is now organized across multiple shards. Read them together for the full normative contract.

- [spec-a-core.md](spec-a-core.md): Core physics, geometry, sampling, data, and associated acceptance tests.
- [spec-a-cli.md](spec-a-cli.md): CLI surface area, precedence rules, and CLI-focused acceptance tests.
- [spec-a-parallel.md](spec-a-parallel.md): Conformance profiles, parallel validation suites, and developer tooling requirements.

Each shard cross-references the others; updates must keep them consistent.
[2025-10-07T02:31:40] tokens used: 20,039

[2025-10-07T02:31:42] thinking

**Reviewing specifications and architecture docs**
[2025-10-07T02:31:43] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' arch.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:43] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' arch.md" succeeded in 728ms:
# nanoBragg PyTorch — Implementation Architecture

Document version: 1.0 (aligned with specs/spec-a.md)
Target spec: specs/spec-a.md (single-file normative spec)
Implementation target: Python (>=3.11) + PyTorch (float32 default). Type blocks may use TypeScript-style for clarity; implement with Python dataclasses and torch tensors.

## Running The CLI

- Install (recommended):
  - `pip install -e .` from repo root (provides `nanoBragg` console script).
- Run (installed):
  - `nanoBragg -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Run (without install):
  - `PYTHONPATH=src python -m nanobrag_torch -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Minimal quick test (no HKL):
  - `nanoBragg -default_F 1 -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -detpixels 128 -pgmfile test.pgm`

Notes
- Entry point: `src/nanobrag_torch/__main__.py` (module runner `python -m nanobrag_torch`).
- Required inputs: provide `-hkl` (or existing `Fdump.bin`) or set `-default_F > 0`, and either `-mat` (MOSFLM A*) or `-cell`.
[2025-10-07T02:31:43] tokens used: 20,401

[2025-10-07T02:31:45] thinking

**Reviewing development documentation**
[2025-10-07T02:31:45] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/development/c_to_pytorch_config_map.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:46] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/development/c_to_pytorch_config_map.md" succeeded in 731ms:
# C-CLI to PyTorch Configuration Map

## Critical Importance Notice

**This document is the authoritative source of truth for configuration parity between nanoBragg.c and the PyTorch implementation.**

Before writing any test or implementation that involves C-code validation, you **MUST** consult this document. Failure to ensure 1:1 configuration parity is the most common source of bugs, particularly with:
- Implicit pivot mode logic
- Convention-dependent beam center calculations
- Rotation axis defaults
- Unit conversions

## Quick Reference Table

### Crystal Parameters

| C-CLI Flag | PyTorch Config Field | C Variable | Units/Convention | Critical Notes |
|------------|---------------------|------------|------------------|----------------|
| `-cell a b c al be ga` | `CrystalConfig.cell_a/b/c/alpha/beta/gamma` | `a[0], b[0], c[0], alpha, beta, gamma` | Å and degrees → radians | Must convert degrees to radians internally |
| `-N <val>` | `CrystalConfig.N_cells` | `Na, Nb, Nc` | Number of unit cells | Sets all three axes to same value |
[2025-10-07T02:31:46] tokens used: 23,261

[2025-10-07T02:31:47] thinking

**Opening debugging documentation**
[2025-10-07T02:31:48] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/debugging/debugging.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:49] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/debugging/debugging.md" succeeded in 704ms:
# nanoBragg PyTorch Debugging Guidelines

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** PyTorch Development Team

## Overview

This document outlines the debugging methodology and tools for the nanoBragg PyTorch implementation. The debugging approach is built around a parallel trace comparison between the C code and the PyTorch implementation to ensure correctness and facilitate rapid issue resolution.

## Core Debugging Philosophy

### Parallel Trace Comparison Rule

**CRITICAL:** All debugging of physics discrepancies MUST begin with a parallel trace comparison. This involves:
1. Generating a detailed, step-by-step log of intermediate variables from the original C code for a specific pixel (the "Golden Trace").
2. Generating an identical log from the PyTorch implementation using `scripts/debug_pixel_trace.py`.
3. Comparing these two logs line-by-line to find the first point of divergence.

This methodology is mandatory to avoid guesswork and ensure bugs are found deterministically.
[2025-10-07T02:31:49] tokens used: 25,134

[2025-10-07T02:31:51] thinking

**Reviewing fix_plan.md document**
[2025-10-07T02:31:52] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,200p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:53] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,200p' docs/fix_plan.md" succeeded in 693ms:
# Fix Plan Ledger

**Last Updated:** 2025-10-17 (galph loop)
**Active Focus:**
- ROUTING: Close the reopened guard plan by capturing a fresh regression audit referencing commit `c49e3be` and re-confirming the guarded `loop.sh` flow (`plans/active/routing-loop-guard/plan.md` Phases A–C) before automation resumes.
- ROUTING-SUPERVISOR: Launch Phase A of `plans/active/supervisor-loop-guard/plan.md`, then drive Phase B guard work (B2–B4) and new task B5 to add `supervisor.sh` to docs/index.md so Protected-Asset policy covers the script before automation runs again.
- AT-012: Plan archived (`plans/archive/at-parallel-012-plateau-regression/plan.md`); monitor for regressions using `reports/2025-10-AT012-regression/phase_c_validation/` artifacts and re-open only if peak matches drop below spec.
- GRADIENT: Execute `plans/active/gradcheck-tier2-completion/plan.md` Phase A (A1–A3 baseline audit + env alignment) before adding misset/beam gradchecks from Phases B/C; once pass logs exist, close `[AT-TIER2-GRADCHECK]` with Phase D documentation updates.
- DTYPE: ✅ Complete. Plan archived to `plans/archive/dtype-default-fp32/`. All phases (A-D) finished; float32 defaults documented in arch.md, pytorch_runtime_checklist.md, CLAUDE.md, prompts/debug.md.
- PERF: Land plan task B7 (benchmark env toggle fix), rerun Phase B6 ten-process reproducibility with the new compile metadata, capture the weighted-source parity memo feeding C5, then execute Phase C diagnostics (C1/C2 plus C8/C9 pixel-grid & rotated-vector cost probes, and new C10 mosaic RNG timing) ahead of Phase D caching work (D5/D6/D7) and detector-scalar hoisting (D8).

## Index
| ID | Title | Priority | Status |
| --- | --- | --- | --- |
| [PROTECTED-ASSETS-001](#protected-assets-001-docsindexmd-safeguard) | Protect docs/index.md assets | Medium | in_progress |
| [REPO-HYGIENE-002](#repo-hygiene-002-restore-canonical-nanobraggc) | Restore canonical nanoBragg.c | Medium | in_progress |
| [PERF-PYTORCH-004](#perf-pytorch-004-fuse-physics-kernels) | Fuse physics kernels | High | in_progress |
| [AT-TIER2-GRADCHECK](#at-tier2-gradcheck-implement-tier-2-gradient-correctness-tests) | Implement Tier 2 gradient correctness tests | High | in_progress |
| [VECTOR-TRICUBIC-001](#vector-tricubic-001-vectorize-tricubic-interpolation-and-detector-absorption) | Vectorize tricubic interpolation and detector absorption | High | in_planning |
| [CLI-DTYPE-002](#cli-dtype-002-cli-dtype-parity) | CLI dtype parity | High | in_progress |
[2025-10-07T02:31:53] tokens used: 27,361

[2025-10-07T02:31:55] thinking

**Identifying key docs to read**
[2025-10-07T02:31:56] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/development/testing_strategy.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:31:56] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' docs/development/testing_strategy.md" succeeded in 717ms:
# nanoBragg PyTorch Testing Strategy

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** [Your Name/Team]

## 1. Introduction & Philosophy

This document outlines the comprehensive testing strategy for the PyTorch implementation of nanoBragg. The primary goal is to ensure that the new application is a correct, verifiable, and trustworthy scientific tool.
Our testing philosophy is a three-tiered hybrid approach, designed to build confidence layer by layer:

1. **Tier 1: Translation Correctness:** We first prove that the PyTorch code is a faithful, numerically precise reimplementation of the original C code's physical model. The C code is treated as the "ground truth" specification.
2. **Tier 2: Gradient Correctness:** We then prove that the new differentiable capabilities are mathematically sound.
3. **Tier 3: Scientific Validation:** Finally, we validate the model against objective physical principles.

All tests will be implemented using the PyTest framework.

### 1.4 PyTorch Device & Dtype Discipline

- **Device-neutral code:** Every PyTorch path MUST operate correctly on both CPU and CUDA tensors and across supported dtypes. Do not hard-code `.cpu()`/`.cuda()` calls, create CPU-only constants, or assume float64 execution when the caller may supply float32/half tensors.
[2025-10-07T02:31:56] tokens used: 55,059

[2025-10-07T02:31:59] thinking

**Listing active plans directory**
[2025-10-07T02:31:59] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls plans/active' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:00] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls plans/active' succeeded in 769ms:
cli-noise-pix0
gradcheck-tier2-completion
perf-pytorch-compile-refactor
routing-loop-guard
supervisor-loop-guard
unification.md
vectorization.md
[2025-10-07T02:32:00] tokens used: 57,716

[2025-10-07T02:32:05] thinking

**Preparing for vectorization analysis**

I'm gathering instructions and goals focused on vectorization, starting with a deep review of the vectorization plan and related docs. I want to understand long-term objectives like parallel testing and CLI parameter support before moving onto next steps like reviewing recent work or flipping a coin. I'll open the main plan file to dig in.
[2025-10-07T02:32:07] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat plans/active/vectorization.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:08] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat plans/active/vectorization.md' succeeded in 733ms:
## Context
- Initiative: PERF-PYTORCH-004 vectorization backlog; supports long-term goal "Parallel parity + performance" from docs/fix_plan.md
- Phase Goal: Deliver a production-ready, fully vectorized tricubic interpolation pipeline (gather + polynomial evaluation) and follow with detector absorption vectorization without regressing physics, gradients, or device neutrality.
- Dependencies: specs/spec-a-core.md §4 (Structure factors), specs/spec-a-parallel.md §2.3 (Interpolation acceptance tests), docs/architecture/pytorch_design.md §§2.2–2.4, docs/architecture/c_code_overview.md (tricubic routines), docs/development/testing_strategy.md §§2–4, docs/development/pytorch_runtime_checklist.md, `nanoBragg.c` lines 2604–3278 (polin3/polin2/polint + detector absorption), `tests/test_at_str_002.py`, `tests/test_at_abs_001.py`, `reports/benchmarks/20250930-165726-compile-cache/` (current performance references). No dedicated tricubic/absorption microbenchmarks exist yet; Phase A will author reusable harnesses under `scripts/benchmarks/` so future loops can rerun baselines without ad-hoc snippets.
- Gap Snapshot (2025-11-09): Phase A remains untouched; no artifacts exist under `reports/2025-10-vectorization/phase_a/`, so Ralph must prioritise A1–A3 baseline capture before any implementation edits. docs/fix_plan.md item VECTOR-TRICUBIC-001 lists these as the gating next actions.

### Phase A — Evidence & Baseline Capture
Goal: Document the current (non-vectorized) behavior, warnings, and performance so we can prove the impact of the refactor.
Prereqs: Editable install (`pip install -e .`), C binary available for parity (when needed), environment variable `KMP_DUPLICATE_LIB_OK=TRUE`.
Exit Criteria: Baseline pytest log, timing snippet results, and notes stored under `reports/2025-10-vectorization/phase_a/` with references in docs/fix_plan.md Attempt history.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| A1 | Re-run tricubic acceptance tests | [ ] | `env KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_at_str_002.py -v` ⇒ save log to `reports/2025-10-vectorization/phase_a/test_at_str_002.log`; confirm nearest-neighbour fallback warning message appears for batched inputs. |
| A2 | Measure current tricubic runtime | [ ] | Author `scripts/benchmarks/tricubic_baseline.py` (device/dtype neutral, honours `KMP_DUPLICATE_LIB_OK=TRUE`) and run `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/tricubic_baseline.py --sizes 256 512 --repeats 5`. Capture CPU timings (and CUDA when available) in `phase_a/tricubic_baseline.md` plus raw JSON/CSV under `phase_a/tricubic_baseline_results.json`. |
| A3 | Record detector absorption baseline | [ ] | Create `scripts/benchmarks/absorption_baseline.py` mirroring A2's interface; execute `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/absorption_baseline.py --sizes 256 512 --repeats 5` alongside `env KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_at_abs_001.py -v`. Store logs in `phase_a/absorption_baseline.md` and metrics in `phase_a/absorption_baseline_results.json`. |

### Phase B — Tricubic Vectorization Design
Goal: Lock the tensor-shape design, broadcasting plan, and gradient checks before code changes.
Prereqs: Phase A artifacts uploaded and referenced in docs/fix_plan.md.
[2025-10-07T02:32:08] tokens used: 58,141

[2025-10-07T02:32:10] thinking

**Checking existing plan and CLI flags**
[2025-10-07T02:32:11] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls plans/active/cli-noise-pix0' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:12] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls plans/active/cli-noise-pix0' succeeded in 716ms:
plan.md
[2025-10-07T02:32:12] tokens used: 61,796
[2025-10-07T02:32:14] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:14] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat plans/active/cli-noise-pix0/plan.md' succeeded in 705ms:
## Context
- Initiative: CLI Parity for nanoBragg PyTorch vs C (supports long-term goal in prompts/supervisor.md)
- Phase Goal: Accept `-nonoise` and `-pix0_vector_mm` flags with C-equivalent semantics so the parallel comparison command in prompts/supervisor.md executes end-to-end.
- Dependencies: specs/spec-a-cli.md §§3.2–3.4, docs/architecture/detector.md §5, docs/development/c_to_pytorch_config_map.md (detector pivot + noise), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860 (flag behavior), docs/debugging/detector_geometry_checklist.md (pix0 validation), docs/development/testing_strategy.md §2 (CLI parity tests).
- Current gap snapshot (2025-10-18 refresh): Phase H4 parity landed (Attempt #25) with pix0 deltas < 2e-8 m and regression tolerances tightened; the remaining blocker before executing the supervisor command end-to-end is Phase I polarization alignment.
- Gap snapshot update (2025-10-19): Attempt #27 parity run reveals 1.24538e5× intensity scaling mismatch despite polarization fix; Phases J–L track the normalization diagnosis, fix, and closure required before long-term Goal #1 completes.
- Gap snapshot refresh (2025-10-22): Attempt #31 restored C precedence for custom detector vectors (pix0 override now gated again). Outstanding work: H5c PyTorch traces and Phase K normalization to eliminate the `F_latt`-driven intensity gap.
- Gap snapshot update (2025-10-24): Attempt #35 reran the PyTorch trace harness post-unit-fix; metrics still show |Δpix0| ≈ 1.145 mm, so a fresh diagnostics phase is required before resuming normalization work.
- Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
- Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
- Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
- Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
- Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
- Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
- Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
- Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
- Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
[2025-10-07T02:32:14] tokens used: 62,383

[2025-10-07T02:32:20] thinking

**Outlining detailed plan and next actions**
[2025-10-07T02:32:21] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && python - <<'PY'
import random
print(random.choice(['heads','tails']))
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:22] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && python - <<'PY'
import random
print(random.choice(['heads','tails']))
PY" succeeded in 711ms:
heads
[2025-10-07T02:32:22] tokens used: 65,852

[2025-10-07T02:32:25] thinking

**Preparing to review recent commits**
[2025-10-07T02:32:29] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git log --oneline -n 15' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:29] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git log --oneline -n 15' succeeded in 699ms:
73498df [SYNC i=63] actor=galph status=running
0e92dd8 [SYNC i=63] actor=ralph → next=galph status=ok ralph_commit=f466b39
f466b39 AT- CLI-FLAGS-003: Phase L2b blocker - Simulator API lacks trace instrumentation
db20691 [SYNC i=62] actor=ralph status=running
4200d97 [SYNC i=62] actor=galph → next=ralph status=ok galph_commit=7534383
7534383 SUPERVISOR: Phase L2 trace planning - tests: not run
04af859 [SYNC i=62] actor=galph status=running
255b404 [SYNC i=62] actor=ralph → next=galph status=ok ralph_commit=f3f116b
f3f116b CLI-FLAGS-003 Phase L2a: C scaling trace capture complete
bd59e66 [SYNC i=61] actor=ralph status=running
025c020 [SYNC i=61] actor=galph → next=ralph status=ok galph_commit=4119716
4119716 SUPERVISOR: CLI Phase L2 planning - tests: not run
500c82f [SYNC i=61] actor=galph status=running
11a8b87 [SYNC i=61] actor=ralph → next=galph status=ok ralph_commit=3e47b38
3e47b38 CLI-FLAGS-003 L1d: HKL parity verified with fresh C cache
[2025-10-07T02:32:29] tokens used: 66,180

[2025-10-07T02:32:32] thinking

**Reviewing specific commits**
[2025-10-07T02:32:32] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git show --stat f466b39' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:33] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git show --stat f466b39' succeeded in 727ms:
commit f466b390268449b949065ab0f539819d8f5dc7a4
Author: ollie <ohoidn>
Date:   Mon Oct 6 19:30:19 2025 -0700

    AT- CLI-FLAGS-003: Phase L2b blocker - Simulator API lacks trace instrumentation
    
    Evidence-only loop i=62 per input.md. Phase L2b requires PyTorch scaling
    trace to compare with C trace from Attempt #55 (Phase L2a), but current
    Simulator.run() API does not expose internal scaling factors
    (I_before_scaling, fluence, steps, omega_pixel, polar, capture_fraction).
    
    **Blocker:**
    - Simulator lacks trace_pixel parameter to enable selective per-pixel output
    - Internal scaling values computed but not returned/logged
    - Attempted custom harness failed due to API limitations (config/object
      circular dependencies, no internal value access)
    - CLI wrapper insufficient (only outputs final stats, no trace mode)
    
    **Artifacts:**
    - reports/2025-10-cli-flags/phase_l/scaling_audit/L2b_blocker.md: Full
[2025-10-07T02:32:33] tokens used: 66,806

[2025-10-07T02:32:37] thinking

**Committing documentation updates**
[2025-10-07T02:32:38] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git show --stat f3f116b' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:39] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git show --stat f3f116b' succeeded in 723ms:
commit f3f116b0cc6b505a7cbef15c083a4070c8a4d668
Author: ollie <ohoidn>
Date:   Mon Oct 6 19:09:05 2025 -0700

    CLI-FLAGS-003 Phase L2a: C scaling trace capture complete
    
    Executed Phase L2a per input.md directive. Instrumentation already present
    in C code (lines 3367-3382), no changes needed. Captured comprehensive
    scaling trace for pixel (685,1039) using supervisor command.
    
    Key Metrics (pixel 685,1039):
    - I_before_scaling: 943654.809
    - r_e_sqr: 7.941e-30 m²
    - fluence: 1e24 photons/m²
    - steps: 10
    - capture_fraction: 1.0
    - polar: 0.9146
    - omega_pixel: 4.204e-7 sr
    - I_pixel_final: 2.881e-7 photons
    
[2025-10-07T02:32:39] tokens used: 104,568

[2025-10-07T02:32:40] thinking

**Preparing next commit**
[2025-10-07T02:32:40] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git show --stat 3e47b38' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:41] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git show --stat 3e47b38' succeeded in 693ms:
commit 3e47b38d0c1c3fa5b84c48ce46e5dec58014ba13
Author: ollie <ohoidn>
Date:   Mon Oct 6 18:55:05 2025 -0700

    CLI-FLAGS-003 L1d: HKL parity verified with fresh C cache
    
    - Regenerated C Fdump cache using supervisor command flags (Na=36, Nb=47, Nc=29)
    - Achieved perfect HKL parity: max |ΔF|=0.0, 0 mismatches
    - Artifacts: Fdump_c_20251109.bin, summary_20251109.md, metrics_20251109.json
    - Phase L2 prerequisites met—structure factors match C exactly
    
    Tests: Collection verified (1 test collected in 2.12s)
    Metrics: Max |ΔF|=0.0 electrons, rel RMS=0.0, shape [49,57,62]
    Plan: Updated Phase L1d to [D] complete, logged Attempt #54
    Next: Phase L2 scaling-chain diagnostics

 docs/fix_plan.md                                   | 15 +++++-
 plans/active/cli-noise-pix0/plan.md                |  2 +-
 .../phase_l/hkl_parity/metrics_20251109.json       | 59 ++++++++++++++++++++
 .../phase_l/hkl_parity/summary_20251109.md         | 63 ++++++++++++++++++++++
[2025-10-07T02:32:41] tokens used: 106,055

[2025-10-07T02:32:45] thinking

**Reviewing recent commits and planning next checks**
[2025-10-07T02:32:47] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "CLI-FLAGS-003" -n docs/fix_plan.md | head' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:47] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "CLI-FLAGS-003" -n docs/fix_plan.md | head' succeeded in 721ms:
21:| [CLI-FLAGS-003](#cli-flags-003-handle-nonoise-and-pix0_vector_mm) | Handle -nonoise and -pix0_vector_mm | High | in_progress |
448:## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
[2025-10-07T02:32:47] tokens used: 107,174

[2025-10-07T02:32:49] thinking

**Inspecting specific code section**
[2025-10-07T02:32:49] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '448,640p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:32:50] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '448,640p' docs/fix_plan.md" succeeded in 745ms:
## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
- Priority: High
- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
- Owner/Date: ralph/2025-10-05
- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
- Reproduction (C & PyTorch):
  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
- First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
- Next Actions (2025-10-06 refresh):
  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
- Attempts History:
  * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
    Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
    Artifacts:
      - `reports/2025-10-cli-flags/phase_i/supervisor_command/README.md` - Full evidence report with metrics, hypotheses, next actions
      - `reports/2025-10-cli-flags/phase_i/supervisor_command/summary.json` - Complete parity metrics (correlation, RMSE, sums, peak distances)
[2025-10-07T02:32:50] tokens used: 107,489

[2025-10-07T02:33:02] thinking

**Assessing analysis and planning needs**

I'm gathering detailed info from commits and code to identify gaps blocking CLI parity and vectorization goals, especially missing PyTorch instrumentation and CLI flag support. I'll analyze to confirm issues before deciding on any plan updates or new steps.
[2025-10-07T02:33:03] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "trace" src/nanobrag_torch -n' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:33:04] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "trace" src/nanobrag_torch -n' succeeded in 931ms:
src/nanobrag_torch/__main__.py:356:    parser.add_argument('-trace_pixel', nargs=2, type=int,
src/nanobrag_torch/__main__.py:358:                        help='Instrument trace for a pixel')
src/nanobrag_torch/__main__.py:1086:            'trace_pixel': args.trace_pixel,  # [slow, fast] indices
src/nanobrag_torch/__main__.py:1111:        if args.trace_pixel:
src/nanobrag_torch/__main__.py:1112:            print(f"  Tracing pixel (slow={args.trace_pixel[0]}, fast={args.trace_pixel[1]})")
src/nanobrag_torch/__main__.py:1254:        import traceback
src/nanobrag_torch/__main__.py:1256:        traceback.print_exc()
src/nanobrag_torch/models/detector.py.backup:86:            # From C code trace: DETECTOR_FAST_AXIS 0 0 1, DETECTOR_SLOW_AXIS 0 -1 0, DETECTOR_NORMAL_AXIS 1 0 0
src/nanobrag_torch/models/detector.py:117:            # From C code trace: DETECTOR_FAST_AXIS 0 0 1, DETECTOR_SLOW_AXIS 0 -1 0, DETECTOR_NORMAL_AXIS 1 0 0
src/nanobrag_torch/models/detector.py:503:            # Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents
src/nanobrag_torch/simulator.py:445:            debug_config: Debug configuration with printout, printout_pixel, trace_pixel options
src/nanobrag_torch/simulator.py:488:        self.trace_pixel = self.debug_config.get('trace_pixel', None)  # [slow, fast]
src/nanobrag_torch/simulator.py:1109:        if self.printout or self.trace_pixel:
src/nanobrag_torch/simulator.py:1146:        """Apply debug output for -printout and -trace_pixel options.
src/nanobrag_torch/simulator.py:1193:        # Handle trace_pixel for detailed single-pixel trace
src/nanobrag_torch/simulator.py:1194:        if self.trace_pixel:
src/nanobrag_torch/simulator.py:1195:            # trace_pixel is [slow, fast] from CLI
src/nanobrag_torch/simulator.py:1196:            target_slow = self.trace_pixel[0]
src/nanobrag_torch/simulator.py:1197:            target_fast = self.trace_pixel[1]
[2025-10-07T02:33:04] tokens used: 111,354

[2025-10-07T02:33:06] thinking

**Inspecting trace_pixel support in simulator.py**
[2025-10-07T02:33:08] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1100,1250p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:33:09] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1100,1250p' src/nanobrag_torch/simulator.py" succeeded in 716ms:
                    mask_array = self.detector.config.mask_array.to(device=self.device, dtype=self.dtype)
                    roi_mask = roi_mask * mask_array
                # If mask doesn't match, skip it (for compatibility with tests)

        # Ensure roi_mask is on the same device as physical_intensity
        roi_mask = roi_mask.to(physical_intensity.device)
        physical_intensity = physical_intensity * roi_mask

        # Apply debug output if requested
        if self.printout or self.trace_pixel:
            # For debug output, compute polarization for single pixel case
            polarization_value = None
            if not oversample or oversample == 1:
                # Calculate polarization factor for the entire detector
                if self.beam_config.nopolar:
                    polarization_value = torch.ones_like(omega_pixel)
                else:
                    # Compute incident and diffracted vectors for debug output
                    # These are already calculated in the main flow but not available here
                    # Since this is only for debug output, we can skip it for now
[2025-10-07T02:33:09] tokens used: 112,221

[2025-10-07T02:33:10] thinking

**Verifying absence of scaling factor logs**
[2025-10-07T02:33:11] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1250,1400p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:33:12] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1250,1400p' src/nanobrag_torch/simulator.py" succeeded in 716ms:
                # Trace reciprocal space information if available
                if rot_a_star is not None and rot_b_star is not None and rot_c_star is not None:
                    # Output rotated real and reciprocal vectors (first phi/mosaic domain)
                    a_vec = rot_a[0, 0] if len(rot_a.shape) > 1 else rot_a
                    b_vec = rot_b[0, 0] if len(rot_b.shape) > 1 else rot_b
                    c_vec = rot_c[0, 0] if len(rot_c.shape) > 1 else rot_c

                    print(f"TRACE_PY: rot_a_angstroms {a_vec[0].item():.15g} {a_vec[1].item():.15g} {a_vec[2].item():.15g}")
                    print(f"TRACE_PY: rot_b_angstroms {b_vec[0].item():.15g} {b_vec[1].item():.15g} {b_vec[2].item():.15g}")
                    print(f"TRACE_PY: rot_c_angstroms {c_vec[0].item():.15g} {c_vec[1].item():.15g} {c_vec[2].item():.15g}")

                    a_star_vec = rot_a_star[0, 0] if len(rot_a_star.shape) > 1 else rot_a_star
                    b_star_vec = rot_b_star[0, 0] if len(rot_b_star.shape) > 1 else rot_b_star
                    c_star_vec = rot_c_star[0, 0] if len(rot_c_star.shape) > 1 else rot_c_star

                    print(f"TRACE_PY: rot_a_star_A_inv {a_star_vec[0].item():.15g} {a_star_vec[1].item():.15g} {a_star_vec[2].item():.15g}")
                    print(f"TRACE_PY: rot_b_star_A_inv {b_star_vec[0].item():.15g} {b_star_vec[1].item():.15g} {b_star_vec[2].item():.15g}")
                    print(f"TRACE_PY: rot_c_star_A_inv {c_star_vec[0].item():.15g} {c_star_vec[1].item():.15g} {c_star_vec[2].item():.15g}")

                    # Calculate Miller indices using scattering vector and real-space vectors
[2025-10-07T02:33:12] tokens used: 114,607

[2025-10-07T02:33:24] thinking

**Analyzing instrumentation and trace_pixel support**
[2025-10-07T02:33:27] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git blame -L 1193,1360 src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:33:28] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git blame -L 1193,1360 src/nanobrag_torch/simulator.py' succeeded in 1.05s:
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1193)         # Handle trace_pixel for detailed single-pixel trace
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1194)         if self.trace_pixel:
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1195)             # trace_pixel is [slow, fast] from CLI
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1196)             target_slow = self.trace_pixel[0]
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1197)             target_fast = self.trace_pixel[1]
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1198) 
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1199)             if 0 <= target_slow < physical_intensity.shape[0] and 0 <= target_fast < physical_intensity.shape[1]:
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1200)                 # Output pix0_vector first
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1201)                 pix0 = self.detector.pix0_vector
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1202)                 print(f"TRACE_PY: pix0_vector_meters {pix0[0].item():.15g} {pix0[1].item():.15g} {pix0[2].item():.15g}")
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1203) 
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1204)                 # Output detector basis vectors
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1205)                 fdet = self.detector.fdet_vec
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1206)                 sdet = self.detector.sdet_vec
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1207)                 print(f"TRACE_PY: fdet_vec {fdet[0].item():.15g} {fdet[1].item():.15g} {fdet[2].item():.15g}")
cacf1f8b (ollie 2025-10-05 18:11:48 -0700 1208)                 print(f"TRACE_PY: sdet_vec {sdet[0].item():.15g} {sdet[1].item():.15g} {sdet[2].item():.15g}")
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1209) 
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1210)                 # Trace coordinate information
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1211)                 if pixel_coords_meters is not None:
d7ee36f3 (ollie 2025-09-25 01:04:47 -0700 1212)                     coords = pixel_coords_meters[target_slow, target_fast]
[2025-10-07T02:33:28] tokens used: 118,278

[2025-10-07T02:33:36] thinking

**Evaluating instrumentation accuracy and planning updates**

I’m examining the current instrumentation and noticing it’s incomplete—key values like steps and capture fractions are placeholders, not reflecting actual computation, so previous conclusions about API accessibility might be misleading. I’m considering whether to refine or update the existing plan to address these gaps, and I’m also planning to review the vectorization plan details in the docs for possible updates.
[2025-10-07T02:33:38] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '280,420p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:33:39] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '280,420p' docs/fix_plan.md" succeeded in 972ms:
- Exit Criteria (quote thresholds from spec):
  * ✅ Phase 2 artifacts demonstrating ≥50× warm/cold delta for CPU float64/float32 and CUDA float32 (37–6428× achieved).
  * ✅ Phase 3 report showing PyTorch warm runs ≤1.5× C runtime for 256²–1024² detectors (CUDA: all pass; CPU: 256²/512² pass, 1024² documented).
  * ✅ Recorded go/no-go decision for Phase 4 graph work based on Phase 3 results (DEFER documented in `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md`).

---


## [PERF-PYTORCH-005-CUDAGRAPHS] CUDA graphs compatibility
- Spec/AT: Core Rule #16 (PyTorch Device & Dtype Neutrality), docs/development/pytorch_runtime_checklist.md §1.4
- Priority: High
- Status: done
- Owner/Date: ralph/2025-09-30 (resolved during PERF-PYTORCH-004 Attempt #19)
- Reproduction (C & PyTorch):
  * C: n/a (CUDA-specific PyTorch issue)
  * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/benchmark_detailed.py --sizes 256 --device cuda --iterations 2`
  * Shapes/ROI: Any detector size on CUDA device
- First Divergence (if known): RuntimeError (`accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run`) at `simulator.py:349` when torch.compile enables CUDA graphs.
- Attempts History:
  * [2025-09-30] Attempt #1 — Result: documented blocker. CUDA execution of P3.3 benchmarks crashed once torch.compile enabled CUDA graphs.
[2025-10-07T02:33:39] tokens used: 122,710

[2025-10-07T02:34:14] thinking

**Analyzing instrumentation and planning updates**

I'm examining key gaps in CLI scaling instrumentation, focusing on missing actual value computations for polarization and capture fraction in the Simulator's trace paths. Since placeholders cause mismatches, I’m considering updating the existing plan to add detailed tasks for modifying `_apply_debug_output` and Simulator instrumentation, ensuring accurate tracing aligned with long-term goals. I’ll also identify relevant documentation to reference before drafting the updated phased plan.
[2025-10-07T02:34:15] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "polarization" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:34:16] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "polarization" -n src/nanobrag_torch/simulator.py' succeeded in 706ms:
16:from .utils.physics import sincg, sinc3, polarization_factor
43:    apply_polarization: bool = True,
45:    polarization_axis: Optional[torch.Tensor] = None,
90:        apply_polarization: Whether to apply Kahn polarization correction (default True)
92:        polarization_axis: Polarization axis unit vector (3,) or broadcastable shape
324:    # PERF-PYTORCH-004 P3.0b: Apply polarization PER-SOURCE before weighted sum
325:    # IMPORTANT: Apply polarization even when kahn_factor==0.0 (unpolarized case)
327:    if apply_polarization:
328:        # Calculate polarization factor for each source
334:            # Multi-source case: apply per-source polarization
350:            # Flatten for polarization calculation
356:            # Calculate polarization
357:            polar_flat = polarization_factor(
361:                polarization_axis
370:            # Apply polarization per source
374:            # Flatten for polarization calculation
382:            # Calculate polarization
383:            polar_flat = polarization_factor(
387:                polarization_axis
396:            # Apply polarization
[2025-10-07T02:34:16] tokens used: 127,114

[2025-10-07T02:34:38] thinking

**Analyzing instrumentation update plan**
[2025-10-07T02:34:38] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "capture_fraction" -n src/nanobrag_torch' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:34:39] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "capture_fraction" -n src/nanobrag_torch' succeeded in 1.01s:
src/nanobrag_torch/simulator.py:1332:                    print(f"TRACE_PY: capture_fraction 1")
src/nanobrag_torch/simulator.py:1507:            capture_fractions = exp_start_all - exp_end_all  # Shape: (thicksteps, S, F)
src/nanobrag_torch/simulator.py:1515:            result = torch.sum(intensity_expanded * capture_fractions, dim=0)
src/nanobrag_torch/simulator.py:1522:            capture_fraction = exp_start - exp_end
src/nanobrag_torch/simulator.py:1524:            result = intensity * capture_fraction
[2025-10-07T02:34:39] tokens used: 129,450

[2025-10-07T02:34:46] thinking

**Analyzing capture fraction calculation**
[2025-10-07T02:34:47] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "total_steps" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:34:47] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "total_steps" -n src/nanobrag_torch/simulator.py' exited 1 in 704ms:

[2025-10-07T02:34:47] tokens used: 151,047

[2025-10-07T02:34:49] thinking

**Searching for total steps variable**
[2025-10-07T02:34:50] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "phi_steps" src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:34:51] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && rg "phi_steps" src/nanobrag_torch/simulator.py' succeeded in 730ms:
            if hasattr(crystal_config, 'phi_steps'):
                self.crystal.config.phi_steps = crystal_config.phi_steps
        phi_steps = self.crystal.config.phi_steps
        steps = source_norm * phi_steps * mosaic_domains * oversample * oversample  # Include sources and oversample^2
                    print(f"TRACE_PY: steps {self.crystal.config.phi_steps}")
[2025-10-07T02:34:51] tokens used: 151,786

[2025-10-07T02:34:55] thinking

**Updating instrumentation plans**
[2025-10-07T02:34:56] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:34:57] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && cat input.md' succeeded in 747ms:
Summary: Capture a PyTorch scaling trace for the supervisor command so Phase L normalization parity can proceed without touching production code.
Mode: Parity
Focus: CLI-FLAGS-003 Handle -nonoise and -pix0_vector_mm — Phase L2 scaling audit
Branch: feature/spec-based-2
Mapped tests: pytest --collect-only -q tests/test_cli_scaling.py::test_f_latt_square_matches_c (selector check only)
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log — primary TRACE_PY output
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json — environment snapshot
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md — timestamped command log
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stdout.txt — stdout capture
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stderr.txt — stderr capture
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_history.md — per-run summary
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json — CLI arg/param dump
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/readme.md — optional overview once trace succeeds
Do Now: CLI-FLAGS-003 Phase L2b — KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --out trace_py_scaling.log --config supervisor
If Blocked: If the harness raises during config or execution, capture stdout/stderr plus traceback into reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_failed.log, append a blocker note to attempt_history.md, and pause for supervisor guidance.
Context: Phase L2a (C trace) is complete and logged as Attempt #55; L2b is now the gating deliverable for long-term Goal #1.
Context: Evidence-only loop — no production code edits, no nb-compare, no full pytest runs beyond selector validation.
Context: Maintain Protected Assets policy; all new files must stay under reports/2025-10-cli-flags/phase_l/scaling_audit/.

Priorities & Rationale:
[2025-10-07T02:34:57] tokens used: 153,162

[2025-10-07T02:35:06] thinking

**Revising plan to enable code instrumentation**

I’m figuring out how to update the plan to move beyond evidence-only tracing by allowing code changes to add real instrumentation in the Simulator. This means adding detailed steps in Phase L2 to patch simulation methods for actual data logging, updating the context with a new gap snapshot date, and specifying necessary code locations to modify, all while keeping the new input concise and focused.
[2025-10-07T02:35:13] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/plans/active/cli-noise-pix0/plan.md
@@ -11,2 +11,3 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
@@ -238,3 +239,3 @@
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
[2025-10-07T02:35:13] apply_patch(auto_approved=true) exited 0 in 15ms:
Success. Updated the following files:
M plans/active/cli-noise-pix0/plan.md
[2025-10-07T02:35:13] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T02:35:13] tokens used: 156,189
[2025-10-07T02:35:13] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:35:18] thinking

**Updating plan and documentation**
[2025-10-07T02:35:22] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -457,6 +457,7 @@
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
[2025-10-07T02:35:22] apply_patch(auto_approved=true) exited 0 in 21ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T02:35:22] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T02:35:22] tokens used: 157,999
[2025-10-07T02:35:22] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:35:27] thinking

**Planning detailed input generation**

I'm working on designing new instructions for generating a 100-200 line input.md file that includes a structured header with summary, mode, focus, branch, mapped tests, artifacts, and next steps. The focus will target the CLI-FLAGS-003 task involving Phase L2 instrumentation updates, setting mode to Parity to reflect implementation aligned with C behavior, and confirming the actual git branch for accuracy.
[2025-10-07T02:35:29] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git status -sb' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:35:29] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git status -sb' succeeded in 953ms:
## feature/spec-based-2...origin/feature/spec-based-2
 M docs/fix_plan.md
 M plans/active/cli-noise-pix0/plan.md
[2025-10-07T02:35:29] tokens used: 159,197
[2025-10-07T02:35:29] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:35:35] thinking

**Planning targeted test instructions**
[2025-10-07T02:35:36] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls tests | head' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:35:36] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls tests | head' succeeded in 764ms:
__init__.py
__pycache__
conftest.py
golden_data
parity_cases.yaml
repomix-output.xml
test_at_abs_001.py
test_at_bkg_001.py
test_at_cli_001.py
test_at_cli_002.py
[2025-10-07T02:35:36] tokens used: 159,859
[2025-10-07T02:35:36] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:35:39] thinking

**Checking for specific test file**
[2025-10-07T02:35:39] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls tests | rg cli_scaling' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:35:40] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && ls tests | rg cli_scaling' succeeded in 705ms:
test_cli_scaling.py
[2025-10-07T02:35:40] tokens used: 160,043
[2025-10-07T02:35:40] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:35:43] thinking

**Checking for test names**
[2025-10-07T02:35:44] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' tests/test_cli_scaling.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:35:45] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && sed -n '1,160p' tests/test_cli_scaling.py" succeeded in 756ms:
"""
CLI Scaling Tests for CLI-FLAGS-003 Phase K

Tests the SQUARE lattice factor (F_latt) calculation correctness after fixing
the (h-h0) bug. Validates that PyTorch matches C for F_latt and I_before_scaling.

Evidence base: reports/2025-10-cli-flags/phase_k/f_latt_fix/
Plan reference: plans/active/cli-noise-pix0/plan.md Phase K1-K3
"""
import os
import sys
import json
import tempfile
import subprocess
from pathlib import Path
import pytest
import numpy as np

# Set required environment variable
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
[2025-10-07T02:35:45] tokens used: 160,224
[2025-10-07T02:35:45] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..a9b853f31d599bb146e0df062d93df32172b59d7
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:36:05] thinking

**Deciding test strategy for instrumentation changes**
[2025-10-07T02:36:09] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/plans/active/cli-noise-pix0/plan.md
@@ -239,3 +239,3 @@
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
[2025-10-07T02:36:09] apply_patch(auto_approved=true) exited 0 in 20ms:
Success. Updated the following files:
M plans/active/cli-noise-pix0/plan.md
[2025-10-07T02:36:09] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T02:36:09] tokens used: 164,016
[2025-10-07T02:36:09] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..12c01b3cb7b2d3595296b287f56b511fe6b0615b
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:36:13] thinking

**Considering updating fix plan with test addition**
[2025-10-07T02:36:16] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -459,3 +459,3 @@
   1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
   3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
[2025-10-07T02:36:16] apply_patch(auto_approved=true) exited 0 in 31ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T02:36:16] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T02:36:17] tokens used: 166,010
[2025-10-07T02:36:17] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:36:29] thinking

**Drafting new input.md file**
[2025-10-07T02:36:42] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' > input.md
Summary: Patch TRACE_PY instrumentation so the supervisor command scaling factors match C and unblock Phase L2.
Mode: Parity
Focus: CLI-FLAGS-003 Handle -nonoise and -pix0_vector_mm — Phase L2 scaling audit
Branch: feature/spec-based-2
Mapped tests: tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log; reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json; reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md; reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json; reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_history.md
Do Now: CLI-FLAGS-003 Phase L2b — After updating TRACE_PY instrumentation, run "'`KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v`
If Blocked: Capture current stdout/stderr to reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_failed.log, snapshot git diff, and document the blocker in attempt_history.md before stopping.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md Phase L2b now demands real scaling factors and a regression test guarding TRACE_PY output.
- docs/fix_plan.md#cli-flags-003 lists instrumentation-first, then harness rerun as the active next actions.
- docs/development/testing_strategy.md#14-pytorch-device--dtype-discipline mandates device/dtype neutrality while adding instrumentation.
- docs/debugging/debugging.md stresses parity traces must compare the exact computed values (no placeholders).
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log is the reference trace the new PyTorch log must mirror.
How-To Map:
- Edit src/nanobrag_torch/simulator.py TRACE_PY branch to thread the live polarization tensor, capture_fraction output, and full steps product through debug_config without breaking gradients.
- Log actual scaling values with `:.15g` precision; keep units consistent with the C trace (meters, steradians, photons/m²).
- Add tests/test_trace_pixel.py with a minimal supervisor-style config (oversample=1, no noise) that asserts TRACE_PY reports the same omega, polarization, capture_fraction, steps, r_e², fluence, I_before_scaling, and final intensity that the simulator computes for the traced pixel.
- Extend reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py to call Simulator with `trace_pixel=[685,1039]`, capture stdout to trace_py_scaling.log, and dump config/env snapshots.
- Diff TRACE_C and TRACE_PY using a small helper (e.g., compare_scaling_traces.py) that emits compare_scaling_traces.json with per-factor deltas ≤1e-6 before moving to L2c.
Pitfalls To Avoid:
- No `.item()` on tensors needed for gradients; use tensor ops and only format scalars inside the guarded trace block.
- Do not assume CPU; honour current device/dtype when wiring tensors into debug output.
- Keep Protected Assets untouched (docs/index.md, loop.sh, supervisor.sh, input.md).
- Avoid reintroducing placeholder constants (polar=1, capture_fraction=1); ensure values come from core pipeline.
- Do not bypass nb-compare or Phase L plan sequencing—stop after producing verified traces.
- Refrain from adding ad-hoc scripts outside reports/…; reuse trace_harness.py and new compare helper under scripts/validation if promoted later.
Pointers:
- docs/architecture/pytorch_design.md:320 (scaling order expectations)
- docs/development/c_to_pytorch_config_map.md:63 (detector pivot + scaling parity parameters)
- docs/debugging/debugging.md:24 (parallel trace compare rule)
- plans/active/cli-noise-pix0/plan.md:95 (Phase L2b table)
- docs/fix_plan.md:460 (Next Actions refresh for CLI-FLAGS-003)
Next Up: Phase L2c — Diff the refreshed PyTorch trace against c_trace_scaling.log and log the first divergent factor.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:36:43] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' > input.md
Summary: Patch TRACE_PY instrumentation so the supervisor command scaling factors match C and unblock Phase L2.
Mode: Parity
Focus: CLI-FLAGS-003 Handle -nonoise and -pix0_vector_mm — Phase L2 scaling audit
Branch: feature/spec-based-2
Mapped tests: tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log; reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json; reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md; reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json; reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_history.md
Do Now: CLI-FLAGS-003 Phase L2b — After updating TRACE_PY instrumentation, run "'`KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v`
If Blocked: Capture current stdout/stderr to reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_failed.log, snapshot git diff, and document the blocker in attempt_history.md before stopping.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md Phase L2b now demands real scaling factors and a regression test guarding TRACE_PY output.
- docs/fix_plan.md#cli-flags-003 lists instrumentation-first, then harness rerun as the active next actions.
- docs/development/testing_strategy.md#14-pytorch-device--dtype-discipline mandates device/dtype neutrality while adding instrumentation.
- docs/debugging/debugging.md stresses parity traces must compare the exact computed values (no placeholders).
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log is the reference trace the new PyTorch log must mirror.
How-To Map:
- Edit src/nanobrag_torch/simulator.py TRACE_PY branch to thread the live polarization tensor, capture_fraction output, and full steps product through debug_config without breaking gradients.
- Log actual scaling values with `:.15g` precision; keep units consistent with the C trace (meters, steradians, photons/m²).
- Add tests/test_trace_pixel.py with a minimal supervisor-style config (oversample=1, no noise) that asserts TRACE_PY reports the same omega, polarization, capture_fraction, steps, r_e², fluence, I_before_scaling, and final intensity that the simulator computes for the traced pixel.
- Extend reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py to call Simulator with `trace_pixel=[685,1039]`, capture stdout to trace_py_scaling.log, and dump config/env snapshots.
- Diff TRACE_C and TRACE_PY using a small helper (e.g., compare_scaling_traces.py) that emits compare_scaling_traces.json with per-factor deltas ≤1e-6 before moving to L2c.
Pitfalls To Avoid:
- No `.item()` on tensors needed for gradients; use tensor ops and only format scalars inside the guarded trace block.
- Do not assume CPU; honour current device/dtype when wiring tensors into debug output.
- Keep Protected Assets untouched (docs/index.md, loop.sh, supervisor.sh, input.md).
- Avoid reintroducing placeholder constants (polar=1, capture_fraction=1); ensure values come from core pipeline.
- Do not bypass nb-compare or Phase L plan sequencing—stop after producing verified traces.
- Refrain from adding ad-hoc scripts outside reports/…; reuse trace_harness.py and new compare helper under scripts/validation if promoted later.
Pointers:
- docs/architecture/pytorch_design.md:320 (scaling order expectations)
- docs/development/c_to_pytorch_config_map.md:63 (detector pivot + scaling parity parameters)
- docs/debugging/debugging.md:24 (parallel trace compare rule)
- plans/active/cli-noise-pix0/plan.md:95 (Phase L2b table)
- docs/fix_plan.md:460 (Next Actions refresh for CLI-FLAGS-003)
Next Up: Phase L2c — Diff the refreshed PyTorch trace against c_trace_scaling.log and log the first divergent factor.
EOF
' succeeded in 986ms:

[2025-10-07T02:36:43] tokens used: 168,212
[2025-10-07T02:36:43] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:36:50] thinking

**Planning detailed file expansion**

I’m figuring out how to expand the existing 30-line file to around 100-200 lines by breaking sections into detailed bullet points and adding more specifics to each part without redundancy, aiming for clearer guidance and better structure throughout.
[2025-10-07T02:37:10] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' > input.md
Summary: Patch TRACE_PY instrumentation so the supervisor command scaling factors match C and unblock Phase L2.
Mode: Parity
Focus: CLI-FLAGS-003 Handle -nonoise and -pix0_vector_mm — Phase L2 scaling audit
Branch: feature/spec-based-2
Mapped tests:
- tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
Artifacts:
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
- reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_history.md
- tests/test_trace_pixel.py (new regression test guarding TRACE_PY)
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stdout.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stderr.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json
- scripts/validation/compare_scaling_traces.py (if promoted from harness helper)
Do Now: CLI-FLAGS-003 Phase L2b — After updating TRACE_PY instrumentation, run "'`KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v`
If Blocked: Capture stdout/stderr in reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_failed.log, dump `git diff > reports/.../diff_on_blocker.patch`, and log the blocking stack trace in attempt_history.md before stopping.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md:95 spells out Phase L2b requirements; instrumentation and regression test are the gating steps before harness reruns.
- docs/fix_plan.md:460 lists the refreshed next actions (instrumentation → regression test → harness capture) for CLI-FLAGS-003; align commits with that ledger.
- docs/development/testing_strategy.md:68 emphasises trace-driven parity before implementation; accurate TRACE_PY output is mandatory evidence.
- docs/debugging/debugging.md:24 reinforces the TRACE_C vs TRACE_PY comparison rule; placeholders like polar=1 break this workflow.
- docs/architecture/pytorch_design.md:320 documents normalization order (steps → r_e² → fluence → capture_fraction → polarization → omega) that TRACE_PY must reflect.
- docs/development/c_to_pytorch_config_map.md:63 reminds that detector pivots and scaling parameters must match C exactly when logging.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log contains the authoritative field list and numeric targets for pixel (685,1039).
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md records the C instrumentation context; reuse its ordering and units.
How-To Map:
- Step 1 — Inspect src/nanobrag_torch/simulator.py lines 1100-1360 to understand current TRACE_PY placeholders (polar=1, capture_fraction=1, steps=phi_steps) and note gaps in notes.md.
- Step 2 — Thread live tensors into the debug path: expose `self._cached_polarization` (or recompute via polarization_factor), pass the actual capture_fraction returned by `_apply_detector_absorption`, and store the scalar `steps = source_norm * phi_steps * mosaic_domains * oversample ** 2` for reuse.
- Step 3 — Ensure the debug tensors stay on the caller device/dtype; use `.to(device=self.device, dtype=self.dtype)` only when preparing values for logging and keep operations inside the trace guard to avoid perf overhead.
- Step 4 — Format TRACE_PY lines with `:.15g` precision and identical keys to C (I_before_scaling, omega_pixel_sr, capture_fraction, polar, steps, r_e_sqr, fluence, I_pixel_final); write helper functions if needed to avoid `.item()` on tensors outside the trace guard.
- Step 5 — Create tests/test_trace_pixel.py with a deterministic micro configuration (e.g., Na=Nb=Nc=1, oversample=1, nopolar=False) that calls Simulator with `trace_pixel=[0,0]`, captures stdout, and asserts each TRACE_PY value matches the internally computed tensors within ≤1e-8 relative error.
- Step 6 — Run `KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v` and update attempt_history.md with pass/fail plus timing; rerun on CUDA if available (guard with `torch.cuda.is_available()` inside the test).
- Step 7 — Update reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py to accept `--trace-pixel`, configure Simulator with the supervisor command parameters, and capture stdout/stderr into trace_py_scaling.log / trace_py_stderr.txt while writing config_snapshot.json.
- Step 8 — Diff TRACE_C vs TRACE_PY using a small helper (add to reports/.../compare_scaling_traces.py or promote to scripts/validation/) that produces compare_scaling_traces.json listing per-factor ratios; require |Δ| ≤ 1e-6 before closing Phase L2b.
- Step 9 — Summarize findings in notes.md (command history, tensor checks, device/dtype runs) and log Attempt #56 (or next number) in docs/fix_plan.md with metrics and artifact links.
Pitfalls To Avoid:
- Do not leave placeholder constants (e.g., `polar_val = 1.0`)—guard assignments must use real tensors.
- Avoid `.cpu()`/.numpy()` conversions in the main path; only convert inside the trace guard when printing.
- Keep trace instrumentation behind the existing `if self.trace_pixel` checks to prevent perf regressions on normal runs.
- Respect Protected Assets; do not modify docs/index.md, loop.sh, supervisor.sh, or existing artifacts outside reports/…
- Do not bypass plan sequencing by attempting Phase L2c or L3 before the regression test and trace artifacts are complete.
- Avoid adding new top-level scripts outside scripts/ or reports/; trace helpers should live under reports/… unless they become reusable validations.
- Remember to set `os.environ['"'KMP_DUPLICATE_LIB_OK']='TRUE'"'` in new tests before importing torch to prevent MKL crashes.
- Ensure new tests clean up temp files (use TemporaryDirectory) so repeated runs stay hermetic.
Pointers:
- docs/architecture/pytorch_design.md:320 — normalization order reference for TRACE_PY field sequence.
- docs/development/c_to_pytorch_config_map.md:63 — detector/beam scaling parameters to mirror in trace output.
- docs/debugging/debugging.md:24 — parallel trace SOP that drives this task.
- docs/development/testing_strategy.md:68 — tiered validation checklist emphasising trace evidence before fixes.
- plans/active/cli-noise-pix0/plan.md:95 — Phase L2b table with instrumentation + regression test requirements.
- docs/fix_plan.md:460 — current next actions for CLI-FLAGS-003.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C trace reference values.
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md — available key mapping for trace fields.
Next Up:
- Phase L2c — Once trace parity is confirmed, build compare_scaling_traces.md summarizing the earliest divergent factor (expect none) and prep for normalization fixes if any delta remains.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:37:11] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' > input.md
Summary: Patch TRACE_PY instrumentation so the supervisor command scaling factors match C and unblock Phase L2.
Mode: Parity
Focus: CLI-FLAGS-003 Handle -nonoise and -pix0_vector_mm — Phase L2 scaling audit
Branch: feature/spec-based-2
Mapped tests:
- tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
Artifacts:
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
- reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_history.md
- tests/test_trace_pixel.py (new regression test guarding TRACE_PY)
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stdout.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stderr.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json
- scripts/validation/compare_scaling_traces.py (if promoted from harness helper)
Do Now: CLI-FLAGS-003 Phase L2b — After updating TRACE_PY instrumentation, run "'`KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v`
If Blocked: Capture stdout/stderr in reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_failed.log, dump `git diff > reports/.../diff_on_blocker.patch`, and log the blocking stack trace in attempt_history.md before stopping.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md:95 spells out Phase L2b requirements; instrumentation and regression test are the gating steps before harness reruns.
- docs/fix_plan.md:460 lists the refreshed next actions (instrumentation → regression test → harness capture) for CLI-FLAGS-003; align commits with that ledger.
- docs/development/testing_strategy.md:68 emphasises trace-driven parity before implementation; accurate TRACE_PY output is mandatory evidence.
- docs/debugging/debugging.md:24 reinforces the TRACE_C vs TRACE_PY comparison rule; placeholders like polar=1 break this workflow.
- docs/architecture/pytorch_design.md:320 documents normalization order (steps → r_e² → fluence → capture_fraction → polarization → omega) that TRACE_PY must reflect.
- docs/development/c_to_pytorch_config_map.md:63 reminds that detector pivots and scaling parameters must match C exactly when logging.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log contains the authoritative field list and numeric targets for pixel (685,1039).
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md records the C instrumentation context; reuse its ordering and units.
How-To Map:
- Step 1 — Inspect src/nanobrag_torch/simulator.py lines 1100-1360 to understand current TRACE_PY placeholders (polar=1, capture_fraction=1, steps=phi_steps) and note gaps in notes.md.
- Step 2 — Thread live tensors into the debug path: expose `self._cached_polarization` (or recompute via polarization_factor), pass the actual capture_fraction returned by `_apply_detector_absorption`, and store the scalar `steps = source_norm * phi_steps * mosaic_domains * oversample ** 2` for reuse.
- Step 3 — Ensure the debug tensors stay on the caller device/dtype; use `.to(device=self.device, dtype=self.dtype)` only when preparing values for logging and keep operations inside the trace guard to avoid perf overhead.
- Step 4 — Format TRACE_PY lines with `:.15g` precision and identical keys to C (I_before_scaling, omega_pixel_sr, capture_fraction, polar, steps, r_e_sqr, fluence, I_pixel_final); write helper functions if needed to avoid `.item()` on tensors outside the trace guard.
- Step 5 — Create tests/test_trace_pixel.py with a deterministic micro configuration (e.g., Na=Nb=Nc=1, oversample=1, nopolar=False) that calls Simulator with `trace_pixel=[0,0]`, captures stdout, and asserts each TRACE_PY value matches the internally computed tensors within ≤1e-8 relative error.
- Step 6 — Run `KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v` and update attempt_history.md with pass/fail plus timing; rerun on CUDA if available (guard with `torch.cuda.is_available()` inside the test).
- Step 7 — Update reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py to accept `--trace-pixel`, configure Simulator with the supervisor command parameters, and capture stdout/stderr into trace_py_scaling.log / trace_py_stderr.txt while writing config_snapshot.json.
- Step 8 — Diff TRACE_C vs TRACE_PY using a small helper (add to reports/.../compare_scaling_traces.py or promote to scripts/validation/) that produces compare_scaling_traces.json listing per-factor ratios; require |Δ| ≤ 1e-6 before closing Phase L2b.
- Step 9 — Summarize findings in notes.md (command history, tensor checks, device/dtype runs) and log Attempt #56 (or next number) in docs/fix_plan.md with metrics and artifact links.
Pitfalls To Avoid:
- Do not leave placeholder constants (e.g., `polar_val = 1.0`)—guard assignments must use real tensors.
- Avoid `.cpu()`/.numpy()` conversions in the main path; only convert inside the trace guard when printing.
- Keep trace instrumentation behind the existing `if self.trace_pixel` checks to prevent perf regressions on normal runs.
- Respect Protected Assets; do not modify docs/index.md, loop.sh, supervisor.sh, or existing artifacts outside reports/…
- Do not bypass plan sequencing by attempting Phase L2c or L3 before the regression test and trace artifacts are complete.
- Avoid adding new top-level scripts outside scripts/ or reports/; trace helpers should live under reports/… unless they become reusable validations.
- Remember to set `os.environ['"'KMP_DUPLICATE_LIB_OK']='TRUE'"'` in new tests before importing torch to prevent MKL crashes.
- Ensure new tests clean up temp files (use TemporaryDirectory) so repeated runs stay hermetic.
Pointers:
- docs/architecture/pytorch_design.md:320 — normalization order reference for TRACE_PY field sequence.
- docs/development/c_to_pytorch_config_map.md:63 — detector/beam scaling parameters to mirror in trace output.
- docs/debugging/debugging.md:24 — parallel trace SOP that drives this task.
- docs/development/testing_strategy.md:68 — tiered validation checklist emphasising trace evidence before fixes.
- plans/active/cli-noise-pix0/plan.md:95 — Phase L2b table with instrumentation + regression test requirements.
- docs/fix_plan.md:460 — current next actions for CLI-FLAGS-003.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C trace reference values.
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md — available key mapping for trace fields.
Next Up:
- Phase L2c — Once trace parity is confirmed, build compare_scaling_traces.md summarizing the earliest divergent factor (expect none) and prep for normalization fixes if any delta remains.
EOF
' succeeded in 863ms:

[2025-10-07T02:37:11] tokens used: 172,231
[2025-10-07T02:37:11] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:37:14] thinking

**Checking line count**
[2025-10-07T02:37:14] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && wc -l input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:37:15] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && wc -l input.md' succeeded in 702ms:
      58 input.md
[2025-10-07T02:37:15] tokens used: 174,190
[2025-10-07T02:37:15] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:37:25] thinking

**Planning detailed expansion**
[2025-10-07T02:37:49] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' > input.md
Summary: Patch TRACE_PY instrumentation so the supervisor command scaling factors match C and unblock Phase L2.
Mode: Parity
Focus: CLI-FLAGS-003 Handle -nonoise and -pix0_vector_mm — Phase L2 scaling audit
Branch: feature/spec-based-2
Mapped tests:
- tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
Artifacts:
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
- reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_history.md
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stdout.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stderr.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/diff_summary.md
- tests/test_trace_pixel.py (new regression test guarding TRACE_PY values)
- scripts/validation/compare_scaling_traces.py (promote helper if reused)
- reports/2025-10-cli-flags/phase_l/scaling_audit/capture_notes.md
Do Now: CLI-FLAGS-003 Phase L2b — After updating TRACE_PY instrumentation, run "'`KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v`
If Blocked: Capture stdout/stderr in reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_failed.log, dump `git diff > reports/.../diff_on_blocker.patch`, append blocker summary to attempt_history.md, and halt for supervisor guidance.
Context:
- Attempt #55 delivered the C scaling trace; the PyTorch side still prints placeholder values.
- Attempt #56 (evidence-only) showed `TRACE_PY: polar 1` and `TRACE_PY: capture_fraction 1` regardless of physics.
- K3g regressions are resolved; Phase L is the remaining blocker for long-term Goal #1.
- Vectorization work (Goal #2) is paused until CLI parity is unlocked.
- Plan Phase L2a is [D]; Phase L2b and L2c remain [ ].
- Protected Assets policy remains in effect; modify only scoped files.
Success Criteria:
- TRACE_PY for pixel (slow=685, fast=1039) matches C trace within ≤1e-6 relative error for all logged factors.
- New regression test in tests/test_trace_pixel.py fails if TRACE_PY reverts to placeholders.
- Updated harness produces trace_py_scaling.log, env snapshot, stdout/stderr, and comparison JSON under reports/2025-10-cli-flags/phase_l/scaling_audit/.
- docs/fix_plan.md Attempt log updated with metrics, artifact links, and Phase L2b status change to [D].
- No new regressions in existing CLI scaling tests.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md:95 sets explicit steps for Phase L2b (instrumentation, regression test, harness rerun).
- docs/fix_plan.md:460 captures the refreshed next actions; staying aligned prevents plan drift.
- docs/development/testing_strategy.md:68 mandates trace parity evidence before implementation fixes.
- docs/debugging/debugging.md:24 details the parallel trace comparison workflow driving this task.
- docs/architecture/pytorch_design.md:320 documents normalization order to mirror in TRACE_PY output.
- docs/development/c_to_pytorch_config_map.md:63 highlights detector/beam scaling interplay relevant to logged fields.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log is the numeric target for parity.
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md lists required TRACE keys and units.
How-To Map:
- Step 1 — Review simulator trace path (src/nanobrag_torch/simulator.py:1100-1360) and note each placeholder value in notes.md.
- Step 2 — Identify where the real tensors live: polarization tensors from `_apply_polarization`, capture fractions from `_apply_detector_absorption`, and total steps from `OversampleController` / run context.
- Step 3 — Extend Simulator to stash these tensors in `self.debug_cache` (or similar) when `trace_pixel` is requested, ensuring no gradient-detaching operations outside the trace guard.
- Step 4 — In `_apply_debug_output`, read the cached tensors, compute scalar values with `.item()` only at print time, and format `TRACE_PY: key value` with `:.15g` precision.
- Step 5 — Include additional context lines (e.g., `TRACE_PY: sources`, `TRACE_PY: mosaic_domains`) if needed to disambiguate the steps product; keep naming consistent with C logs.
- Step 6 — Construct tests/test_trace_pixel.py with deterministic config (Na=36, Nb=47, Nc=29, oversample=1, nopolar=False) that uses `capsys` to capture TRACE_PY output and asserts equality against simulator internals.
- Step 7 — Parameterise the new test over device/dtype (cpu float32, cuda float32 when available) to enforce device neutrality; guard CUDA branch with `torch.cuda.is_available()`.
- Step 8 — Run `KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v` and record results in attempt_history.md; rerun with `CUDA_VISIBLE_DEVICES=0` when GPU present.
- Step 9 — Update trace_harness.py to pass `trace_pixel=[685,1039]`, reuse CLI parsing for the supervisor parameters, and stream stdout/stderr into the dedicated files under reports/…
- Step 10 — Execute `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --out trace_py_scaling.log --config supervisor --device cpu` and capture run metadata.
- Step 11 — Compare TRACE_C and TRACE_PY via a helper script that outputs compare_scaling_traces.json summarising absolute and relative deltas for each field.
- Step 12 — Summarise findings in notes.md (commands, devices, tolerances) and update docs/fix_plan.md Attempt log with the new evidence and outcomes.
Pitfalls To Avoid:
- No placeholder constants (polar=1, capture_fraction=1) may remain once instrumentation is complete.
- Do not detach tensors prematurely; keep gradients intact by printing only inside the trace guard.
- Avoid `.cpu()` conversions when storing debug state; handle tensors on their native device.
- Keep trace logic behind `if self.trace_pixel` to prevent runtime penalties in production runs.
- Do not bypass plan sequencing by skipping the regression test or comparison JSON.
- Respect Protected Assets (docs/index.md, loop.sh, supervisor.sh, input.md) and only add files inside reports/… or scripts/validation/…
- Ensure new test cleans up temporary files (use TemporaryDirectory) to keep loops hermetic.
- Record environment snapshots (torch, python, git SHA) in trace_py_env.json for reproducibility.
- Avoid running full pytest suite; stick to targeted selectors plus the new regression test.
- Confirm `KMP_DUPLICATE_LIB_OK=TRUE` is set in any harness/test before importing torch.
Pointers:
- docs/architecture/pytorch_design.md:320 — normalization field order reference.
- docs/development/c_to_pytorch_config_map.md:63 — scaling parameter parity checklist.
- docs/debugging/debugging.md:24 — mandatory parallel trace workflow.
- docs/development/testing_strategy.md:68 — evidence-first validation rule.
- plans/active/cli-noise-pix0/plan.md:95 — Phase L2b guidance.
- docs/fix_plan.md:460 — CLI-FLAGS-003 next actions refresh.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C trace baseline.
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md — field descriptions.
- reports/2025-10-cli-flags/phase_i/supervisor_command/summary.json — original parity failure metrics for context.
- scripts/debug_pixel_trace.py — reference for existing trace formatting conventions.
Next Up:
- Phase L2c — Diff refreshed TRACE_PY against C trace, capture compare_scaling_traces.md, and identify any remaining divergent factor (expect none once instrumentation is fixed).
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:37:50] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' > input.md
Summary: Patch TRACE_PY instrumentation so the supervisor command scaling factors match C and unblock Phase L2.
Mode: Parity
Focus: CLI-FLAGS-003 Handle -nonoise and -pix0_vector_mm — Phase L2 scaling audit
Branch: feature/spec-based-2
Mapped tests:
- tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
Artifacts:
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
- reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_history.md
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stdout.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_stderr.txt
- reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json
- reports/2025-10-cli-flags/phase_l/scaling_audit/diff_summary.md
- tests/test_trace_pixel.py (new regression test guarding TRACE_PY values)
- scripts/validation/compare_scaling_traces.py (promote helper if reused)
- reports/2025-10-cli-flags/phase_l/scaling_audit/capture_notes.md
Do Now: CLI-FLAGS-003 Phase L2b — After updating TRACE_PY instrumentation, run "'`KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v`
If Blocked: Capture stdout/stderr in reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_failed.log, dump `git diff > reports/.../diff_on_blocker.patch`, append blocker summary to attempt_history.md, and halt for supervisor guidance.
Context:
- Attempt #55 delivered the C scaling trace; the PyTorch side still prints placeholder values.
- Attempt #56 (evidence-only) showed `TRACE_PY: polar 1` and `TRACE_PY: capture_fraction 1` regardless of physics.
- K3g regressions are resolved; Phase L is the remaining blocker for long-term Goal #1.
- Vectorization work (Goal #2) is paused until CLI parity is unlocked.
- Plan Phase L2a is [D]; Phase L2b and L2c remain [ ].
- Protected Assets policy remains in effect; modify only scoped files.
Success Criteria:
- TRACE_PY for pixel (slow=685, fast=1039) matches C trace within ≤1e-6 relative error for all logged factors.
- New regression test in tests/test_trace_pixel.py fails if TRACE_PY reverts to placeholders.
- Updated harness produces trace_py_scaling.log, env snapshot, stdout/stderr, and comparison JSON under reports/2025-10-cli-flags/phase_l/scaling_audit/.
- docs/fix_plan.md Attempt log updated with metrics, artifact links, and Phase L2b status change to [D].
- No new regressions in existing CLI scaling tests.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md:95 sets explicit steps for Phase L2b (instrumentation, regression test, harness rerun).
- docs/fix_plan.md:460 captures the refreshed next actions; staying aligned prevents plan drift.
- docs/development/testing_strategy.md:68 mandates trace parity evidence before implementation fixes.
- docs/debugging/debugging.md:24 details the parallel trace comparison workflow driving this task.
- docs/architecture/pytorch_design.md:320 documents normalization order to mirror in TRACE_PY output.
- docs/development/c_to_pytorch_config_map.md:63 highlights detector/beam scaling interplay relevant to logged fields.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log is the numeric target for parity.
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md lists required TRACE keys and units.
How-To Map:
- Step 1 — Review simulator trace path (src/nanobrag_torch/simulator.py:1100-1360) and note each placeholder value in notes.md.
- Step 2 — Identify where the real tensors live: polarization tensors from `_apply_polarization`, capture fractions from `_apply_detector_absorption`, and total steps from `OversampleController` / run context.
- Step 3 — Extend Simulator to stash these tensors in `self.debug_cache` (or similar) when `trace_pixel` is requested, ensuring no gradient-detaching operations outside the trace guard.
- Step 4 — In `_apply_debug_output`, read the cached tensors, compute scalar values with `.item()` only at print time, and format `TRACE_PY: key value` with `:.15g` precision.
- Step 5 — Include additional context lines (e.g., `TRACE_PY: sources`, `TRACE_PY: mosaic_domains`) if needed to disambiguate the steps product; keep naming consistent with C logs.
- Step 6 — Construct tests/test_trace_pixel.py with deterministic config (Na=36, Nb=47, Nc=29, oversample=1, nopolar=False) that uses `capsys` to capture TRACE_PY output and asserts equality against simulator internals.
- Step 7 — Parameterise the new test over device/dtype (cpu float32, cuda float32 when available) to enforce device neutrality; guard CUDA branch with `torch.cuda.is_available()`.
- Step 8 — Run `KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v` and record results in attempt_history.md; rerun with `CUDA_VISIBLE_DEVICES=0` when GPU present.
- Step 9 — Update trace_harness.py to pass `trace_pixel=[685,1039]`, reuse CLI parsing for the supervisor parameters, and stream stdout/stderr into the dedicated files under reports/…
- Step 10 — Execute `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --out trace_py_scaling.log --config supervisor --device cpu` and capture run metadata.
- Step 11 — Compare TRACE_C and TRACE_PY via a helper script that outputs compare_scaling_traces.json summarising absolute and relative deltas for each field.
- Step 12 — Summarise findings in notes.md (commands, devices, tolerances) and update docs/fix_plan.md Attempt log with the new evidence and outcomes.
Pitfalls To Avoid:
- No placeholder constants (polar=1, capture_fraction=1) may remain once instrumentation is complete.
- Do not detach tensors prematurely; keep gradients intact by printing only inside the trace guard.
- Avoid `.cpu()` conversions when storing debug state; handle tensors on their native device.
- Keep trace logic behind `if self.trace_pixel` to prevent runtime penalties in production runs.
- Do not bypass plan sequencing by skipping the regression test or comparison JSON.
- Respect Protected Assets (docs/index.md, loop.sh, supervisor.sh, input.md) and only add files inside reports/… or scripts/validation/…
- Ensure new test cleans up temporary files (use TemporaryDirectory) to keep loops hermetic.
- Record environment snapshots (torch, python, git SHA) in trace_py_env.json for reproducibility.
- Avoid running full pytest suite; stick to targeted selectors plus the new regression test.
- Confirm `KMP_DUPLICATE_LIB_OK=TRUE` is set in any harness/test before importing torch.
Pointers:
- docs/architecture/pytorch_design.md:320 — normalization field order reference.
- docs/development/c_to_pytorch_config_map.md:63 — scaling parameter parity checklist.
- docs/debugging/debugging.md:24 — mandatory parallel trace workflow.
- docs/development/testing_strategy.md:68 — evidence-first validation rule.
- plans/active/cli-noise-pix0/plan.md:95 — Phase L2b guidance.
- docs/fix_plan.md:460 — CLI-FLAGS-003 next actions refresh.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C trace baseline.
- reports/2025-10-cli-flags/phase_l/scaling_audit/instrumentation_notes.md — field descriptions.
- reports/2025-10-cli-flags/phase_i/supervisor_command/summary.json — original parity failure metrics for context.
- scripts/debug_pixel_trace.py — reference for existing trace formatting conventions.
Next Up:
- Phase L2c — Diff refreshed TRACE_PY against C trace, capture compare_scaling_traces.md, and identify any remaining divergent factor (expect none once instrumentation is fixed).
EOF
' succeeded in 943ms:

[2025-10-07T02:37:50] tokens used: 190,215
[2025-10-07T02:37:50] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:37:53] thinking

**Checking line count**
[2025-10-07T02:37:53] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && wc -l input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:37:54] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && wc -l input.md' succeeded in 682ms:
      80 input.md
[2025-10-07T02:37:54] tokens used: 192,898
[2025-10-07T02:37:54] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:37:59] thinking

**Expanding documentation with more sections**
[2025-10-07T02:38:07] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' >> input.md
Environment Prep:
- Verify editable install with "'`pip install -e .`; record outcome in notes.md even if already satisfied.
- Export `NB_RUN_PARALLEL=1` only when running C parity helpers; leave unset for unit tests to avoid accidental skips.
- Set `NB_C_BIN=./golden_suite_generator/nanoBragg` before invoking comparison helpers so the harness targets the instrumented binary.
- Confirm CUDA availability via `python - <<'"'PY'"'` snippet and log result (available/unavailable) in notes.md prior to running GPU variant.
- Capture `python -m nanobrag_torch --version` (or equivalent) to note CLI entry point wiring.
Command Checklist:
- `KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v` (primary regression test)
- `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --out trace_py_scaling.log --config supervisor --device cpu`
- `python scripts/validation/compare_scaling_traces.py --c-log reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log --py-log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --out reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json`
- `git status -sb` before and after code changes; paste summaries into notes.md for traceability.
Logging Requirements:
- Append attempt metadata to attempt_history.md (timestamp, device, dtype, commands, pass/fail, metrics).
- Store stdout/stderr separately to keep trace_py_scaling.log clean for diffing.
- Include env snapshot (python version, torch version, git SHA) in trace_py_env.json.
- Document any deviations (e.g., missing CUDA) in notes.md so future loops understand context.
- If compare_scaling_traces.py reports deltas >1e-6, list each offending key with numeric diff in diff_summary.md.
Quality Gates:
- Regression test must pass on CPU float32; mark CUDA variant optional but encouraged when hardware is present.
- compare_scaling_traces.json should report `status: pass`; if fail, halt and escalate rather than tweaking tolerances.
- New test should fail on purpose if TRACE_PY reverts to placeholder constants (validate by temporarily forcing polar to 1 during development).
- Input.md instructions must be followed sequentially; document any intentional deviations in notes.md.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:08] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' >> input.md
Environment Prep:
- Verify editable install with "'`pip install -e .`; record outcome in notes.md even if already satisfied.
- Export `NB_RUN_PARALLEL=1` only when running C parity helpers; leave unset for unit tests to avoid accidental skips.
- Set `NB_C_BIN=./golden_suite_generator/nanoBragg` before invoking comparison helpers so the harness targets the instrumented binary.
- Confirm CUDA availability via `python - <<'"'PY'"'` snippet and log result (available/unavailable) in notes.md prior to running GPU variant.
- Capture `python -m nanobrag_torch --version` (or equivalent) to note CLI entry point wiring.
Command Checklist:
- `KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -v` (primary regression test)
- `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --out trace_py_scaling.log --config supervisor --device cpu`
- `python scripts/validation/compare_scaling_traces.py --c-log reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log --py-log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --out reports/2025-10-cli-flags/phase_l/scaling_audit/compare_scaling_traces.json`
- `git status -sb` before and after code changes; paste summaries into notes.md for traceability.
Logging Requirements:
- Append attempt metadata to attempt_history.md (timestamp, device, dtype, commands, pass/fail, metrics).
- Store stdout/stderr separately to keep trace_py_scaling.log clean for diffing.
- Include env snapshot (python version, torch version, git SHA) in trace_py_env.json.
- Document any deviations (e.g., missing CUDA) in notes.md so future loops understand context.
- If compare_scaling_traces.py reports deltas >1e-6, list each offending key with numeric diff in diff_summary.md.
Quality Gates:
- Regression test must pass on CPU float32; mark CUDA variant optional but encouraged when hardware is present.
- compare_scaling_traces.json should report `status: pass`; if fail, halt and escalate rather than tweaking tolerances.
- New test should fail on purpose if TRACE_PY reverts to placeholder constants (validate by temporarily forcing polar to 1 during development).
- Input.md instructions must be followed sequentially; document any intentional deviations in notes.md.
EOF
' succeeded in 898ms:

[2025-10-07T02:38:08] tokens used: 193,879
[2025-10-07T02:38:08] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:11] thinking

**Checking line count**
[2025-10-07T02:38:12] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && wc -l input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:13] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && wc -l input.md' succeeded in 710ms:
     102 input.md
[2025-10-07T02:38:13] tokens used: 195,490
[2025-10-07T02:38:13] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:19] thinking

**Preparing galph_memory.md update**
[2025-10-07T02:38:23] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' >> galph_memory.md

## 2025-11-10 (galph loop — Phase L2b instrumentation)
- Synced cleanly; re-read docs/index.md, specs/spec-a.md shards pointer, arch.md, c_to_pytorch_config_map.md, debugging/debugging.md, docs/fix_plan.md, testing_strategy.md, and plans/active/* for continuity before analysis.
- Deep dive vs long-term Goal #1 found TRACE_PY still emits placeholder scaling data ("'`polar=1`, `capture_fraction=1`, `steps=phi_steps`) so Phase L2b cannot progress; identified need to thread real tensors from `_apply_polarization` and `_apply_detector_absorption` through the debug path.
- Coin flip → heads: Reviewed Ralph’s recent commits (`3e47b38`, `f3f116b`, `f466b39`); C trace capture solid, but evidence-only harness attempt stalled because instrumentation can’t expose values—no regressions spotted.
- Updated `plans/active/cli-noise-pix0/plan.md` Phase L2 context and table to require simulator instrumentation fixes, a new TRACE_PY regression test, and harness rerun sequencing; synced `docs/fix_plan.md` next actions accordingly.
- Authored new `input.md` directing Ralph to patch TRACE_PY, add `tests/test_trace_pixel.py`, rerun the scaling harness, and archive comparison artifacts under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
- Follow-up for Ralph: Execute Phase L2b per updated plan—implement real scaling trace output, add the regression test, regenerate PyTorch trace + comparison JSON, and log Attempt update before moving to L2c.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:24] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && cat <<'EOF' >> galph_memory.md

## 2025-11-10 (galph loop — Phase L2b instrumentation)
- Synced cleanly; re-read docs/index.md, specs/spec-a.md shards pointer, arch.md, c_to_pytorch_config_map.md, debugging/debugging.md, docs/fix_plan.md, testing_strategy.md, and plans/active/* for continuity before analysis.
- Deep dive vs long-term Goal #1 found TRACE_PY still emits placeholder scaling data ("'`polar=1`, `capture_fraction=1`, `steps=phi_steps`) so Phase L2b cannot progress; identified need to thread real tensors from `_apply_polarization` and `_apply_detector_absorption` through the debug path.
- Coin flip → heads: Reviewed Ralph’s recent commits (`3e47b38`, `f3f116b`, `f466b39`); C trace capture solid, but evidence-only harness attempt stalled because instrumentation can’t expose values—no regressions spotted.
- Updated `plans/active/cli-noise-pix0/plan.md` Phase L2 context and table to require simulator instrumentation fixes, a new TRACE_PY regression test, and harness rerun sequencing; synced `docs/fix_plan.md` next actions accordingly.
- Authored new `input.md` directing Ralph to patch TRACE_PY, add `tests/test_trace_pixel.py`, rerun the scaling harness, and archive comparison artifacts under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
- Follow-up for Ralph: Execute Phase L2b per updated plan—implement real scaling trace output, add the regression test, regenerate PyTorch trace + comparison JSON, and log Attempt update before moving to L2c.
EOF
' succeeded in 828ms:

[2025-10-07T02:38:24] tokens used: 196,847
[2025-10-07T02:38:24] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:26] thinking

**Checking git status**
[2025-10-07T02:38:26] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git status -sb' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:27] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git status -sb' succeeded in 734ms:
## feature/spec-based-2...origin/feature/spec-based-2
 M docs/fix_plan.md
 M galph_memory.md
 M input.md
 M plans/active/cli-noise-pix0/plan.md
[2025-10-07T02:38:27] tokens used: 198,288
[2025-10-07T02:38:27] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:29] thinking

**Preparing to commit changes**
[2025-10-07T02:38:30] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git diff' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:31] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git diff' succeeded in 723ms:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f..5b01654 100644
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
[2025-10-07T02:38:31] tokens used: 198,532
[2025-10-07T02:38:31] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:35] thinking

**Noticing truncated diffs**
[2025-10-07T02:38:36] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md galph_memory.md input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:37] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md galph_memory.md input.md' succeeded in 712ms:

[2025-10-07T02:38:37] tokens used: 201,570
[2025-10-07T02:38:37] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:42] thinking

**Verifying staged changes**
[2025-10-07T02:38:43] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git status -sb' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:44] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git status -sb' succeeded in 727ms:
## feature/spec-based-2...origin/feature/spec-based-2
M  docs/fix_plan.md
M  galph_memory.md
M  input.md
M  plans/active/cli-noise-pix0/plan.md
[2025-10-07T02:38:44] tokens used: 202,129
[2025-10-07T02:38:44] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:46] thinking

**Committing instrumentation plan update**
[2025-10-07T02:38:47] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git commit -m "SUPERVISOR: Phase L2 instrumentation plan update - tests: not run"' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:48] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git commit -m "SUPERVISOR: Phase L2 instrumentation plan update - tests: not run"' succeeded in 735ms:
[feature/spec-based-2 5661ef5] SUPERVISOR: Phase L2 instrumentation plan update - tests: not run
 4 files changed, 110 insertions(+), 119 deletions(-)
[2025-10-07T02:38:48] tokens used: 202,336
[2025-10-07T02:38:48] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:49] thinking

**Preparing to push changes**
[2025-10-07T02:38:50] exec bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git push' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:38:52] bash -lc 'cd /Users/ollie/Documents/nanoBragg3 && git push' succeeded in 2.35s:
To github.com:hoidn/nanoBragg.git
   a178f1d..5661ef5  feature/spec-based-2 -> feature/spec-based-2
[2025-10-07T02:38:52] tokens used: 202,567
[2025-10-07T02:38:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:38:59] thinking

**Outlining final response structure**

I’m organizing the final message to include a concise bullet list covering plan updates, debugging hypotheses, commit reviews, and next steps, with precise file line references using line numbering tools like `nl -ba` to point to key modifications.
[2025-10-07T02:39:00] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '80,150p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:39:01] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '80,150p'" succeeded in 880ms:
    80	Goal: Pinpoint the first physics divergence between C and PyTorch for the supervisor command using the mandated parallel trace workflow.
    81	Prereqs: Phase D3 report published; C binary ready for instrumentation (`NB_C_BIN=./golden_suite_generator/nanoBragg`).
    82	Exit Criteria: Trace comparison identifies first divergent variable; findings logged in `reports/2025-10-cli-flags/phase_e/trace_comparison.md` and summarized in docs/fix_plan.md `[CLI-FLAGS-003]` Attempts.
    83	
    84	| ID | Task Description | State | How/Why & Guidance |
    85	| --- | --- | --- | --- |
    86	| E0 | Verify beam vector parity | [D] | ✅ 2025-10-16: `reports/2025-10-cli-flags/phase_e/beam_vector_check.txt` shows PyTorch Detector beam direction `[0., 0., 1.]` vs C trace override `[0.00051387949, 0, -0.99999986]`; confirms CLI wiring gap. |
    87	| E1 | Instrument C trace for peak pixel | [D] | ✅ 2025-10-16: `c_trace_beam.log` captured via instrumented binary (pixel 1039,685) with pix0, beam, scattering, h/k/l, F_latt entries. |
    88	| E2 | Generate matching PyTorch trace | [D] | ✅ 2025-10-16: `pytorch_trace_beam.log` generated with `trace_harness.py` (double precision). Harness currently overrides beam vector manually to mimic the supervisor command. |
    89	| E3 | Diff traces and identify first divergence | [D] | ✅ 2025-10-16: `trace_diff_beam.txt` + `trace_summary.md` pinpoint pix0 mismatch (1.14 mm Y error) and reveal lost crystal orientation + polarization delta. docs/fix_plan.md Attempts updated with evidence requirements for implementation phases. |
    90	
    91	### Phase F — Detector Implementation (beam + pix0 parity)
    92	Goal: Port CUSTOM detector wiring so CLI overrides reproduce C behavior without ad-hoc harness patches.
    93	Prereqs: Phase E traces complete; docs/fix_plan.md Attempt log captures pix0/beam/orientation findings.
    94	Exit Criteria: Detector trace matches C for pix0 and incident beam; CLI parity run no longer relies on manual overrides.
    95	
    96	| ID | Task Description | State | How/Why & Guidance |
    97	| --- | --- | --- | --- |
    98	| F1 | Thread `custom_beam_vector` through Detector | [D] | ✅ 2025-10-05 (ralph): Refactored `_calculate_pix0_vector()` to use `self.beam_vector` property instead of hardcoded beam vectors (lines 438-440, 519-521). Beam vector now exactly matches C trace. Artifacts in `reports/2025-10-cli-flags/phase_f/`. See docs/fix_plan.md Attempt #11. |
    99	| F2 | Port CUSTOM pix0 transform for overrides | [D] | ✅ 2025-10-05: CUSTOM pathway from nanoBragg.c:1739-1846 ported (pivot override, MOSFLM vs CUSTOM offsets, close_distance recompute). Validation report at `reports/2025-10-cli-flags/phase_f2/pix0_transform_refit.txt`. Follow-up: ensure downstream uses keep `close_distance` as a tensor (no `.item()` detaches) when further tuning detector gradients. |
[2025-10-07T02:39:01] tokens used: 203,842
[2025-10-07T02:39:01] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:39:04] thinking

**Identifying plan file line references**
[2025-10-07T02:39:05] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '210,260p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:39:05] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '210,260p'" succeeded in 696ms:
   210	Goal: Mirror nanoBragg.c’s MOSFLM branch so PyTorch recomputes real lattice vectors, volumes, and reciprocal duals from the supplied A* matrix before φ/mosaic rotations.
   211	Prereqs: K3f4 root-cause doc committed; guard tasks K3a/K3b still green.
   212	Exit Criteria: `Crystal.compute_cell_tensors` updates documented with in-code C reference, `tests/test_cli_scaling.py::test_f_latt_square_matches_c` passes at default tolerances, Δh/Δk/Δl in base trace <5e-4, and fix_plan Attempt #46 records parity metrics.
   213	
   214	| ID | Task Description | State | How/Why & Guidance |
   215	| --- | --- | --- | --- |
   216	| K3g1 | Recompute MOSFLM cell metrics inside `Crystal.compute_cell_tensors` | [D] | Commit 46ba36b landed MOSFLM branch with C reference, recomputing `V_star`, `V_cell`, `a/b/c` and updating reciprocal duals. See `tests/test_cli_scaling.py::TestMOSFLMCellVectors` and `reports/.../post_fix/cell_tensors_py.txt`. |
   217	| K3g2 | Add regression coverage for MOSFLM cell rebuild | [D] | Added `TestMOSFLMCellVectors::test_mosflm_cell_vectors` validating V_cell and |a|,|b|,|c| to ≤5e-4 against C reference; artifacts stored under `phase_k/base_lattice/post_fix/`. |
   218	| K3g3 | Re-run scaling & parity evidence | [D] | ✅ Attempt #49 (2025-10-06) — `env KMP_DUPLICATE_LIB_OK=TRUE NB_RUN_PARALLEL=1 pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` now passes after isolating HKL caches; `phase_k/f_latt_fix/post_fix/` holds refreshed traces and nb-compare artifacts. |
   219	
   220	### Phase L — Supervisor Command Normalization
   221	Goal: Isolate and resolve the 1.26×10⁵ intensity mismatch that persists when running the full supervisor command after Phase K fixes.
   222	Prereqs: Phase K3g artifacts refreshed (Attempt #49), nb-compare data in `phase_k/f_latt_fix/post_fix/`, instrumentation from Phase J available.
   223	Exit Criteria: Structure-factor grids, scaling chain, and final intensities match C within thresholds (|Δh|,|Δk|,|Δl| < 5e-4; sum_ratio 0.99–1.01; mean_peak_distance ≤ 1 px); nb-compare rerun archived under `phase_l/supervisor_command/`; docs/fix_plan Attempt log updated; plan ready for archival.
   224	
   225	| ID | Task Description | State | How/Why & Guidance |
   226	| --- | --- | --- | --- |
   227	| L1a | Run HKL ↔ Fdump parity script and capture metrics | [D] | Attempt #50 (2025-10-06) produced `summary_20251006175032.md` / `metrics_20251006175032.json`; this loop repeated the run with `Fdump_scaled_20251006175946.bin`, confirming max |ΔF|=5.22e2, 99,209 mismatched voxels, and storing `c_fdump_20251006175946.log`. Artefacts live under `reports/2025-10-cli-flags/phase_l/hkl_parity/`. |
   228	| L1b | Diagnose binary-layout mismatch between HKL and Fdump | [D] | ✅ Attempt #52 (2025-10-06) produced `scripts/validation/analyze_fdump_layout.py` plus `reports/2025-10-cli-flags/phase_l/hkl_parity/layout_analysis.md`. Confirmed nanoBragg writes `(h_range+2)×(k_range+2)×(l_range+2)` voxels via `for(h0=0; h0<=h_range; h0++)` loops (see `nanoBragg.c:2400-2486`), leaving 9,534 zero-padded entries. PyTorch reader must drop the padded plane per axis. |
   229	| L1c | Align PyTorch HKL cache reader/writer with confirmed layout | [D] | ✅ Attempt #53 (2025-10-17): Updated `src/nanobrag_torch/io/hkl.py` to honour `(range+1)` padding, added C-code docstring quotes per Rule #11, and introduced `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` (max |ΔF| = 0.0, mismatches = 0). |
[2025-10-07T02:39:05] tokens used: 205,651
[2025-10-07T02:39:05] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:39:10] thinking

**Noting file line references**
[2025-10-07T02:39:10] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba docs/fix_plan.md | sed -n '440,520p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:39:11] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba docs/fix_plan.md | sed -n '440,520p'" succeeded in 681ms:
   440	- Immediate Next Actions (2025-10-15):
   441	  * Update `parse_and_validate_args` (and any helper that constructs `CrystalConfig`) to thread the parsed dtype/device into HKL/Fdump loaders so CLI `-dtype` honours float64 from the start.
   442	  * Add a regression test (e.g., `tests/test_cli_dtype.py`) that invokes the parser with `-dtype float64` and asserts `config['hkl_data'][0].dtype == torch.float64`, covering both HKL and Fdump paths.
   443	  * Re-run AT-IO-003 and the CLI smoke (help + minimal render) to ensure no regressions, capturing logs under `reports/DTYPE-CLI/<date>/`.
   444	- Attempts History: None — new item.
   445	- Risks/Assumptions: Ensure `write_fdump` continues to emit float64 on disk (spec requirement) while the in-memory tensor honours caller dtype; watch for latent callers that relied on the old float64 default during plan DTYPE-DEFAULT-001 migration.
   446	- Exit Criteria: CLI runs with `-dtype float64` produce double-precision HKL/Fdump tensors end-to-end, regression test passes, and existing dtype-sensitive tests (AT-IO-003, CLI smoke, gradchecks) remain green.
   447	
   448	## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
   449	- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
   450	- Priority: High
   451	- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
   452	- Owner/Date: ralph/2025-10-05
   453	- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
   454	- Reproduction (C & PyTorch):
   455	  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   456	  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
   457	- First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
   458	- Next Actions (2025-11-10 refresh):
   459	  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
[2025-10-07T02:39:11] tokens used: 208,617
[2025-10-07T02:39:11] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:39:20] thinking

**Summarizing debugging hypotheses and plan updates**

I'll assemble a clear final message that lays out key observations and hypotheses with evidence references, plan updates by file and line, and feedback on Ralph's commits. The summary will include the new instructions and focus issues, wrapping everything in a structured bullet list for easy reading.
[2025-10-07T02:39:21] exec bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba src/nanobrag_torch/simulator.py | sed -n '1300,1350p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T02:39:22] bash -lc "cd /Users/ollie/Documents/nanoBragg3 && nl -ba src/nanobrag_torch/simulator.py | sed -n '1300,1350p'" succeeded in 794ms:
  1300	                    F_latt = F_latt_a * F_latt_b * F_latt_c
  1301	
  1302	                    print(f"TRACE_PY: F_latt_a {F_latt_a:.15g}")
  1303	                    print(f"TRACE_PY: F_latt_b {F_latt_b:.15g}")
  1304	                    print(f"TRACE_PY: F_latt_c {F_latt_c:.15g}")
  1305	                    print(f"TRACE_PY: F_latt {F_latt:.15g}")
  1306	
  1307	                    # Get structure factor
  1308	                    F_cell = self.crystal.get_structure_factor(
  1309	                        torch.tensor([[h0]]),
  1310	                        torch.tensor([[k0]]),
  1311	                        torch.tensor([[l0]])
  1312	                    ).item()
  1313	                    print(f"TRACE_PY: F_cell {F_cell:.15g}")
  1314	
  1315	                    # Calculate I_before_scaling (this is before r_e^2 * fluence / steps)
  1316	                    I_before_scaling = (F_cell * F_latt) ** 2
  1317	                    print(f"TRACE_PY: I_before_scaling {I_before_scaling:.15g}")
  1318	
  1319	                    # Physical constants and scaling factors
[2025-10-07T02:39:22] tokens used: 210,803
[2025-10-07T02:39:22] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T02:39:28] thinking

**Organizing final message highlights**

I'll focus on summarizing key points including the instrumentation gap with file references, a ranked hypotheses list with next steps that includes Phase L2 instrumentation focus, plan updates, new instructions, a commit review summary, and natural next steps—making sure to note the coin flip results and review as well.
[2025-10-07T02:39:30] codex

- Exposed the parity blocker: TRACE_PY still prints placeholder scaling data (`steps`=φ-steps only, `capture_fraction 1`, `polar 1.0`) rather than the live tensors from the physics pipeline (`src/nanobrag_torch/simulator.py:1324` & `src/nanobrag_torch/simulator.py:1332` & `src/nanobrag_torch/simulator.py:1341`). This keeps Phase L2 from producing a trustworthy PyTorch trace for the supervisor command.

- Hypotheses (ranked): 1) High confidence that the missing polarization tensor feed-through is the first divergence—once instrumentation threads the actual tensor into TRACE_PY, rerun the supervisor harness to confirm (next step: capture the traced pixel after the patch). 2) Medium confidence that capture_fraction must be pulled from `_apply_detector_absorption`; diff compare scaling traces after surfacing the stored scalar. 3) Medium confidence that the printed `steps` value omits source/mosaic factors; verify by logging the same aggregate used in normalization before continuing to Phase L3.

- Updated long-term plan focus to Phase L2b instrumentation: the CLI parity plan now requires simulator trace fixes, a regression test, and a refreshed harness before advancing (`plans/active/cli-noise-pix0/plan.md:239`). Aligned fix-plan next actions with that sequence so the ledger and plan stay in lock-step (`docs/fix_plan.md:458`).

- Composed a new supervisor memo directing Ralph to implement the TRACE_PY upgrade, add `tests/test_trace_pixel.py`, regenerate the PyTorch scaling trace, and archive comparison artifacts under `reports/2025-10-cli-flags/phase_l/scaling_audit/` (`input.md:1`).

- Coin flip came up heads; reviewed Ralph’s recent commits (`3e47b38`, `f3f116b`, `f466b39`) and found solid progress on C trace capture plus clear documentation of the current harness blocker—no regressions detected, but the instrumentation gap explains the stalled Attempt #56.

- Next step for Ralph: execute the refreshed Phase L2b plan—patch TRACE_PY, land the regression test, rerun the scaling harness, diff against the C trace, and log the attempt before proceeding to L2c.
[2025-10-07T02:39:34] tokens used: 213,174
[2025-10-07T02:39:34] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 04fb39f68a05b87617484e8e3fbf36714efe1d68..5b01654e825619c7e28fc1c5f77eb44f3b82ff63
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,10 +455,11 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-10-06 refresh):
-  1. Phase L2b — Extend the PyTorch trace harness to emit the same scaling quantities (I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence) and save the log as `trace_py_scaling.log` under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
-  2. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  3. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh):
+  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index e505089dbc8573d9be316084c09a290c24d416bd..134552af42c386200c78c8a4006793ea7d35c782
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -9,6 +9,7 @@
 - Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
 - Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
+- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
@@ -236,7 +237,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Extend PyTorch trace harness | [ ] | Update `reports/2025-10-cli-flags/phase_i/supervisor_command/trace_harness.py` (or create `phase_l/scaling_audit/trace_harness.py`) to log the same quantities: I_before_scaling, ω, polarization, capture_fraction, steps, r_e², fluence, final intensity. Execute with `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python trace_harness.py` and store output as `trace_py_scaling.log`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

Reading prompt from stdin...
To github.com:hoidn/nanoBragg.git
   5661ef5..35bdfc6  HEAD -> feature/spec-based-2
Waiting for Ralph to complete i=63...
Already up to date.
[sync] state: actor=ralph status=waiting-ralph iter=63
Updating 35bdfc6..856793b
Fast-forward
 sync/state.json | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
From github.com:hoidn/nanoBragg
   35bdfc6..856793b  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=ralph status=running-ralph iter=63
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating 856793b..6b055dc
Fast-forward
 src/nanobrag_torch/simulator.py |  93 +++++++++++----
 tests/test_trace_pixel.py       | 242 ++++++++++++++++++++++++++++++++++++++++
 2 files changed, 316 insertions(+), 19 deletions(-)
 create mode 100644 tests/test_trace_pixel.py
From github.com:hoidn/nanoBragg
   856793b..6b055dc  feature/spec-based-2 -> origin/feature/spec-based-2
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating 6b055dc..0b5840e
Fast-forward
 docs/fix_plan.md | 14 ++++++++++++++
 1 file changed, 14 insertions(+)
From github.com:hoidn/nanoBragg
   6b055dc..0b5840e  feature/spec-based-2 -> origin/feature/spec-based-2
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating 0b5840e..8fd91fd
Fast-forward
 sync/state.json | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)
From github.com:hoidn/nanoBragg
   0b5840e..8fd91fd  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=galph status=complete iter=64
Ralph completed iteration 63; proceeding to 64
