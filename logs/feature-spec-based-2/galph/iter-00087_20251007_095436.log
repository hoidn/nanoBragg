Already up to date.
Waiting for expected_actor=galph...
Already up to date.
[sync] state: actor=galph status=complete iter=87
To github.com:hoidn/nanoBragg.git
   0a203dc..09d2ee2  HEAD -> feature/spec-based-2
$ codex exec -m gpt-5-codex -c model_reasoning_effort=high --dangerously-bypass-approvals-and-sandbox
[2025-10-07T09:54:40] OpenAI Codex v0.38.0 (research preview)
--------
workdir: /Users/ollie/Documents/nanoBragg3
model: gpt-5-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
--------
[2025-10-07T09:54:40] User instructions:
<role>
planning, review and analysis. do not make implementation code changes.
</role>
<current long-term goals>
in order of decreasing priority:
<1>
run a successful parallel test of pytorch nanobragg against nanobragg c with this command:

nanoBragg  -mat A.mat -floatfile img.bin -hkl scaled.hkl  -nonoise  -nointerpolate -oversample 1  -exposure 1  -flux 1e18 -beamsize 1.0  -spindle_axis -1 0 0 -Xbeam 217.742295 -Ybeam 213.907080  -distance 231.274660 -lambda 0.976800 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527 -odet_vector -0.000088 0.004914 -0.999988 -sdet_vector -0.005998 -0.999970 -0.004913 -fdet_vector 0.999982 -0.005998 -0.000118 -pix0_vector_mm -216.336293 215.205512 -230.200866  -beam_vector 0.00051387949 0.0 -0.99999986  -Na 36  -Nb 47 -Nc 29 -osc 0.1 -phi 0 -phisteps 10 -detector_rotx 0 -detector_roty 0 -detector_rotz 0 -twotheta 0

this will require first adding support for the following cli params to pytorch nanobragg:
-nonoise
-pix0_vector_mm

</1>
<2>
review and firm up if needed plans/active/vectorization.md. then implement it (or rather, delegate its implementation to ralph), starting with the tricubic interpolation vectorization 
</2>
</current long-term goals>
</past long term goals>
(just for archival / reference purposes):
- error-correcting the engineer agent 
- profiling pytorch nanobragg to find performance bottlenecks. analyzing to understand the CRITICAL slowness of the pytorch implementation compared to reference C, which persists after our recent improvement attempts
- finding performance issues in the PyTorch reimplementation of nanobragg (originally a C code) and speeding it up. It should be efficiently vectorized and faster than C nanobragg, but currently it's slower 
- ensuring that the pytorch implementation uses fp32 (not fp64) by default 
- understanding why pytorch is slower than C in the warm condition at 4096x4096 resolution and fixing the underlying performance issue, if one is found. THIS IS A CRITICAL PERFORMANCE ISSUE.
- once all the above are taken care of: building a user-facing showcase of autodiff-based parameter refinement, with plots / visuals

</past long-term goals>
<task>
You are galph, a planner / supervisor agent. you are overseeing the work of an agent (ralph) that is running prompts/main.md in a loop, using docs/fix_plan.md as its instruction set and long term memory. 

You will get invoked repeatedly, for multiple iterations. Use galph_memory.md to communicate with your future self. You'll plans under plans/, when needed, to help steer multi-turn efforts by the coder agent (ralph). Those plans will be cross referenced from docs/fix_plan.md so that ralph can find / read them. 

At the start of every invocation:
- Run `timeout 30 git pull --rebase` with a hard 30-second timeout  to sync with origin before reviewing context. If the command times out, immediately abort any partial rebase (`git rebase --abort`) and fall back to a normal merge via `git pull --no-rebase`. Whatever path you take, resolve resulting conflicts (docs/fix_plan.md is a frequent hotspot) and document key decisions in galph_memory.md. If the pull reports conflicts:
  * Run `git status --short` to list conflicted files.
  * Fix each file (remove conflict markers, keep the intended content) and stage it with `git add`.
  * When the index is clean, resume with `timeout 30 git rebase --continue --no-edit`. IMPORTANT: NEVER RUN THIS COMMAND WITHOUT TIMEOUT
- After resolving any conflicts, read the latest galph_memory.md entry (and any linked plan files) so you do not lose past context.
- Prune or summarize older entries when they become stale or redundant.

During the main, central phase of invocation you will be doing essentially three things:
<main analysis step 1> 
An initial analysis pass in which you gather and review information in order to choose a <focus issue> for the turn, corresponding to <instructions> steps <0> through <3>. This will involve a multi-category selection process. You will choose both a <focus issue> and an associated <Action type> for the remainder of the iteration. The available <Action type>s are the following:
<Action type> list:
<1>
<Evidence collection> (i.e. Evidence task)
No full pytest runs. Allowed: non-mutating probes and CLI validation tools (e.g., scripts/validation/*, nb-compare, pixel trace); pytest --collect-only to validate selectors; and in TDD mode only, a single targeted pytest run to confirm the newly authored test fails. Do not change production code. This gate applies only to galph. Ralph is exempt. Do not include any Phase label in input.md.

Evidence collection includes the following subtype:
    Callchain Tracing (Evidence subtype)
    - When to use: before code edits when the pipeline/factor order relevant to the current issue is unclear; when onboarding a new surface; or upon parity failures where the locus is unknown.
    - Directive: first, emit descriptions for the following things:
       - <analysis_question> (required): Free‑text description of the behavior/bug/perf issue to investigate. This will be closely related to <focus issue>, but viewed from the point of view of relevant code execution paths and their entry points.
       - <initiative_id> and <scope_hints> 

    - Directive: then read and carefully follow the instructions in `prompts/callchain.md` with the question‑driven invocation (<analysis_question>) and produce standardized artifacts under `reports/<initiative_id>/`.
    - Example invocation variables (fill at run time):
      analysis_question: "<what are we trying to understand or fix?>"
      initiative_id: "<short‑slug>"
      scope_hints: ["CLI flags", "normalization", "scaling"]
      roi_hint: "<minimal input/ROI>"
      namespace_filter: "<project primary package>"
      time_budget_minutes: 30
    - Expected outputs:
      - `reports/<initiative_id>/callchain/static.md` (entry→sink with file:line anchors)
      - `reports/<initiative_id>/callgraph/dynamic.txt` (optional, module‑filtered)
      - `reports/<initiative_id>/trace/tap_points.md` (proposed numeric taps with owners)
      - `reports/<initiative_id>/summary.md` (question‑oriented narrative + next steps)
      - `reports/<initiative_id>/env/trace_env.json`
    - Guardrails: evidence‑only (no prod edits), module/device/dtype neutrality, small ROI, respect Protected Assets, stable key names in traces.
</Evidence collection>
</1>
<2>
<Debug> i.e. Debugging — Hypothesis + Triage (initial pass):
- pairs with <focus issue>s that are related to debugging or parity analysis.
<steps>
- Formulate 1–3 plausible hypotheses for the observed gap/issue.
- Triage each with checks appropriate to scope using existing artifacts or quick, documented reproductions; this triage may constitute the primary work for the iteration. Record outcomes succinctly in the evidence.
- For the top hypothesis, state confidence and the single confirming step to run next. Deliver 1–3 ranked hypotheses with supporting evidence, artifact paths, and the next confirming step.
</steps>
</Debug>
</2>
<3>
<Planning>
- we will be calling the plan topic the <focus issue> of this turn.
- based on which long term <goal> and sub-goal is that effort / plan? 
- Which existing docs/fix_plan.md items does it (i.e. the <focus issue>) relate to? 
- Documentation review for <focus issue>: Using `docs/index.md` as the map, identify and read the specific documents relevant to the chosen <focus issue> (e.g., component contracts, architecture ADRs, parity/testing strategy). List the file paths you will rely on (with a one‑line rationale each) before drafting or updating the plan.
- think deeply. draft / redraft the plan and save it to a .md under plans/active/. Structure the write-up as a phased implementation document (see `plans/archive/general-detector-geometry/implementation.md` for tone/shape): begin with context + phase overviews, then outline each phase’s intent, prerequisites, and exit criteria. When a phase benefits from explicit tracking, embed a checklist table using the `ID | Task Description | State | How/Why & Guidance` format (with `[ ]`, `[P]`, `[D]` markers) inside that phase section.
  • Include reproduction commands, owners (if known), and decision rules in the guidance column.
  • Favor narrative flow first; layer checklists only where they clarify verification steps or deliverables.
  • Mini-template to crib when drafting:
    ```md
    ## Context
    - Initiative: <initiative>
    - Phase Goal: <outcome>
    - Dependencies: <docs/tests>

    ### Phase A — <short title>
    Goal: <what this phase proves or delivers>
    Prereqs: <artifacts or measurements required before starting>
    Exit Criteria: <verifiable completion signal>

    | ID | Task Description | State | How/Why & Guidance (including API / document / artifact / source file references) |
    | --- | --- | --- | --- |
    | A1 | <Key diagnostic or implementation step> | [ ] | Run `<command>`; capture results under `reports/<date>/...`. |
    | A2 | <Follow-up validation> | [ ] | Compare against `<artifact>`; stop if deviation > threshold. |
    ```
- When refreshing an existing plan, retrofit it to this phased format before adding or editing tasks.
- review docs/fix_plan.md. edit if needed. cross reference the new plans .md so that ralph can find it.
</Planning>
</3>

<4>
<Review or housekeeping>
- Review and evaluate ralph's work. Scrutinize the commit history. Look at the diffs. 
- Are the docs/fix_plan.md contents and priorities sane? things to consider:
  - if docs/fix_plan.md is longer than 1000 lines it should be housecleaned. If it's disorganized and / or internally inconsistent, consider how this could be addressed. 
- after considering the above, you have enough information to choose a <focus issue> for this turn. do so. Consider the nature of the <focus issue>:
    - Do we need a new docs/fix_plan item to put ralph back on course, fix one of his mistakes, or instruct him to do something that he overlooked? If so, draft it and add it to docs/fix_plans
    - does the <focus issue> involve identified issues in docs/fix_plan.md? If so, fix them. If you decide to shorten docs/fix_plan.md, the least relevant portions should be moved to archive/fix_plan_archive.md (with summary + archive cross reference if appropriate)
</Review or housekeeping>
</4>

In addition to selecting an <Action type> and <focus issue> you will choose a <Mode> flag. 
 - Mode flags are combinable and refine execution, and are applicable to both the supervisor (you) and ralph; do not include any Phase label in input.md.
 - available modes: <TDD | Parity | Perf | Docs | none>
<mode descriptions>
TDD mode (supervisor-scoped): When operating in supervisor evidence mode, author/update a single minimal failing test that encodes the acceptance criterion. Confirm it fails via a targeted pytest selector; record the selector and expected failure text in input.md. Do not change production code, and do not include any Phase label in input.md.
</mode descriptions>

</main analysis step 1>

<main analysis step 2>
Here you will do a deep analysis. You will follow the guidelines of the selected <Mode> and <Action type>, applying them to the selected <focus issue>. This corresponds to <instructions> step <3.2>.


</main analysis step 2>

<main analysis step 3>
Here you synthesize the analysis findings into output files (<instructions> steps <3.5> to <4>)
</main analysis step 3>


Before concluding each invocation:
- Append a concise update to galph_memory.md capturing key findings, decisions, and open questions (reference file paths or plan names).
- Note any follow-up actions you expect Ralph to take.
- If no substantial update is needed, explicitly log that fact so future runs know context was reviewed.
</task>

<instructions>
<0>
READ the following files:
- Index of project documentation: `./docs/index.md`
- $SPECS: `./specs/spec-a.md`
- $ARCH: `./arch.md` (ADR-backed implementation architecture; reconcile design with spec, surface conflicts)
- docs/development/c_to_pytorch_config_map.md — C↔Py config parity and implicit rules
-- docs/debugging/debugging.md — Parallel trace-driven debugging SOP
- $PLAN: `./fix_plan.md` (living, prioritized to‑do; keep it up to date)
- $TESTS: `./docs/development/testing_strategy.md` (testing philosophy, tiers, seeds/tolerances, commands)
- Set `AUTHORITATIVE_CMDS_DOC=./docs/development/testing_strategy.md` (or project‑specific path) and consult it for authoritative reference commands and test discovery.
<0>
<1>
do a deep analysis of the codebase in light of the <current long term goals>. What are some current issues / gaps and possible approaches to resolving them? Review docs/fix_plan.md and plans/active/, as previous iterations of you may have already done some legwork.
 
At this point, you can do a premature <Callchain tracing> action ahead of time, if it helps you gather information needed for this analysis.
</1>
<2>
flip a coin using python. if it comes up <heads>:
review ralph's work over the last N (~10 but can be more or less - you decide) iterations. Check the commit history. Has the work been productive? Have there been regressions? Do we need to provide any feedback / course-correction?
</heads>
if it comes up <tails>: proceed to step <3>
</2>
<3>
Given your findings in <1> and <2>, think about whether there's any need for a multi-turn planning effort -- i.e. ralph can't see the forest for the trees and may struggle with major refactorings and multi-turn implementation efforts unless they are coordinated by you. Is there a need for such planning *right now*? If such a plan is already spoken for (plans/active/ or wherever else past galph might have saved to), does that plan require updates or is it up to date / high quality and simply pending? IFF you determine the need for a new plan or modification of an existing one:
<yes case>
This plan-related work is now your <focus issue>.
You will be following the <Planning> <Action type> instructions. Think about what this will entail but don't execute the <Planning> instructions yet.
</yes case>
</no case>
Since you are not <Planning>, you must select one of the other <Action type>s and an associated <focus issue> based on what you think is the most productive work increment for this iteration. Think about the associated work, but don't execute it yet.
</no case>
</3>
<3.2>
- Documentation review for <focus issue>: From `docs/index.md`, enumerate and read the documents that are most relevant to the chosen <focus issue>`; note the key file paths you will rely on for the impending analysis.
- Then carefully follow the guidelines of the selected <Mode> and <Action type>, applying them to the selected <focus issue>. Complete the task that you selected in step <3>.
</3.2>
<3.5>
Render and write ./input.md (supervisor→engineer steering memo). Overwrite the entire file every invocation. Keep length ~100–200 lines. Include these sections in order:
Header:
- Summary: <one‑sentence goal for this loop>
 - Mode: <TDD | Parity | Perf | Docs | none>
- Focus: <plan item ID/title from docs/fix_plan.md>
- Branch: <expected branch>
- Mapped tests: <validated list | none — evidence‑only>
- Artifacts: <key paths to produce under reports/>
- Do Now: 1 line naming the exact docs/fix_plan.md item (ID and title) to execute this loop, plus the exact pytest command/env to reproduce it (only when an authoritative mapping exists). If no such test exists, the Do Now MUST be to author the minimal targeted test first and then run it. If you intend to delegate the choice, write “Do Now: delegate” and provide decision guidance below.
  - Note: If operating in supervisor <Evidence collection> (internal-only), do not run the full suite. Allowed: pytest --collect-only to validate selectors; in TDD mode only, run one targeted selector to confirm a new failing test. All other test execution is deferred to Ralph. Do not include any Phase label in input.md.
<Do Now guidelines>
Verification scripts: You may run nb-compare and scripts/validation/* to collect metrics and artifacts (no code changes). Record outputs under reports/.
Mapped tests under supervisor evidence mode: Include exact selectors in input.md; validate with --collect-only; do not run them (except the TDD failure confirmation). Do not include any Phase label in input.md.
Command Sourcing (tests): Only include an exact pytest command in Do Now when sourced from an authoritative mapping (e.g., docs/development/testing_strategy.md) or an existing, known test file/identifier. If no authoritative mapping exists, set the Do Now task to author the minimal targeted test first; do not guess at a pytest selector here.
Implementation: refer to phased plans, checklists, and all associated artifacts
</Do Now guidelines>
- If Blocked: fallback action (what to run/capture, how to record it in Attempts History).
- Priorities & Rationale: 3–6 bullets with file pointers (specs/arch/tests/plan) justifying the Do Now.
- How-To Map: exact commands (pytest, nb-compare), env vars, ROI/thresholds, and where to store artifacts; prefer using `scripts/validation/*` when available.
- Pitfalls To Avoid: 5–10 crisp do/don’t reminders (device/dtype neutrality, vectorization, Protected Assets, two‑message loop policy, no ad‑hoc scripts).
- Pointers: file paths with line anchors to the most relevant spec/arch/testing_strategy/fix_plan entries for this loop.
- Next Up (optional): 1–2 candidates Ralph may choose next loop if he finishes early (still one thing per loop).

 Rules:
 - Ralph must not edit input.md. You (galph) are the single writer.
 - If input.md’s Do Now conflicts with an existing in_progress selection in fix_plan.md, Ralph is allowed to switch; he must record the change in Attempts History.
- Do not instruct Ralph to run the full test suite more than once per turn. Require targeted tests first; run the full suite once at the end only if code changed. For prompt/docs‑only loops, require `pytest --collect-only -q` instead.
- Prefer referencing reusable scripts under `scripts/validation/` from Do Now when available.
- Commit input.md each run as part of step <4> (commit only if the content changed).
 </3.5>

<4>
Before finishing the loop, enforce git hygiene:
- Run `git status --short` to inspect new or modified files that appeared during this invocation.
- Review diffs for each change; revert only if you created it this loop and it was accidental.
- Stage intentional updates with `git add -A` and commit via `git commit -m "SUPERVISOR: <scope> - <tests or rationale>"`, noting any tests run (use `tests: not run` when you skip them).
- After committing, run `git push` to share the updates. If the push is rejected, `timeout 30 git pull --rebase`, resolve conflicts (capture decisions—especially for docs/fix_plan.md—in galph_memory.md), then push again.
- If you deliberately leave the tree dirty, document the rationale in galph_memory.md so the next invocation knows why.
</4>
</instructions>
<notes>
- ignore 'routing violations'. these are out of scope.
- ignore AT parallel 012-related items for now
</notes>
Now carefully and exhaustively follow your <instructions>, while keeping in mind the <task> directives.


[2025-10-07T09:54:42] thinking

**Preparing to update repository**
[2025-10-07T09:54:43] exec bash -lc 'timeout 30 git pull --rebase' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:54:45] bash -lc 'timeout 30 git pull --rebase' succeeded in 2.14s:
Already up to date.
[2025-10-07T09:54:45] tokens used: 795

[2025-10-07T09:54:46] thinking

**Preparing to read latest galph_memory.md entry**
[2025-10-07T09:54:47] exec bash -lc 'ls galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:54:48] bash -lc 'ls galph_memory.md' succeeded in 699ms:
galph_memory.md
[2025-10-07T09:54:48] tokens used: 933

[2025-10-07T09:54:49] thinking

**Viewing file tail**
[2025-10-07T09:54:49] exec bash -lc 'tail -n +1 galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:54:50] bash -lc 'tail -n +1 galph_memory.md' succeeded in 750ms:
## 2025-09-30 (loops 1-4 summary)
- Established parity backlog: AT-020/021/022/024 failing; created first supervisor plan for AT-021 and enforced prompts/debug.md routing.
- Diagnosed AT-024 random misset cap mismatch (π/2 vs 90 radians) and opened dedicated plan; insisted Ralph stop verification-only loops.
- Flagged torch.compile warm-up overhead (per-instance compilation, inline tensor factories) and opened AT-012 rotation parity plan (Phases A–E).
- Documented performance hotspots (`_compute_physics_for_position` guard tensors, `Crystal.compute_cell_tensors` clamps) and chronic dirty worktree artifacts blocking `git pull --rebase`.

## 2025-09-30 (loops 5-10 summary)
- Authored repo hygiene checklist (REPO-HYGIENE-002) to undo accidental `golden_suite_generator/nanoBragg.c` churn and purge stray reports.
- Created PERF-PYTORCH-004 plan to hoist constants, introduce compile caching, and remove Dynamo graph breaks; reiterated need for prompts/debug.md while parity fails.
- Repeatedly documented routing violations from Ralph and emphasized restoring Core Rule #13 metric duality after WIP commit 058986f swapped `V_actual` for formula volume.
- Noted that `_compute_physics_for_position` still recompiles each Simulator and that benchmarks in `reports/benchmarks/20250930-004916/` show ≥200× slowdown vs C for small detectors.

## 2025-09-30 (ralph debugging loop — AT-012 float32 regression)
- **Context**: AT-PARALLEL-012 test_simple_cubic_correlation failing (43/50 peaks matched, need ≥95%) after DTYPE-DEFAULT-001 (commit 8c2ceb4) switched simulator to native float32.
- **Investigation**:
  * Reproduced: 43/50 peaks matched (86%), corr≈1.0 (perfect physics) ✅
  * Parity matrix test: PASSES perfectly ✅ (live C↔Py comparison with current float32)
  * **Root cause identified**: PyTorch float32 produces 7× more unique values (4901 vs C's 669) in hot pixels, fragmenting intensity plateaus. Peak detection algorithm (scipy.ndimage.maximum_filter) is sensitive to plateau structure.
  * Parallel comparison: C float32 beam center (20×20 ROI) has 91 unique values; PyTorch has 331 (3.6× fragmentation). Max absolute diff tiny (0.0017).
  * **First Divergence**: Numerical accumulation patterns differ (likely FMA, compiler opts, vectorized accumulation order, or torch.compile kernel fusion) → perfect correlation but incompatible plateau structure.
[2025-10-07T09:54:50] tokens used: 1,187

[2025-10-07T09:54:53] thinking

**Checking for more file content**
[2025-10-07T09:54:53] exec bash -lc 'tail -n 40 galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:54:54] bash -lc 'tail -n 40 galph_memory.md' succeeded in 706ms:
- Updated `plans/active/vectorization.md` gap snapshot + supervisory note to reflect Phase A completion and highlight Phase B design memo as the new blocker.
- Synced `docs/fix_plan.md` status/index for VECTOR-TRICUBIC-001 (Phase A complete, Phase B outstanding) and refreshed Next Actions to call for the Phase B design memo.
- Authored new `input.md` (Docs mode) directing Ralph to draft `reports/2025-10-vectorization/phase_b/design_notes.md` covering tasks B1–B3, including tensor shapes, polin3 tensorization, and gradient/device checklist.
- Follow-up for Ralph: produce the memo, cite baseline artifacts, outline regression/benchmark plan, and log the attempt in fix_plan before attempting any implementation work.
## 2025-11-18 (galph loop — VECTOR-TRICUBIC Phase C kick-off)
- Synced clean; reviewed latest plan/docs plus Ralph’s recent commits (`d30a581`, `30ab2d1`). Design memo now exists and captures B1–B3; CLI device audit revealed HKL tensors stuck on CPU but no regressions observed.
- Updated `plans/active/vectorization.md` context + Phase B table to mark B1–B3 complete, steering focus to Phase C implementation (C1–C3). Refreshed `docs/fix_plan.md` next-actions accordingly.
- Authored new `input.md` (Perf mode) directing Ralph to implement Phase C1 batched tricubic gather with a fresh targeted pytest and to log implementation notes under phase_c/.
- Expect Ralph next loop to land the gather vectorization, add the new test, and document deltas before moving to Phase C2 fallback validation. CLI-FLAGS-003 remains pending after gather work.
## 2025-11-18 (galph loop — CLI-FLAGS L3c audit prep)
- Chose Evidence focus on CLI-FLAGS-003 Phase L3c to unblock the supervisor-command parity goal.
- Reviewed src/nanobrag_torch/__main__.py:438-450 and 1068-1076; confirmed CLI still attaches HKL data after Crystal construction but leaves tensors on CPU and instantiates Crystal without propagating the CLI device/dtype.
- Noted interpolation flag wiring mismatch (cli writes `crystal.interpolation_enabled`) for follow-up once L3c closes.
- Replaced input.md with Parity-mode guidance directing Ralph to refresh the CLI HKL device probe, update cli_hkl_audit.md with current code references, and run pytest --collect-only for tests/test_cli_scaling.py.
- Expect Ralph to capture new probe artifacts under reports/2025-10-cli-flags/phase_l/structure_factor/, update the audit doc with 2025-11-18 findings, and log the attempt in docs/fix_plan.md before proceeding to L3d.

## 2025-11-18 (galph loop — CLI L3d planning refresh)
- Marked Phase L3c complete in `plans/active/cli-noise-pix0/plan.md` and added 2025-11-18 gap snapshot noting Attempt #80’s audit results; fix_plan Next Actions now target L3d implementation, L3e validation, and L3f documentation.
- Authored new `input.md` (Parity mode) instructing Ralph to add the L3d regression test, capture the failing selector, patch `__main__.py:1073` with `.to(device=device, dtype=dtype)`, rerun the probe, and log artifacts under `reports/2025-10-cli-flags/phase_l/structure_factor/`.
- Reminder for next loop: verify vectorization plan still shows C1 as open despite commit 88fd76a; plan update pending once CLI L3d work is underway.
[2025-10-07T09:54:54] tokens used: 4,054

[2025-10-07T09:54:56] thinking

**Preparing to review documentation**
[2025-10-07T09:54:57] exec bash -lc "sed -n '1,200p' docs/index.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:54:58] bash -lc "sed -n '1,200p' docs/index.md" succeeded in 709ms:
# Project Documentation Index

Welcome to the central index for all `nanoBragg-PyTorch` project documentation. This guide provides a map to the key technical specifications, development processes, and historical records that define the project.

---

## 🏛️ Permanent Documentation

These are the core, living documents that guide the project.

### Core Project Guides
### PyTorch Guardrails
* **[PyTorch Runtime Checklist](./development/pytorch_runtime_checklist.md)** – quick do/don’t list for vectorization and device placement. Always review before editing simulator code.
* **[PyTorch Design](./architecture/pytorch_design.md#vectorization-strategy)** – detailed explanation of the required broadcast shapes and batched flows.
* **[Testing Strategy §1.4](./development/testing_strategy.md#14-pytorch-device--dtype-discipline)** – required CPU/GPU smoke tests and compile checks.

* **[CLAUDE.md](../CLAUDE.md)** - The primary instruction set for the AI agent.
* **[README.md](../README.md)** - The main project entry point.
* **[README_PYTORCH.md](../README_PYTORCH.md)** - Comprehensive user guide for the PyTorch implementation, including CLI usage, parallel comparison tools, and visualization.
* **[PROJECT_STATUS.md](../PROJECT_STATUS.md)** - Tracks the current active development initiative.
[2025-10-07T09:54:58] tokens used: 6,333

[2025-10-07T09:55:00] thinking

**Moving to the next specification**
[2025-10-07T09:55:01] exec bash -lc "sed -n '1,160p' specs/spec-a.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:02] bash -lc "sed -n '1,160p' specs/spec-a.md" succeeded in 717ms:
# nanoBragg Spec A — Sharded Specification

Spec A is now organized across multiple shards. Read them together for the full normative contract.

- [spec-a-core.md](spec-a-core.md): Core physics, geometry, sampling, data, and associated acceptance tests.
- [spec-a-cli.md](spec-a-cli.md): CLI surface area, precedence rules, and CLI-focused acceptance tests.
- [spec-a-parallel.md](spec-a-parallel.md): Conformance profiles, parallel validation suites, and developer tooling requirements.

Each shard cross-references the others; updates must keep them consistent.
[2025-10-07T09:55:02] tokens used: 8,064

[2025-10-07T09:55:05] thinking

**Considering core spec files**
[2025-10-07T09:55:05] exec bash -lc "sed -n '1,200p' arch.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:06] bash -lc "sed -n '1,200p' arch.md" succeeded in 729ms:
# nanoBragg PyTorch — Implementation Architecture

Document version: 1.0 (aligned with specs/spec-a.md)
Target spec: specs/spec-a.md (single-file normative spec)
Implementation target: Python (>=3.11) + PyTorch (float32 default). Type blocks may use TypeScript-style for clarity; implement with Python dataclasses and torch tensors.

## Running The CLI

- Install (recommended):
  - `pip install -e .` from repo root (provides `nanoBragg` console script).
- Run (installed):
  - `nanoBragg -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Run (without install):
  - `PYTHONPATH=src python -m nanobrag_torch -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Minimal quick test (no HKL):
  - `nanoBragg -default_F 1 -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -detpixels 128 -pgmfile test.pgm`

Notes
- Entry point: `src/nanobrag_torch/__main__.py` (module runner `python -m nanobrag_torch`).
- Required inputs: provide `-hkl` (or existing `Fdump.bin`) or set `-default_F > 0`, and either `-mat` (MOSFLM A*) or `-cell`.
[2025-10-07T09:55:06] tokens used: 9,238

[2025-10-07T09:55:08] thinking

**Noticing file truncation and locating docs**
[2025-10-07T09:55:09] exec bash -lc "sed -n '1,200p' docs/development/c_to_pytorch_config_map.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:10] bash -lc "sed -n '1,200p' docs/development/c_to_pytorch_config_map.md" succeeded in 699ms:
# C-CLI to PyTorch Configuration Map

## Critical Importance Notice

**This document is the authoritative source of truth for configuration parity between nanoBragg.c and the PyTorch implementation.**

Before writing any test or implementation that involves C-code validation, you **MUST** consult this document. Failure to ensure 1:1 configuration parity is the most common source of bugs, particularly with:
- Implicit pivot mode logic
- Convention-dependent beam center calculations
- Rotation axis defaults
- Unit conversions

## Quick Reference Table

### Crystal Parameters

| C-CLI Flag | PyTorch Config Field | C Variable | Units/Convention | Critical Notes |
|------------|---------------------|------------|------------------|----------------|
| `-cell a b c al be ga` | `CrystalConfig.cell_a/b/c/alpha/beta/gamma` | `a[0], b[0], c[0], alpha, beta, gamma` | Å and degrees → radians | Must convert degrees to radians internally |
| `-N <val>` | `CrystalConfig.N_cells` | `Na, Nb, Nc` | Number of unit cells | Sets all three axes to same value |
[2025-10-07T09:55:10] tokens used: 11,709

[2025-10-07T09:55:11] thinking

**Opening debugging documentation**
[2025-10-07T09:55:12] exec bash -lc "sed -n '1,200p' docs/debugging/debugging.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:13] bash -lc "sed -n '1,200p' docs/debugging/debugging.md" succeeded in 682ms:
# nanoBragg PyTorch Debugging Guidelines

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** PyTorch Development Team

## Overview

This document outlines the debugging methodology and tools for the nanoBragg PyTorch implementation. The debugging approach is built around a parallel trace comparison between the C code and the PyTorch implementation to ensure correctness and facilitate rapid issue resolution.

## Core Debugging Philosophy

### Parallel Trace Comparison Rule

**CRITICAL:** All debugging of physics discrepancies MUST begin with a parallel trace comparison. This involves:
1. Generating a detailed, step-by-step log of intermediate variables from the original C code for a specific pixel (the "Golden Trace").
2. Generating an identical log from the PyTorch implementation using `scripts/debug_pixel_trace.py`.
3. Comparing these two logs line-by-line to find the first point of divergence.

This methodology is mandatory to avoid guesswork and ensure bugs are found deterministically.
[2025-10-07T09:55:13] tokens used: 14,092

[2025-10-07T09:55:15] thinking

**Checking for testing strategy docs**
[2025-10-07T09:55:15] exec bash -lc "sed -n '1,200p' docs/development/testing_strategy.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:16] bash -lc "sed -n '1,200p' docs/development/testing_strategy.md" succeeded in 700ms:
# nanoBragg PyTorch Testing Strategy

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** [Your Name/Team]

## 1. Introduction & Philosophy

This document outlines the comprehensive testing strategy for the PyTorch implementation of nanoBragg. The primary goal is to ensure that the new application is a correct, verifiable, and trustworthy scientific tool.
Our testing philosophy is a three-tiered hybrid approach, designed to build confidence layer by layer:

1. **Tier 1: Translation Correctness:** We first prove that the PyTorch code is a faithful, numerically precise reimplementation of the original C code's physical model. The C code is treated as the "ground truth" specification.
2. **Tier 2: Gradient Correctness:** We then prove that the new differentiable capabilities are mathematically sound.
3. **Tier 3: Scientific Validation:** Finally, we validate the model against objective physical principles.

All tests will be implemented using the PyTest framework.

### 1.4 PyTorch Device & Dtype Discipline

- **Device-neutral code:** Every PyTorch path MUST operate correctly on both CPU and CUDA tensors and across supported dtypes. Do not hard-code `.cpu()`/`.cuda()` calls, create CPU-only constants, or assume float64 execution when the caller may supply float32/half tensors.
[2025-10-07T09:55:16] tokens used: 16,578

[2025-10-07T09:55:19] thinking

**Preparing to examine fix_plan**
[2025-10-07T09:55:22] exec bash -lc "sed -n '1,200p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:22] bash -lc "sed -n '1,200p' docs/fix_plan.md" succeeded in 790ms:
# Fix Plan Ledger

**Last Updated:** 2025-10-17 (galph loop)
**Active Focus:**
- ROUTING: Close the reopened guard plan by capturing a fresh regression audit referencing commit `c49e3be` and re-confirming the guarded `loop.sh` flow (`plans/active/routing-loop-guard/plan.md` Phases A–C) before automation resumes.
- ROUTING-SUPERVISOR: Launch Phase A of `plans/active/supervisor-loop-guard/plan.md`, then drive Phase B guard work (B2–B4) and new task B5 to add `supervisor.sh` to docs/index.md so Protected-Asset policy covers the script before automation runs again.
- AT-012: Plan archived (`plans/archive/at-parallel-012-plateau-regression/plan.md`); monitor for regressions using `reports/2025-10-AT012-regression/phase_c_validation/` artifacts and re-open only if peak matches drop below spec.
- GRADIENT: Execute `plans/active/gradcheck-tier2-completion/plan.md` Phase A (A1–A3 baseline audit + env alignment) before adding misset/beam gradchecks from Phases B/C; once pass logs exist, close `[AT-TIER2-GRADCHECK]` with Phase D documentation updates.
- DTYPE: ✅ Complete. Plan archived to `plans/archive/dtype-default-fp32/`. All phases (A-D) finished; float32 defaults documented in arch.md, pytorch_runtime_checklist.md, CLAUDE.md, prompts/debug.md.
- PERF: Land plan task B7 (benchmark env toggle fix), rerun Phase B6 ten-process reproducibility with the new compile metadata, capture the weighted-source parity memo feeding C5, then execute Phase C diagnostics (C1/C2 plus C8/C9 pixel-grid & rotated-vector cost probes, and new C10 mosaic RNG timing) ahead of Phase D caching work (D5/D6/D7) and detector-scalar hoisting (D8).

## Index
| ID | Title | Priority | Status |
| --- | --- | --- | --- |
| [PROTECTED-ASSETS-001](#protected-assets-001-docsindexmd-safeguard) | Protect docs/index.md assets | Medium | in_progress |
| [REPO-HYGIENE-002](#repo-hygiene-002-restore-canonical-nanobraggc) | Restore canonical nanoBragg.c | Medium | in_progress |
| [PERF-PYTORCH-004](#perf-pytorch-004-fuse-physics-kernels) | Fuse physics kernels | High | in_progress |
| [AT-TIER2-GRADCHECK](#at-tier2-gradcheck-implement-tier-2-gradient-correctness-tests) | Implement Tier 2 gradient correctness tests | High | in_progress |
| [VECTOR-TRICUBIC-001](#vector-tricubic-001-vectorize-tricubic-interpolation-and-detector-absorption) | Vectorize tricubic interpolation and detector absorption | High | in_progress |
| [ABS-OVERSAMPLE-001](#abs-oversample-001-fix-oversample_thick-subpixel-absorption) | Fix -oversample_thick subpixel absorption | High | in_planning |
[2025-10-07T09:55:22] tokens used: 45,866

[2025-10-07T09:55:27] thinking

**Preparing for in-depth code analysis**
[2025-10-07T09:55:28] exec bash -lc 'ls plans/active' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:29] bash -lc 'ls plans/active' succeeded in 710ms:
cli-noise-pix0
gradcheck-tier2-completion
oversample-thick-subpixel.md
perf-pytorch-compile-refactor
routing-loop-guard
source-weight-normalization.md
supervisor-loop-guard
unification.md
vectorization.md
[2025-10-07T09:55:29] tokens used: 51,221

[2025-10-07T09:55:34] thinking

**Preparing to analyze CLI noise and pix0 features**
[2025-10-07T09:55:35] exec bash -lc "sed -n '1,200p' plans/active/cli-noise-pix0/plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:35] bash -lc "sed -n '1,200p' plans/active/cli-noise-pix0/plan.md" succeeded in 699ms:
## Context
- Initiative: CLI Parity for nanoBragg PyTorch vs C (supports long-term goal in prompts/supervisor.md)
- Phase Goal: Accept `-nonoise` and `-pix0_vector_mm` flags with C-equivalent semantics so the parallel comparison command in prompts/supervisor.md executes end-to-end.
- Dependencies: specs/spec-a-cli.md §§3.2–3.4, docs/architecture/detector.md §5, docs/development/c_to_pytorch_config_map.md (detector pivot + noise), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860 (flag behavior), docs/debugging/detector_geometry_checklist.md (pix0 validation), docs/development/testing_strategy.md §2 (CLI parity tests).
- Current gap snapshot (2025-10-18 refresh): Phase H4 parity landed (Attempt #25) with pix0 deltas < 2e-8 m and regression tolerances tightened; the remaining blocker before executing the supervisor command end-to-end is Phase I polarization alignment.
- Gap snapshot update (2025-10-19): Attempt #27 parity run reveals 1.24538e5× intensity scaling mismatch despite polarization fix; Phases J–L track the normalization diagnosis, fix, and closure required before long-term Goal #1 completes.
- Gap snapshot refresh (2025-10-22): Attempt #31 restored C precedence for custom detector vectors (pix0 override now gated again). Outstanding work: H5c PyTorch traces and Phase K normalization to eliminate the `F_latt`-driven intensity gap.
- Gap snapshot update (2025-10-24): Attempt #35 reran the PyTorch trace harness post-unit-fix; metrics still show |Δpix0| ≈ 1.145 mm, so a fresh diagnostics phase is required before resuming normalization work.
- Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
- Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
- Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
- Gap snapshot update (2025-11-11): Discovered the trace harness omitted `-beam_vector`, `-flux`, and `-beamsize`, forcing oversample=4 and a +Z beam; Py traces zeroed `F_cell` and inflated `steps`. Marked unresolved pending harness fix.
- Gap snapshot refresh (2025-11-12): Attempt #66 (commit 1d2e6ac) completed Phase L2b0 — the trace harness now feeds the full supervisor flag set, and the refreshed `trace_py_scaling.log` confirms `steps=10`, `fluence≈1.0e24`, and `r_e^2` parity with C. TRACE_PY still reports `I_before_scaling NOT_EXTRACTED`, `polar=0`, and `I_pixel_final=0`, so Phase L2b instrumentation must surface the real polarization/absorption tensors before we can quantify the K3e k_frac drift.
- Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
- Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
- Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code.
- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
- Gap snapshot update (2025-11-17 late): Attempt #78 completed the Phase L3c harness rerun; TRACE_PY now reports F_cell=190.27 with matching metadata, so the immediate focus moves to auditing CLI ingestion (L3c) and adding regression coverage (L3d).
[2025-10-07T09:55:35] tokens used: 51,597

[2025-10-07T09:55:38] thinking

**Starting analysis of goals**
[2025-10-07T09:55:38] exec bash -lc "sed -n '1,200p' plans/active/vectorization.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:39] bash -lc "sed -n '1,200p' plans/active/vectorization.md" succeeded in 717ms:
## Context
- Initiative: PERF-PYTORCH-004 vectorization backlog; supports long-term goal "Parallel parity + performance" from docs/fix_plan.md.
- Phase Goal: Deliver a production-ready, fully vectorized tricubic interpolation pipeline (gather + polynomial evaluation) and follow with detector absorption vectorization without regressing physics, gradients, or device neutrality.
- Dependencies: specs/spec-a-core.md §4 (Structure factors), specs/spec-a-parallel.md §2.3 (Interpolation acceptance tests), docs/architecture/pytorch_design.md §§2.2–2.4, docs/architecture/c_code_overview.md (tricubic routines), docs/development/testing_strategy.md §§2–4, docs/development/pytorch_runtime_checklist.md, `nanoBragg.c` lines 2604–3278 (polin3/polin2/polint + detector absorption), `tests/test_at_str_002.py`, `tests/test_at_abs_001.py`, `reports/benchmarks/20250930-165726-compile-cache/` (current performance references). No dedicated tricubic/absorption microbenchmarks exist yet; Phase A will author reusable harnesses under `scripts/benchmarks/` so future loops can rerun baselines without ad-hoc snippets.
- Gap Snapshot (2025-11-18): Phase A1–A3 evidence captured under `reports/2025-10-vectorization/phase_a/` (pytest logs plus tricubic/absorption baselines). **Phase B design memo landed in Attempt #3 (`reports/2025-10-vectorization/phase_b/design_notes.md`) covering tensor shapes, polynomial mapping, and gradient/device checklists.** Next blocker is executing Phase C implementation tasks using that design as the contract.
- Supervisory note (2025-11-18 → refreshed 2025-11-18b): fix_plan#VECTOR-TRICUBIC-001 now pivots to Phase C execution (tasks C1–C3). Keep `KMP_DUPLICATE_LIB_OK=TRUE` exported, reuse the design memo’s Section 2/3 diagrams, and stage changes behind feature-flag-friendly commits to ease review if batching becomes necessary.

### Phase A — Evidence & Baseline Capture
Goal: Document the current (non-vectorized) behavior, warnings, and performance so we can prove the impact of the refactor.
Prereqs: Editable install (`pip install -e .`), C binary available for parity (when needed), environment variable `KMP_DUPLICATE_LIB_OK=TRUE`.
Exit Criteria: Baseline pytest log, timing snippet results, and notes stored under `reports/2025-10-vectorization/phase_a/` with references in docs/fix_plan.md Attempt history.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| A1 | Re-run tricubic acceptance tests | [D] | ✅ Completed 2025-10-06 (`reports/2025-10-vectorization/phase_a/test_at_str_002.log`, `.collect.log`, `test_at_abs_001.log`, `env.json`). Re-run only if dependencies change or new warnings appear. |
| A2 | Measure current tricubic runtime | [D] | ✅ Completed 2025-10-07 (`scripts/benchmarks/tricubic_baseline.py` created; CPU & CUDA baselines captured in `phase_a/tricubic_baseline.md` and `phase_a/tricubic_baseline_results.json`). Key finding: scalar-only implementation @ ~1.4 ms/call (CPU), ~5.5 ms/call (CUDA). |
| A3 | Record detector absorption baseline | [D] | ✅ Completed 2025-10-07 (`scripts/benchmarks/absorption_baseline.py` created; CPU & CUDA baselines captured in `phase_a/absorption_baseline.md` and `phase_a/absorption_baseline_results.json`). GPU shows 5.4× speedup for 512² detector. |

### Phase B — Tricubic Vectorization Design
Goal: Lock the tensor-shape design, broadcasting plan, and gradient checks before code changes.
[2025-10-07T09:55:39] tokens used: 54,697

[2025-10-07T09:55:42] thinking

**Reviewing instructions and goals**
[2025-10-07T09:55:43] exec bash -lc 'rg "CLI-FLAGS-003" -n docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:43] bash -lc 'rg "CLI-FLAGS-003" -n docs/fix_plan.md' succeeded in 710ms:
22:| [CLI-FLAGS-003](#cli-flags-003-handle-nonoise-and-pix0_vector_mm) | Handle -nonoise and -pix0_vector_mm | High | in_progress |
450:## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
463:  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
[2025-10-07T09:55:43] tokens used: 57,499

[2025-10-07T09:55:45] thinking

**Reviewing nearby section**
[2025-10-07T09:55:46] exec bash -lc "sed -n '450,620p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:46] bash -lc "sed -n '450,620p' docs/fix_plan.md" succeeded in 707ms:
## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
- Priority: High
- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
- Owner/Date: ralph/2025-10-05
- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
- Reproduction (C & PyTorch):
  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
- Next Actions (2025-11-19 evidence refresh):
  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
- Attempts History:
  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
    Artifacts:
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log:1-41` - Refreshed trace with live values (polar=0.91464138, omega_pixel=4.20404859e-07 sr, F_cell=0, I_before_scaling=0)
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1-54` - Phase L2c comparison report showing first divergence and all divergent factors
[2025-10-07T09:55:46] tokens used: 57,840

[2025-10-07T09:55:50] thinking

**Reviewing fix_plan and input references**

I'm preparing to do a deep analysis by checking the current fix_plan effort around attempt #86 with pyro, while cross-referencing docs and input.md for context and to ensure alignment with long-term goals. I'll gather info and spot gaps before proceeding further.
[2025-10-07T09:55:51] exec bash -lc "sed -n '1,160p' input.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:55:52] bash -lc "sed -n '1,160p' input.md" succeeded in 692ms:
Summary: Regenerate and analyse the per-φ scaling traces so we can pinpoint the lattice-factor sign flip that blocks the supervisor command parity run.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: [tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics]
Artifacts:
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log
- reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.{log,json}
- reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md
- reports/2025-10-cli-flags/phase_l/scaling_validation/{scaling_validation_summary_20251119.md,metrics.json,run_metadata.json}
Do Now: CLI-FLAGS-003 — run `PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --config supervisor --device cpu --dtype float32 --out trace_py_scaling_20251119.log`, then `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validation/compare_scaling_traces.py --c reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log --py reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log --out reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md`, finally `KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics -q` to confirm instrumentation stability.
If Blocked: Capture the full harness stdout to `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_raw.txt`, note whether any `TRACE_PY_PHI` lines appear, and log the failure mode plus commands under docs/fix_plan.md Attempt history before requesting supervisor guidance.
Priorities & Rationale:
- docs/fix_plan.md:460-463 promotes refreshed per-φ scaling validation as the top blocker before documentation or parity reruns.
- plans/active/cli-noise-pix0/plan.md:20-33,260-266 records that the existing per-φ artifact lacks `TRACE_PY_PHI`, and that Phase L3e remains open until new evidence is captured.
- reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md:1-41 links the residual 24% intensity gap directly to missing lattice-factor parity, prioritising renewed trace capture.
- reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md:1-96 documents the previous instrumentation contract and expected schema, which the new run must reproduce.
- scripts/validation/compare_scaling_traces.py:1-320 formalises the tolerance envelope (≤1e-6) that L3e must satisfy before the plan can progress.
- reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log anchors the C reference that every refreshed per-φ trace must match within Δk < 5e-4.
How-To Map:
[2025-10-07T09:55:52] tokens used: 61,243

[2025-10-07T09:56:03] thinking

**Evaluating long-term CLI goals**
[2025-10-07T09:56:03] exec bash -lc "python - <<'PY'
import random
print(random.choice(['heads','tails']))
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:56:04] bash -lc "python - <<'PY'
import random
print(random.choice(['heads','tails']))
PY" succeeded in 742ms:
heads
[2025-10-07T09:56:04] tokens used: 63,396

[2025-10-07T09:56:07] thinking

**Reviewing recent commit history**
[2025-10-07T09:56:07] exec bash -lc 'git log --oneline -10' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:56:08] bash -lc 'git log --oneline -10' succeeded in 724ms:
09d2ee2 [SYNC i=87] actor=galph status=running
0a203dc [SYNC i=87] actor=ralph → next=galph status=ok ralph_commit=c3efd5a
c3efd5a CLI-FLAGS-003 Phase L3e: Per-φ scaling validation complete
f3e65b8 [SYNC i=86] actor=ralph status=running
e165240 [SYNC i=86] actor=galph → next=ralph status=ok galph_commit=71dc1dc
71dc1dc SUPERVISOR: CLI-FLAGS-003 per-phi prep - tests: not run
1227bbd [SYNC i=86] actor=galph status=running
2ef99aa [SYNC i=86] actor=ralph → next=galph status=ok ralph_commit=f08d657
f08d657 CLI-FLAGS-003 Phase L3e: Add per-φ lattice trace instrumentation
8e92b48 [SYNC i=85] actor=ralph status=running
[2025-10-07T09:56:08] tokens used: 63,666

[2025-10-07T09:56:11] thinking

**Reviewing recent commits for evaluation**
[2025-10-07T09:56:12] exec bash -lc 'git show c3efd5a --stat' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:56:12] bash -lc 'git show c3efd5a --stat' succeeded in 759ms:
commit c3efd5a89d9902ff95b8fd77cd62d226c3f93e7a
Author: ollie <ohoidn>
Date:   Tue Oct 7 02:53:46 2025 -0700

    CLI-FLAGS-003 Phase L3e: Per-φ scaling validation complete
    
    - Executed trace harness generating 10 TRACE_PY_PHI entries (φ=0° to 0.1°) + 40 TRACE_PY lines
    - Reran compare_scaling_traces.py with ≤1e-6 tolerance enforcement
    - Confirmed pytest instrumentation test passes 4/4 variants (cpu/cuda × float32/float64)
    - First divergence: I_before_scaling (C=943654.809, PyTorch=713431, -24.4% rel)
    - Scaling factors r_e², fluence, steps, capture_fraction all PASS; polar/omega/cos_2theta MINOR
    - Phase L3e exit criteria satisfied: per-φ JSON + scaling validation artifacts captured
    
    Test collection: 652 tests collected successfully
    Evidence-only loop: no production code changes

 docs/fix_plan.md                                   | 15 ++++
 .../per_phi/trace_py_scaling_20251119_per_phi.json | 82 ++++++++++++++++++++++
 .../per_phi/trace_py_scaling_20251119_per_phi.log  | 10 +++
 .../phase_l/scaling_audit/trace_py_env.json        |  6 +-
[2025-10-07T09:56:12] tokens used: 64,208

[2025-10-07T09:56:14] thinking

**Reviewing recent modifications**
[2025-10-07T09:56:15] exec bash -lc "git show c3efd5a:docs/fix_plan.md | sed -n '450,520p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:56:16] bash -lc "git show c3efd5a:docs/fix_plan.md | sed -n '450,520p'" succeeded in 711ms:
## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
- Priority: High
- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
- Owner/Date: ralph/2025-10-05
- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
- Reproduction (C & PyTorch):
  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
- Next Actions (2025-11-19 evidence refresh):
  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
- Attempts History:
  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
    Artifacts:
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log:1-41` - Refreshed trace with live values (polar=0.91464138, omega_pixel=4.20404859e-07 sr, F_cell=0, I_before_scaling=0)
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1-54` - Phase L2c comparison report showing first divergence and all divergent factors
[2025-10-07T09:56:16] tokens used: 65,808

[2025-10-07T09:56:18] thinking

**Reviewing recent commit status**
[2025-10-07T09:56:18] exec bash -lc 'git show f08d657 --stat' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:56:19] bash -lc 'git show f08d657 --stat' succeeded in 730ms:
commit f08d6576fb119e5bf23157b6a34041ac848f851b
Author: ollie <ohoidn>
Date:   Tue Oct 7 02:32:59 2025 -0700

    CLI-FLAGS-003 Phase L3e: Add per-φ lattice trace instrumentation
    
    Extend Simulator to emit TRACE_PY_PHI entries for each φ step, enabling per-φ
    parity validation to localize F_latt drift.
    
    Changes:
    - src/nanobrag_torch/simulator.py (lines 1424-1460): Add per-φ tracing loop
      that reuses rotated vectors and emits k_frac, F_latt_b, F_latt per step
    - reports/.../trace_harness.py (lines 260-333): Capture TRACE_PY_PHI lines,
      write per-φ log + structured JSON for comparison tooling
    - reports/.../comparison_summary_20251119.md: Document instrumentation + findings
    
    Metrics:
    - 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°)
    - k_frac range: -0.589 to -0.607
    - Confirms Δk ≈ 3.27 mismatch vs C reference (consistent with Phase K3e)
[2025-10-07T09:56:19] tokens used: 67,920

[2025-10-07T09:56:21] thinking

**Verifying modification compliance**
[2025-10-07T09:56:22] exec bash -lc "git show f08d657:src/nanobrag_torch/simulator.py | sed -n '1380,1500p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:56:22] bash -lc "git show f08d657:src/nanobrag_torch/simulator.py | sed -n '1380,1500p'" succeeded in 712ms:
                    print(f"TRACE_PY: fluence_photons_per_m2 {self.fluence.item():.15g}")

                    # Use actual steps value passed from run() method
                    # This is the full calculation: sources × mosaic × φ × oversample²
                    if steps is not None:
                        print(f"TRACE_PY: steps {steps}")
                    else:
                        # Fallback to phi_steps only if not provided (backward compatibility)
                        print(f"TRACE_PY: steps {self.crystal.config.phi_steps}")

                    # Oversample flags
                    print(f"TRACE_PY: oversample_thick {1 if self.detector.config.oversample_thick else 0}")
                    print(f"TRACE_PY: oversample_polar {1 if self.detector.config.oversample_polar else 0}")
                    print(f"TRACE_PY: oversample_omega {1 if self.detector.config.oversample_omega else 0}")

                    # Capture fraction from detector absorption (actual value, not placeholder)
                    if capture_fraction is not None and isinstance(capture_fraction, torch.Tensor):
                        capture_frac_val = capture_fraction[target_slow, target_fast].item()
                    else:
                        # If absorption is not configured, capture fraction is 1.0
[2025-10-07T09:56:22] tokens used: 69,623

[2025-10-07T09:56:38] thinking

**Assessing instrumentation impact and plan status**

I'm examining how repeated imports and tensor creations inside loops affect instrumentation and runtime overhead, ensuring tracing only happens when enabled. I also need to verify if existing development plans remain accurate given recent progress and instrumentation additions, checking for necessary updates or multi-turn planning steps.
[2025-10-07T09:56:40] exec bash -lc 'rg "Phase L" -n plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:56:40] bash -lc 'rg "Phase L" -n plans/active/cli-noise-pix0/plan.md' succeeded in 817ms:
10:- Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
11:- Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
12:- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
13:- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
15:- Gap snapshot refresh (2025-11-12): Attempt #66 (commit 1d2e6ac) completed Phase L2b0 — the trace harness now feeds the full supervisor flag set, and the refreshed `trace_py_scaling.log` confirms `steps=10`, `fluence≈1.0e24`, and `r_e^2` parity with C. TRACE_PY still reports `I_before_scaling NOT_EXTRACTED`, `polar=0`, and `I_pixel_final=0`, so Phase L2b instrumentation must surface the real polarization/absorption tensors before we can quantify the K3e k_frac drift.
18:- Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code.
19:- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
20:- Gap snapshot update (2025-11-17 late): Attempt #78 completed the Phase L3c harness rerun; TRACE_PY now reports F_cell=190.27 with matching metadata, so the immediate focus moves to auditing CLI ingestion (L3c) and adding regression coverage (L3d).
21:- Gap snapshot refresh (2025-11-18): Attempt #80 (evidence loop) closed Phase L3c — `cli_hkl_audit.md` now documents the missing `.to(device=device)` transfer at `__main__.py:1073`, confirmed probe artifacts, and queued the device fix + regression test for L3d.
22:- Gap snapshot update (2025-11-18 late): Attempt #81 implemented `.to(device=device, dtype=dtype)` and added the `TestHKLDevice::test_hkl_tensor_respects_device` regression (CPU/CUDA × float32/float64). Next blocker: Phase L3e scaling validation to prove all factors match C (≤1e-6) before rerunning nb-compare in Phase L4.
23:- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
24:- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
29:- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
195:Exit Criteria: Code changes merged with targeted tests covering the corrected scaling, and docs/fix_plan.md records Attempt #29 metrics before moving to Phase L.
230:### Phase L — Supervisor Command Normalization
240:| L1d | Re-run parity script after fixes and update artefacts | [D] | ✅ Attempt #54 (2025-10-17): Regenerated C cache using supervisor command flags, achieved perfect HKL parity (max |ΔF|=0.0, 0 mismatches). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/{Fdump_c_20251109.bin, summary_20251109.md, metrics_20251109.json}`. Phase L2 prerequisites now met—structure factors match C exactly. |
242:### Phase L2 — Scaling Chain Audit
244:Prereqs: Phase L1d artifacts in place; Ralph working under `prompts/debug.md`; C binary instrumentable via `golden_suite_generator/` per CLAUDE Rule #0.3.
253:### Phase L3 — Normalization Fix Implementation
267:### Phase L4 — Supervisor Command Parity Rerun
[2025-10-07T09:56:40] tokens used: 73,041

[2025-10-07T09:57:05] thinking

**Evaluating plan update necessity**
[2025-10-07T09:57:05] exec bash -lc "sed -n '1,160p' specs/spec-a-cli.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:57:06] bash -lc "sed -n '1,160p' specs/spec-a-cli.md" succeeded in 968ms:
# nanoBragg Spec A — CLI Binding

This shard documents the command-line interface, header precedence, and CLI-focused acceptance tests. It should be read alongside [spec-a-core.md](spec-a-core.md) for the simulation engine requirements and [spec-a-parallel.md](spec-a-parallel.md) for conformance profiles.

CLI Interface (Normative)

All options recognized by this code version are enumerated below. Values with units are converted
immediately as specified in Units & Conversions. Unsupported flags are not recognized even if
present in header comments.

- Input files:
    - -hkl <file>: Text file of “h k l F” (integers h,k,l; real F). Required unless Fdump.bin exists
or -default_F used.
    - -mat <file>: 3×3 MOSFLM-style A matrix (reciprocal cell vectors scaled by 1/λ). Required
unless -cell used.
    - -cell a b c alpha beta gamma: Direct cell in Å and degrees; alternative to -mat.
    - -img <file>: Read SMV header; may set fpixels, spixels, pixel size (mm), distance (mm), close
distance (mm), wavelength (Å), beam centers (mm), ORGX/ORGY (pixels), φ start (deg), oscillation
range (deg), 2θ (deg). Image data is read but not used.
    - -mask <file>: Read SMV mask; uses same headers to initialize geometry as -img. Zero-valued
[2025-10-07T09:57:06] tokens used: 76,273

[2025-10-07T09:57:10] thinking

**Reviewing and refining plan structure**
[2025-10-07T09:57:11] exec bash -lc "sed -n '230,340p' plans/active/cli-noise-pix0/plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:57:11] bash -lc "sed -n '230,340p' plans/active/cli-noise-pix0/plan.md" succeeded in 716ms:
### Phase L — Supervisor Command Normalization
Goal: Isolate and resolve the 1.26×10⁵ intensity mismatch that persists when running the full supervisor command after Phase K fixes.
Prereqs: Phase K3g artifacts refreshed (Attempt #49), nb-compare data in `phase_k/f_latt_fix/post_fix/`, instrumentation from Phase J available.
Exit Criteria: Structure-factor grids, scaling chain, and final intensities match C within thresholds (|Δh|,|Δk|,|Δl| < 5e-4; sum_ratio 0.99–1.01; mean_peak_distance ≤ 1 px); nb-compare rerun archived under `phase_l/supervisor_command/`; docs/fix_plan Attempt log updated; plan ready for archival.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| L1a | Run HKL ↔ Fdump parity script and capture metrics | [D] | Attempt #50 (2025-10-06) produced `summary_20251006175032.md` / `metrics_20251006175032.json`; this loop repeated the run with `Fdump_scaled_20251006175946.bin`, confirming max |ΔF|=5.22e2, 99,209 mismatched voxels, and storing `c_fdump_20251006175946.log`. Artefacts live under `reports/2025-10-cli-flags/phase_l/hkl_parity/`. |
| L1b | Diagnose binary-layout mismatch between HKL and Fdump | [D] | ✅ Attempt #52 (2025-10-06) produced `scripts/validation/analyze_fdump_layout.py` plus `reports/2025-10-cli-flags/phase_l/hkl_parity/layout_analysis.md`. Confirmed nanoBragg writes `(h_range+2)×(k_range+2)×(l_range+2)` voxels via `for(h0=0; h0<=h_range; h0++)` loops (see `nanoBragg.c:2400-2486`), leaving 9,534 zero-padded entries. PyTorch reader must drop the padded plane per axis. |
| L1c | Align PyTorch HKL cache reader/writer with confirmed layout | [D] | ✅ Attempt #53 (2025-10-17): Updated `src/nanobrag_torch/io/hkl.py` to honour `(range+1)` padding, added C-code docstring quotes per Rule #11, and introduced `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` (max |ΔF| = 0.0, mismatches = 0). |
| L1d | Re-run parity script after fixes and update artefacts | [D] | ✅ Attempt #54 (2025-10-17): Regenerated C cache using supervisor command flags, achieved perfect HKL parity (max |ΔF|=0.0, 0 mismatches). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/{Fdump_c_20251109.bin, summary_20251109.md, metrics_20251109.json}`. Phase L2 prerequisites now met—structure factors match C exactly. |

### Phase L2 — Scaling Chain Audit
Goal: Capture a side-by-side breakdown of the intensity scaling pipeline (I_before_scaling → ω → polarization → r_e² → fluence → steps) for the supervisor command to isolate the first divergence that drives the 1.26×10⁵ sum ratio.
Prereqs: Phase L1d artifacts in place; Ralph working under `prompts/debug.md`; C binary instrumentable via `golden_suite_generator/` per CLAUDE Rule #0.3.
Exit Criteria: C and PyTorch traces capturing scaling factors are stored under `reports/2025-10-cli-flags/phase_l/scaling_audit/`, a comparison summary identifies the first mismatched term with quantitative deltas, and docs/fix_plan.md Attempt log references the findings.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
[2025-10-07T09:57:11] tokens used: 79,523

[2025-10-07T09:57:24] thinking

**Revising plan to reflect partial completion and next steps**

I'm updating the plan to mark L3e as instrumented but not fully complete due to a persistent F_latt sign mismatch. I’ll add new tasks focused on analyzing this sign difference and revise next-step instructions accordingly, potentially referencing relevant C code to support the updated plan.
[2025-10-07T09:57:26] exec bash -lc 'rg "F_latt" -n golden_suite_generator/nanoBragg.c | head' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:57:26] bash -lc 'rg "F_latt" -n golden_suite_generator/nanoBragg.c | head' succeeded in 843ms:
331:    double F_latt,F_cell;
2805:        F_cell,F_latt,polar,omega_pixel,\
3131:                                        F_latt = sin(M_PI*Na*h)*sin(M_PI*Nb*k)*sin(M_PI*Nc*l)/sin(M_PI*h)/sin(M_PI*k)/sin(M_PI*l);
3133:                                    F_latt = 1.0;
3137:                                        double F_latt_a = 1.0, F_latt_b = 1.0, F_latt_c = 1.0;
3139:                                            F_latt_a = sincg(M_PI*h,Na);
3140:                                            F_latt *= F_latt_a;
3143:                                            F_latt_b = sincg(M_PI*k,Nb);
3144:                                            F_latt *= F_latt_b;
3147:                                            F_latt_c = sincg(M_PI*l,Nc);
[2025-10-07T09:57:26] tokens used: 82,707

[2025-10-07T09:57:29] thinking

**Preparing to review large file**
[2025-10-07T09:57:30] exec bash -lc "sed -n '3120,3160p' golden_suite_generator/nanoBragg.c" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:57:31] bash -lc "sed -n '3120,3160p' golden_suite_generator/nanoBragg.c" succeeded in 748ms:
                                        printf("TRACE_C: rot_b_angstroms %.15g %.15g %.15g\n", b[1]*1e10, b[2]*1e10, b[3]*1e10);
                                        printf("TRACE_C: rot_c_angstroms %.15g %.15g %.15g\n", c[1]*1e10, c[2]*1e10, c[3]*1e10);
                                        printf("TRACE_C: rot_a_star_A_inv %.15g %.15g %.15g\n", a_star[1], a_star[2], a_star[3]);
                                        printf("TRACE_C: rot_b_star_A_inv %.15g %.15g %.15g\n", b_star[1], b_star[2], b_star[3]);
                                        printf("TRACE_C: rot_c_star_A_inv %.15g %.15g %.15g\n", c_star[1], c_star[2], c_star[3]);
                                        printf("TRACE_C: hkl_frac %.15g %.15g %.15g\n", h, k, l);
                                        printf("TRACE_C: hkl_rounded %d %d %d\n", h0, k0, l0);
                                    }


                                    /* structure factor of the lattice (paralelpiped crystal)
                                        F_latt = sin(M_PI*Na*h)*sin(M_PI*Nb*k)*sin(M_PI*Nc*l)/sin(M_PI*h)/sin(M_PI*k)/sin(M_PI*l);
                                    */
                                    F_latt = 1.0;
                                    if(xtal_shape == SQUARE)
                                    {
                                        /* xtal is a paralelpiped */
                                        double F_latt_a = 1.0, F_latt_b = 1.0, F_latt_c = 1.0;
                                        if(Na>1){
                                            F_latt_a = sincg(M_PI*h,Na);
[2025-10-07T09:57:31] tokens used: 112,596

[2025-10-07T09:57:35] thinking

**Investigating sincg function details**
[2025-10-07T09:57:36] exec bash -lc 'rg "def sincg" -n src/nanobrag_torch' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:57:36] bash -lc 'rg "def sincg" -n src/nanobrag_torch' succeeded in 736ms:
src/nanobrag_torch/utils/physics.py:33:def sincg(u: torch.Tensor, N: torch.Tensor) -> torch.Tensor:
[2025-10-07T09:57:36] tokens used: 114,027
[2025-10-07T09:57:39] exec bash -lc "sed -n '1,120p' src/nanobrag_torch/utils/physics.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:57:40] bash -lc "sed -n '1,120p' src/nanobrag_torch/utils/physics.py" succeeded in 739ms:
"""
Vectorized physics utilities for nanoBragg PyTorch implementation.

This module contains PyTorch implementations of physical models and
calculations from the original C code.
"""

import torch


# Use different compile modes for CPU vs GPU to avoid CUDA graph issues
# On GPU, "max-autotune" avoids the nested compilation CUDA graph problem
# On CPU, "reduce-overhead" is faster
def _get_compile_decorator():
    """Get appropriate compile decorator based on available hardware."""
    import os
    disable_compile = os.environ.get("NANOBRAGG_DISABLE_COMPILE", "0") == "1"

    if disable_compile:
        # Return identity decorator when compilation is disabled (for gradient testing)
[2025-10-07T09:57:40] tokens used: 114,313

[2025-10-07T09:57:58] thinking

**Examining potential sign calculation issues**
[2025-10-07T09:58:00] exec bash -lc "python - <<'PY'
import math
k=-0.606 # approx maybe? We'll compute actual soon but guess
N=47
u=math.pi*k
num=math.sin(N*nu)
den=math.sin(u:=nu)
print(num/den)

# compute using formula described N*(-1)"'^(n*(N-1)) with n=round(k)
k_over_pi=u/math.pi
nearest=round(k_over_pi)
print('"'nearest',nearest)
print('factor',N*((-1)**(nearest*(N-1))))
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:01] bash -lc "python - <<'PY'
import math
k=-0.606 # approx maybe? We'll compute actual soon but guess
N=47
u=math.pi*k
num=math.sin(N*nu)
den=math.sin(u:=nu)
print(num/den)

# compute using formula described N*(-1)"'^(n*(N-1)) with n=round(k)
k_over_pi=u/math.pi
nearest=round(k_over_pi)
print('"'nearest',nearest)
print('factor',N*((-1)**(nearest*(N-1))))
PY" exited 1 in 969ms:
Traceback (most recent call last):
  File "<stdin>", line 5, in <module>
NameError: name 'nu' is not defined. Did you mean: 'u'?
[2025-10-07T09:58:01] tokens used: 116,983

[2025-10-07T09:58:03] thinking

**Retrying after error**
[2025-10-07T09:58:05] exec bash -lc "python - <<'PY'
import math
k=-0.606
guess = math.pi*k
num = math.sin(47*guess)
den = math.sin(guess)
print('ratio', num/den)
nearest = round(k)
print('nearest', nearest)
print('N formula', 47*((-1)**(nearest*(47-1))))
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:06] bash -lc "python - <<'PY'
import math
k=-0.606
guess = math.pi*k
num = math.sin(47*guess)
den = math.sin(guess)
print('ratio', num/den)
nearest = round(k)
print('nearest', nearest)
print('N formula', 47*((-1)**(nearest*(47-1))))
PY" succeeded in 761ms:
ratio 1.056439063518614
nearest -1
N formula 47.0
[2025-10-07T09:58:06] tokens used: 117,416

[2025-10-07T09:58:11] thinking

**Verifying numerical values and data traces**
[2025-10-07T09:58:11] exec bash -lc 'rg "TRACE_C: F_latt" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:12] bash -lc 'rg "TRACE_C: F_latt" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 749ms:
273:TRACE_C: F_latt_a -2.36012735995387
274:TRACE_C: F_latt_b 1.05079664302326
275:TRACE_C: F_latt_c 0.960961003611676
276:TRACE_C: F_latt -2.38319665299058
[2025-10-07T09:58:12] tokens used: 117,849

[2025-10-07T09:58:18] thinking

**Analyzing lattice parameter discrepancies**
[2025-10-07T09:58:20] exec bash -lc 'rg "TRACE_PY: F_latt" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:21] bash -lc 'rg "TRACE_PY: F_latt" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' succeeded in 778ms:
22:TRACE_PY: F_latt_a -2.40480732917786
23:TRACE_PY: F_latt_b -0.861241281032562
24:TRACE_PY: F_latt_c 0.652478158473969
25:TRACE_PY: F_latt 1.35136013608642
[2025-10-07T09:58:21] tokens used: 122,908

[2025-10-07T09:58:28] thinking

**Analyzing F_latt_b sign discrepancy**
[2025-10-07T09:58:29] exec bash -lc 'rg "sincg" -n src/nanobrag_torch' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:29] bash -lc 'rg "sincg" -n src/nanobrag_torch' succeeded in 709ms:
src/nanobrag_torch/config.py:90:    SQUARE = "square"  # Parallelepiped/grating using sincg function
src/nanobrag_torch/simulator.py:16:from .utils.physics import sincg, sinc3, polarization_factor
src/nanobrag_torch/simulator.py:224:    #         F_latt_a = sincg(M_PI*h,Na);
src/nanobrag_torch/simulator.py:228:    #         F_latt_b = sincg(M_PI*k,Nb);
src/nanobrag_torch/simulator.py:232:    #         F_latt_c = sincg(M_PI*l,Nc);
src/nanobrag_torch/simulator.py:239:    # "SQUARE (grating): F_latt = sincg(π·h, Na) · sincg(π·k, Nb) · sincg(π·l, Nc)"
src/nanobrag_torch/simulator.py:249:        # Note: sincg internally handles Na/Nb/Nc > 1 guards per C implementation
src/nanobrag_torch/simulator.py:250:        F_latt_a = sincg(torch.pi * h, Na)
src/nanobrag_torch/simulator.py:251:        F_latt_b = sincg(torch.pi * k, Nb)
src/nanobrag_torch/simulator.py:252:        F_latt_c = sincg(torch.pi * l, Nc)
src/nanobrag_torch/simulator.py:1348:                    from nanobrag_torch.utils import sincg
src/nanobrag_torch/simulator.py:1349:                    F_latt_a = sincg(torch.pi * torch.tensor(h), Na).item()
src/nanobrag_torch/simulator.py:1350:                    F_latt_b = sincg(torch.pi * torch.tensor(k), Nb).item()
src/nanobrag_torch/simulator.py:1351:                    F_latt_c = sincg(torch.pi * torch.tensor(l), Nc).item()
src/nanobrag_torch/simulator.py:1453:                            from nanobrag_torch.utils import sincg
src/nanobrag_torch/simulator.py:1454:                            F_latt_a_phi = sincg(torch.pi * torch.tensor(h_phi), Na).item()
src/nanobrag_torch/simulator.py:1455:                            F_latt_b_phi = sincg(torch.pi * torch.tensor(k_phi), Nb).item()
src/nanobrag_torch/simulator.py:1456:                            F_latt_c_phi = sincg(torch.pi * torch.tensor(l_phi), Nc).item()
src/nanobrag_torch/utils/physics.py:33:def sincg(u: torch.Tensor, N: torch.Tensor) -> torch.Tensor:
src/nanobrag_torch/utils/physics.py:120:    It provides an alternative to the `sincg` function for modeling the
[2025-10-07T09:58:29] tokens used: 124,437

[2025-10-07T09:58:32] thinking

**Examining code snippet around line 249**
[2025-10-07T09:58:33] exec bash -lc "sed -n '220,270p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:33] bash -lc "sed -n '220,270p' src/nanobrag_torch/simulator.py" succeeded in 704ms:
    # {
    #     /* xtal is a paralelpiped */
    #     double F_latt_a = 1.0, F_latt_b = 1.0, F_latt_c = 1.0;
    #     if(Na>1){
    #         F_latt_a = sincg(M_PI*h,Na);
    #         F_latt *= F_latt_a;
    #     }
    #     if(Nb>1){
    #         F_latt_b = sincg(M_PI*k,Nb);
    #         F_latt *= F_latt_b;
    #     }
    #     if(Nc>1){
    #         F_latt_c = sincg(M_PI*l,Nc);
    #         F_latt *= F_latt_c;
    #     }
    # }
    # ```
    #
    # Per specs/spec-a-core.md §4.3:
    # "SQUARE (grating): F_latt = sincg(π·h, Na) · sincg(π·k, Nb) · sincg(π·l, Nc)"
[2025-10-07T09:58:33] tokens used: 125,354

[2025-10-07T09:58:36] thinking

**Verifying variable definitions**
[2025-10-07T09:58:37] exec bash -lc "sed -n '150,240p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:38] bash -lc "sed -n '150,240p' src/nanobrag_torch/simulator.py" succeeded in 732ms:
                wavelength = wavelength.view(n_sources, 1, 1)
            else:
                # (S, F, 3) case: (n_sources,) -> (n_sources, 1, 1, 1)
                wavelength = wavelength.view(n_sources, 1, 1, 1)

    # Scattering vector using crystallographic convention
    scattering_vector = (diffracted_beam_unit - incident_beam_unit) / wavelength

    # Apply dmin culling if specified
    dmin_mask = None
    if dmin > 0:
        stol = 0.5 * torch.norm(scattering_vector, dim=-1)
        stol_threshold = 0.5 / dmin
        dmin_mask = (stol > 0) & (stol > stol_threshold)

    # Calculate Miller indices
    # scattering_vector shape: (S, F, 3) single source, or
    #                          (n_sources, S, F, 3) or (n_sources, batch, 3) multi-source
    # Need to add phi and mosaic dimensions for broadcasting
    if is_multi_source:
[2025-10-07T09:58:38] tokens used: 126,318

[2025-10-07T09:58:40] thinking

**Searching for N_cells_a definition**
[2025-10-07T09:58:40] exec bash -lc 'rg "N_cells_a" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:41] bash -lc 'rg "N_cells_a" -n src/nanobrag_torch/simulator.py' succeeded in 716ms:
37:    N_cells_a: int = 0,
87:        N_cells_a/b/c: Number of unit cells in each direction
242:    Na = N_cells_a
667:            N_cells_a=self.crystal.N_cells_a,
1344:                    Na = self.crystal.N_cells_a
[2025-10-07T09:58:41] tokens used: 127,792

[2025-10-07T09:58:45] thinking

**Analyzing data types in crystal model**
[2025-10-07T09:58:45] exec bash -lc 'rg "N_cells_a" -n src/nanobrag_torch/models/crystal.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:46] bash -lc 'rg "N_cells_a" -n src/nanobrag_torch/models/crystal.py' succeeded in 757ms:
92:        self.N_cells_a = torch.as_tensor(
115:            self.N_cells_a.item(),
135:        self.N_cells_a = self.N_cells_a.to(device=self.device, dtype=self.dtype)
[2025-10-07T09:58:46] tokens used: 128,159

[2025-10-07T09:58:48] thinking

**Reviewing file snippet**
[2025-10-07T09:58:48] exec bash -lc "sed -n '80,160p' src/nanobrag_torch/models/crystal.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:50] bash -lc "sed -n '80,160p' src/nanobrag_torch/models/crystal.py" succeeded in 1.11s:
        )
        self.cell_beta = torch.as_tensor(
            self.config.cell_beta, device=self.device, dtype=self.dtype
        )
        self.cell_gamma = torch.as_tensor(
            self.config.cell_gamma, device=self.device, dtype=self.dtype
        )

        # Validate cell parameters for numerical stability
        self._validate_cell_parameters()

        # Crystal size from config
        self.N_cells_a = torch.as_tensor(
            self.config.N_cells[0], device=self.device, dtype=self.dtype
        )
        self.N_cells_b = torch.as_tensor(
            self.config.N_cells[1], device=self.device, dtype=self.dtype
        )
        self.N_cells_c = torch.as_tensor(
            self.config.N_cells[2], device=self.device, dtype=self.dtype
[2025-10-07T09:58:50] tokens used: 128,421

[2025-10-07T09:58:57] thinking

**Analyzing discrepancies in lattice factor components**
[2025-10-07T09:58:58] exec bash -lc 'rg "TRACE_PY: hkl_frac" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:58:59] bash -lc 'rg "TRACE_PY: hkl_frac" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' succeeded in 777ms:
20:TRACE_PY: hkl_frac -6.87916421890259 -0.589141607284546 -13.763635635376
[2025-10-07T09:58:59] tokens used: 129,867

[2025-10-07T09:59:03] thinking

**Calculating trigonometric ratio**
[2025-10-07T09:59:03] exec bash -lc "python - <<'PY'
import math
k=-0.589141607284546
Nb=47
u=math.pi*k
ratio=math.sin(Nb*nu)/math.sin(u:=nu)
print('ratio',ratio)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:04] bash -lc "python - <<'PY'
import math
k=-0.589141607284546
Nb=47
u=math.pi*k
ratio=math.sin(Nb*nu)/math.sin(u:=nu)
print('ratio',ratio)
PY" exited 1 in 760ms:
Traceback (most recent call last):
  File "<stdin>", line 5, in <module>
NameError: name 'nu' is not defined. Did you mean: 'u'?
[2025-10-07T09:59:04] tokens used: 130,255

[2025-10-07T09:59:06] thinking

**Correcting variable misuse**
[2025-10-07T09:59:08] exec bash -lc "python - <<'PY'
import math
k=-0.589141607284546
Nb=47
u=math.pi*k
ratio=math.sin(Nb*nu)/math.sin(u:=nu)
print('ratio',ratio)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:08] bash -lc "python - <<'PY'
import math
k=-0.589141607284546
Nb=47
u=math.pi*k
ratio=math.sin(Nb*nu)/math.sin(u:=nu)
print('ratio',ratio)
PY" exited 1 in 724ms:
Traceback (most recent call last):
  File "<stdin>", line 5, in <module>
NameError: name 'nu' is not defined. Did you mean: 'u'?
[2025-10-07T09:59:08] tokens used: 130,555

[2025-10-07T09:59:11] thinking

**Correcting variable assignment mistake**
[2025-10-07T09:59:12] exec bash -lc "python - <<'PY'
import math
k=-0.589141607284546
Nb=47
u=math.pi*k
ratio=math.sin(Nb*nu)/math.sin(nu)
print('ratio',ratio)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:13] bash -lc "python - <<'PY'
import math
k=-0.589141607284546
Nb=47
u=math.pi*k
ratio=math.sin(Nb*nu)/math.sin(nu)
print('ratio',ratio)
PY" exited 1 in 697ms:
Traceback (most recent call last):
  File "<stdin>", line 5, in <module>
NameError: name 'nu' is not defined. Did you mean: 'u'?
[2025-10-07T09:59:13] tokens used: 130,900

[2025-10-07T09:59:19] thinking

**Troubleshooting undefined variable error**
[2025-10-07T09:59:20] exec bash -lc "python - <<'PY'
nu=1
import math
print(math.sin(nu))
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:20] bash -lc "python - <<'PY'
nu=1
import math
print(math.sin(nu))
PY" succeeded in 717ms:
0.8414709848078965
[2025-10-07T09:59:20] tokens used: 131,410

[2025-10-07T09:59:27] thinking

**Debugging undefined variable error**
[2025-10-07T09:59:27] exec bash -lc "python - <<'PY'
import math
k = -0.589141607284546
Nb = 47
nu = math.pi * k
ratio = math.sin(Nb * nu) / math.sin(nu)
print('ratio', ratio)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:28] bash -lc "python - <<'PY'
import math
k = -0.589141607284546
Nb = 47
nu = math.pi * k
ratio = math.sin(Nb * nu) / math.sin(nu)
print('ratio', ratio)
PY" succeeded in 745ms:
ratio -0.8612400590365636
[2025-10-07T09:59:28] tokens used: 139,129

[2025-10-07T09:59:32] thinking

**Investigating discrepancy in ratio values**
[2025-10-07T09:59:33] exec bash -lc 'rg "TRACE_C: hkl_frac" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:33] bash -lc 'rg "TRACE_C: hkl_frac" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 733ms:
271:TRACE_C: hkl_frac -6.8795206024501 -0.607255839576692 -13.766298525469
[2025-10-07T09:59:33] tokens used: 139,845

[2025-10-07T09:59:38] thinking

**Comparing constants across libraries**
[2025-10-07T09:59:39] exec bash -lc "python - <<'PY'
import math
k = -0.607255839576692
Nb = 47
nu = math.pi * k
ratio = math.sin(Nb * nu) / math.sin(nu)
print('ratio', ratio)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:40] bash -lc "python - <<'PY'
import math
k = -0.607255839576692
Nb = 47
nu = math.pi * k
ratio = math.sin(Nb * nu) / math.sin(nu)
print('ratio', ratio)
PY" succeeded in 749ms:
ratio 1.0507966430232665
[2025-10-07T09:59:40] tokens used: 140,934

[2025-10-07T09:59:44] thinking

**Analyzing scattering vector differences**
[2025-10-07T09:59:45] exec bash -lc 'rg "rot_b_angstroms" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:46] bash -lc 'rg "rot_b_angstroms" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' succeeded in 709ms:
15:TRACE_PY: rot_b_angstroms -11.4986982345581 0.717319786548615 -29.1132164001465
[2025-10-07T09:59:46] tokens used: 141,436

[2025-10-07T09:59:48] thinking

**Comparing to C**
[2025-10-07T09:59:49] exec bash -lc 'rg "TRACE_C: rot_b_angstroms" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:49] bash -lc 'rg "TRACE_C: rot_b_angstroms" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 880ms:
266:TRACE_C: rot_b_angstroms -11.4986968432508 0.671588233999813 -29.1143056268565
[2025-10-07T09:59:49] tokens used: 141,731

[2025-10-07T09:59:56] thinking

**Analyzing orientation mismatch causes**
[2025-10-07T09:59:57] exec bash -lc 'head -n 20 reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T09:59:58] bash -lc 'head -n 20 reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log' succeeded in 800ms:
TRACE_PY_PHI phi_tic=0 phi_deg=0 k_frac=-0.589141607284546 F_latt_b=-0.861241281032562 F_latt=1.35136013608642
TRACE_PY_PHI phi_tic=1 phi_deg=0.0111111111111111 k_frac=-0.591151714324951 F_latt_b=-0.654002606868744 F_latt=1.07852524234798
TRACE_PY_PHI phi_tic=2 phi_deg=0.0222222222222222 k_frac=-0.593161582946777 F_latt_b=-0.388755053281784 F_latt=0.671564671377393
TRACE_PY_PHI phi_tic=3 phi_deg=0.0333333333333333 k_frac=-0.595171689987183 F_latt_b=-0.0884189531207085 F_latt=0.159541734546532
TRACE_PY_PHI phi_tic=4 phi_deg=0.0444444444444444 k_frac=-0.597181558609009 F_latt_b=0.220785543322563 F_latt=-0.415164805287365
TRACE_PY_PHI phi_tic=5 phi_deg=0.0555555555555556 k_frac=-0.599191427230835 F_latt_b=0.511898458003998 F_latt=-1.00054741889294
TRACE_PY_PHI phi_tic=6 phi_deg=0.0666666666666667 k_frac=-0.601201057434082 F_latt_b=0.759344637393951 F_latt=-1.53945502474902
TRACE_PY_PHI phi_tic=7 phi_deg=0.0777777777777778 k_frac=-0.60321044921875 F_latt_b=0.941327095031738 F_latt=-1.97558293388473
TRACE_PY_PHI phi_tic=8 phi_deg=0.0888888888888889 k_frac=-0.605220317840576 F_latt_b=1.04169797897339 F_latt=-2.25921634449701
TRACE_PY_PHI phi_tic=9 phi_deg=0.1 k_frac=-0.607229709625244 F_latt_b=1.05128383636475 F_latt=-2.35148472430236
[2025-10-07T09:59:58] tokens used: 142,230

[2025-10-07T10:00:16] thinking

**Analyzing phi indexing discrepancy**

I’m examining differences between pyro per-φ detailed logs and the aggregated pyro trace summaries; it looks like the summary uses values from the first phi step while final intensity calculations rely on averaging over all phi steps. This mismatch suggests the aggregation or logging in pyro doesn’t capture the same phi index consistently, so I’ll need to verify data shapes and indexing in the code to clarify and update the plan accordingly.
[2025-10-07T10:00:18] exec bash -lc 'rg "TRACE_PY: I_before_scaling" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:00:19] bash -lc 'rg "TRACE_PY: I_before_scaling" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log' succeeded in 971ms:
27:TRACE_PY: I_before_scaling 713431
[2025-10-07T10:00:19] tokens used: 144,591

[2025-10-07T10:00:28] thinking

**Planning document review and task updates**

I'll organize relevant docs to support the next step focused on CLI-FLAGS scaling and update the plan to highlight tasks around phi step alignment and F_latt discrepancies, making sure to reference traces and log confirmations needed for consistent aggregation.
[2025-10-07T10:00:30] exec bash -lc 'rg "TRACE_C_PHI" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:00:30] bash -lc 'rg "TRACE_C_PHI" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 736ms:
277:TRACE_C_PHI phi_tic=0 phi_deg=0 k_frac=-0.607255839576692 F_latt_b=1.05079664302326 F_latt=-2.38319665299058
279:TRACE_C_PHI phi_tic=1 phi_deg=0.01 k_frac=-0.591177727127411 F_latt_b=-0.650897255741202 F_latt=1.09860741704307
280:TRACE_C_PHI phi_tic=2 phi_deg=0.02 k_frac=-0.593187745641004 F_latt_b=-0.385020463934971 F_latt=0.679724660042308
281:TRACE_C_PHI phi_tic=3 phi_deg=0.03 k_frac=-0.595197691574922 F_latt_b=-0.0844254727339029 F_latt=0.155482072719562
282:TRACE_C_PHI phi_tic=4 phi_deg=0.04 k_frac=-0.597207564867936 F_latt_b=0.224723979612609 F_latt=-0.430675460925897
283:TRACE_C_PHI phi_tic=5 phi_deg=0.05 k_frac=-0.599217365458823 F_latt_b=0.515419354922621 F_latt=-1.02559631818979
284:TRACE_C_PHI phi_tic=6 phi_deg=0.06 k_frac=-0.601227093286361 F_latt_b=0.762161832840823 F_latt=-1.57135079979647
285:TRACE_C_PHI phi_tic=7 phi_deg=0.07 k_frac=-0.60323674828933 F_latt_b=0.943193755044476 F_latt=-2.01093178282
286:TRACE_C_PHI phi_tic=8 phi_deg=0.08 k_frac=-0.605246330406512 F_latt_b=1.04241217237842 F_latt=-2.29416161853174
287:TRACE_C_PHI phi_tic=9 phi_deg=0.09 k_frac=-0.607255839576692 F_latt_b=1.05079664302326 F_latt=-2.38319665299058
[2025-10-07T10:00:30] tokens used: 145,185

[2025-10-07T10:00:40] thinking

**Analyzing phi step differences**
[2025-10-07T10:00:41] exec bash -lc 'rg "phi_step" -n src/nanobrag_torch/models/crystal.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:00:42] bash -lc 'rg "phi_step" -n src/nanobrag_torch/models/crystal.py' succeeded in 762ms:
991:            config.phi_steps, device=self.device, dtype=self.dtype
994:            config.osc_range_deg / config.phi_steps
995:            if config.phi_steps > 0
999:        # where step_index ranges from 0 to (phi_steps - 1)
[2025-10-07T10:00:42] tokens used: 150,060

[2025-10-07T10:00:44] thinking

**Continuing file reading**
[2025-10-07T10:00:45] exec bash -lc "sed -n '960,1030p' src/nanobrag_torch/models/crystal.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:00:45] bash -lc "sed -n '960,1030p' src/nanobrag_torch/models/crystal.py" succeeded in 713ms:
        ```

        Args:
            config: CrystalConfig containing rotation parameters.

        Returns:
            Tuple containing:
            - First tuple: rotated (a, b, c) real-space vectors with shape (N_phi, N_mos, 3)
            - Second tuple: rotated (a*, b*, c*) reciprocal-space vectors with shape (N_phi, N_mos, 3)
        """
        from ..utils.geometry import rotate_axis, rotate_umat

        # Generate phi angles
        # Assume config parameters are tensors (enforced at call site)
        # torch.linspace doesn't preserve gradients, so we handle different cases manually
        #
        # CRITICAL: Match C code loop formula: phi = phi0 + phistep*phi_tic
        # where phistep = osc/phisteps and phi_tic ranges from 0 to (phisteps-1)
        #
        # C code reference (nanoBragg.c lines 3004-3009):
[2025-10-07T10:00:45] tokens used: 151,914

[2025-10-07T10:00:59] thinking

**Analyzing phi step size discrepancy**
[2025-10-07T10:00:59] exec bash -lc 'rg "target_slow" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:01:00] bash -lc 'rg "target_slow" -n src/nanobrag_torch/simulator.py' succeeded in 929ms:
1215:            target_slow = self.printout_pixel[1]
1218:            if 0 <= target_slow < physical_intensity.shape[0] and 0 <= target_fast < physical_intensity.shape[1]:
1219:                print(f"\n=== Pixel ({target_fast}, {target_slow}) [fast, slow] ===")
1220:                print(f"  Final intensity: {physical_intensity[target_slow, target_fast].item():.6e}")
1221:                print(f"  Normalized intensity: {normalized_intensity[target_slow, target_fast].item():.6e}")
1223:                    coords = pixel_coords_meters[target_slow, target_fast]
1229:                    print(f"  Solid angle: {omega_pixel[target_slow, target_fast].item():.6e}")
1231:                    print(f"  Polarization: {polarization[target_slow, target_fast].item():.4f}")
1248:            target_slow = self.trace_pixel[0]
1251:            if 0 <= target_slow < physical_intensity.shape[0] and 0 <= target_fast < physical_intensity.shape[1]:
1264:                    coords = pixel_coords_meters[target_slow, target_fast]
1323:                    coords_ang = pixel_coords_meters[target_slow, target_fast] * 1e10  # m to Å
1370:                        I_before_scaling = I_total[target_slow, target_fast].item()
1397:                        capture_frac_val = capture_fraction[target_slow, target_fast].item()
1406:                        polar_val = polarization[target_slow, target_fast].item()
1420:                    I_pixel_final = physical_intensity[target_slow, target_fast].item()
1465:                        print(f"TRACE: Omega (solid angle) = {omega_pixel[target_slow, target_fast].item():.12e}")
1470:                        print(f"TRACE: Polarization = {polarization[target_slow, target_fast].item():.12e}")
1481:                print(f"  = {normalized_intensity[target_slow, target_fast].item():.12e} * {self.r_e_sqr:.12e} * {self.fluence:.12e}")
1482:                print(f"  = {physical_intensity[target_slow, target_fast].item():.12e}")
[2025-10-07T10:01:00] tokens used: 153,841

[2025-10-07T10:01:03] thinking

**Reviewing code block for F_latt tracing**
[2025-10-07T10:01:05] exec bash -lc "sed -n '1290,1405p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:01:06] bash -lc "sed -n '1290,1405p' src/nanobrag_torch/simulator.py" succeeded in 687ms:
                    print(f"TRACE_PY: incident_vec {incident_vec[0].item():.15g} {incident_vec[1].item():.15g} {incident_vec[2].item():.15g}")

                    # Wavelength
                    lambda_m = self.wavelength.item() * 1e-10
                    print(f"TRACE_PY: lambda_meters {lambda_m:.15g}")
                    print(f"TRACE_PY: lambda_angstroms {self.wavelength.item():.15g}")

                    # Scattering vector (in m^-1, not Å^-1!)
                    scattering_vec_ang_inv = (diffracted_vec - incident_vec) / self.wavelength
                    scattering_vec_m_inv = scattering_vec_ang_inv * 1e10  # Å^-1 to m^-1
                    print(f"TRACE_PY: scattering_vec_A_inv {scattering_vec_m_inv[0].item():.15g} {scattering_vec_m_inv[1].item():.15g} {scattering_vec_m_inv[2].item():.15g}")

                # Trace reciprocal space information if available
                if rot_a_star is not None and rot_b_star is not None and rot_c_star is not None:
                    # Output rotated real and reciprocal vectors (first phi/mosaic domain)
                    a_vec = rot_a[0, 0] if len(rot_a.shape) > 1 else rot_a
                    b_vec = rot_b[0, 0] if len(rot_b.shape) > 1 else rot_b
                    c_vec = rot_c[0, 0] if len(rot_c.shape) > 1 else rot_c

                    print(f"TRACE_PY: rot_a_angstroms {a_vec[0].item():.15g} {a_vec[1].item():.15g} {a_vec[2].item():.15g}")
[2025-10-07T10:01:06] tokens used: 156,909

[2025-10-07T10:01:40] thinking

**Analyzing orientation and indexing discrepancies**
[2025-10-07T10:01:41] exec bash -lc 'rg "phistep" -n golden_suite_generator/nanoBragg.c | head' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:01:42] bash -lc 'rg "phistep" -n golden_suite_generator/nanoBragg.c | head' succeeded in 856ms:
321:    double phi,phi0=0.0,phistep=-1.0,osc=-1.0;
322:    int phi_tic,phisteps=-1;
966:            if(strstr(argv[i], "-phistep") && strlen(argv[i])==8 && (argc > (i+1)))
968:                phistep = atof(argv[i+1])/RTD;
970:            if(strstr(argv[i], "-phisteps") && (argc > (i+1)))
972:                phisteps = atoi(argv[i+1]);
1353:        printf("\t-phisteps        \tnumber of rotation steps to render\n");
1401:    if(phisteps < 0){
1405:            if(phistep <= 0.0) {
1407:                phisteps = 1;
[2025-10-07T10:01:42] tokens used: 161,304

[2025-10-07T10:01:45] thinking

**Examining phi_tic loop section**
[2025-10-07T10:01:47] exec bash -lc "sed -n '3000,3080p' golden_suite_generator/nanoBragg.c" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:01:47] bash -lc "sed -n '3000,3080p' golden_suite_generator/nanoBragg.c" succeeded in 725ms:
                            scattering[2] = (diffracted[2]-incident[2])/lambda;
                            scattering[3] = (diffracted[3]-incident[3])/lambda;
                            
                            /* trace output for specific pixel */
                            if(fpixel==trace_fpixel && spixel==trace_spixel && source==0) {
                                printf("TRACE_C: pix0_vector_meters %.15g %.15g %.15g\n", pix0_vector[1], pix0_vector[2], pix0_vector[3]);
                                printf("TRACE_C: fdet_vec %.15g %.15g %.15g\n", fdet_vector[1], fdet_vector[2], fdet_vector[3]);
                                printf("TRACE_C: sdet_vec %.15g %.15g %.15g\n", sdet_vector[1], sdet_vector[2], sdet_vector[3]);
                                printf("TRACE_C: pixel_pos_meters %.15g %.15g %.15g\n", pixel_pos[1], pixel_pos[2], pixel_pos[3]);
                                printf("TRACE_C: R_distance_meters %.15g\n", airpath);
                                printf("TRACE_C: omega_pixel_sr %.15g\n", omega_pixel);
                                printf("TRACE_C: close_distance_meters %.15g\n", close_distance);
                                printf("TRACE_C: obliquity_factor %.15g\n", close_distance/airpath);
                                printf("TRACE_C: diffracted_vec %.15g %.15g %.15g\n", diffracted[1], diffracted[2], diffracted[3]);
                                printf("TRACE_C: incident_vec %.15g %.15g %.15g\n", incident[1], incident[2], incident[3]);
                                printf("TRACE_C: lambda_meters %.15g\n", lambda);
                                printf("TRACE_C: lambda_angstroms %.15g\n", lambda*1e10);
                                printf("TRACE_C: scattering_vec_A_inv %.15g %.15g %.15g\n", scattering[1], scattering[2], scattering[3]);
                            }

[2025-10-07T10:01:47] tokens used: 161,644

[2025-10-07T10:02:03] thinking

**Planning analysis of rotation differences**

I'm outlining plan updates to investigate discrepancies in initial rotation vectors between C and pyro, focusing on verifying alignment of initial orientations, checking rotation order, and adding new diagnostic tasks to the plan and documentation to track this detailed analysis.
[2025-10-07T10:02:05] exec bash -lc 'rg "Attempt #86" -n docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:02:06] bash -lc 'rg "Attempt #86" -n docs/fix_plan.md' succeeded in 1.22s:
617:  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
[2025-10-07T10:02:06] tokens used: 164,314

[2025-10-07T10:02:30] thinking

**Organizing plan updates and patch strategy**

I'm working out how to comprehensively update the plan to reflect new tasks and findings related to the fractional Miller index alignment, including revising the gap snapshot, marking completed steps, adding new subtasks, and ensuring next actions highlight these changes; I'll prepare a patch to insert these structured updates smoothly into the existing plan file.
[2025-10-07T10:02:41] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/plans/active/cli-noise-pix0/plan.md
@@ -28,3 +28,3 @@
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
@@ -263,4 +263,6 @@
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
[2025-10-07T10:02:41] apply_patch(auto_approved=true) exited 0 in 21ms:
Success. Updated the following files:
M plans/active/cli-noise-pix0/plan.md
[2025-10-07T10:02:41] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:02:41] tokens used: 166,672
[2025-10-07T10:02:41] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:02:47] thinking

**Ensuring consistent table updates**

I’m making sure the table rows and their IDs remain sequential and reflect the latest renaming—like replacing L3f with the rotation-vector audit and introducing L3g and L3h for new tasks. I also want to update the plan’s context and Next Actions list to clearly mention these new phases and tasks, including sedation exit criteria and any doc updates referencing TOT. This way, everything stays aligned and clear.
[2025-10-07T10:02:51] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -459,6 +459,6 @@
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
[2025-10-07T10:02:52] apply_patch(auto_approved=true) exited 0 in 24ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T10:02:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..fefb19a6c88e717cae807007649aea4bb43282a6
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:02:52] tokens used: 168,475
[2025-10-07T10:02:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..fefb19a6c88e717cae807007649aea4bb43282a6
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:02:56] thinking

**Reviewing and updating attempt notes**
[2025-10-07T10:02:56] exec bash -lc "sed -n '600,660p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:02:57] bash -lc "sed -n '600,660p' docs/fix_plan.md" succeeded in 1.05s:
      - **Relative deltas now fractional:** Output format changed from percentage (`+0.003%`) to scientific notation (`+3.094e-08`) for consistency with threshold
      - **Deprecation warning fixed:** Replaced datetime.utcnow() with datetime.now().astimezone() to avoid Python 3.13+ deprecation warning
      - **No production code changes:** Evidence-only loop per input.md Parity mode; script updates are tooling-layer only
    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized ingestion/normalization flow once scaling validation passes (per input.md lines 462-463).
  * [2025-11-19] Attempt #83 (galph loop) — Result: **EVIDENCE UPDATE** (Phase L3e parity snapshot). **Reran scaling comparison with latest PyTorch trace (`trace_py_scaling_20251117.log`) and documented the surviving lattice-factor gap.**
    Metrics: Comparison reports 5 divergent factors with `I_before_scaling` still CRITICAL (−0.24397 relative). `r_e^2`, `fluence`, `steps`, `capture_fraction` remain PASS; `polar`, `omega_pixel`, `cos_2theta` MINOR. `I_pixel_final` mirrors the raw accumulation delta (−0.17343 relative).
    Artifacts:
      - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
      - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
      - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
    Observations/Hypotheses:
      - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
      - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
      - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
    Artifacts:
[2025-10-07T10:02:57] tokens used: 169,449
[2025-10-07T10:02:57] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..fefb19a6c88e717cae807007649aea4bb43282a6
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:03:01] thinking

**Updating observations and next actions**
[2025-10-07T10:03:07] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -627,6 +627,6 @@
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
[2025-10-07T10:03:07] apply_patch(auto_approved=true) exited 0 in 32ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T10:03:07] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:03:07] tokens used: 173,103
[2025-10-07T10:03:07] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:03:35] thinking

**Aligning and detailing new Next Actions**

I'm making sure the new Next Actions steps match earlier overall plans, especially focusing on the creation of the new `reports/.../rot_vector` directory and the input.md instructions for Ralph. I’m carefully structuring the 'Do Now' commands to capture rotation vector traces, run specific pytest commands for targeted tests, and produce detailed analysis artifacts, while also outlining priorities, how-to steps, pitfalls, and relevant documentation references to keep everything clear and consistent.
[2025-10-07T10:03:37] exec bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '230,310p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:03:38] bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '230,310p'" succeeded in 969ms:
   230	### Phase L — Supervisor Command Normalization
   231	Goal: Isolate and resolve the 1.26×10⁵ intensity mismatch that persists when running the full supervisor command after Phase K fixes.
   232	Prereqs: Phase K3g artifacts refreshed (Attempt #49), nb-compare data in `phase_k/f_latt_fix/post_fix/`, instrumentation from Phase J available.
   233	Exit Criteria: Structure-factor grids, scaling chain, and final intensities match C within thresholds (|Δh|,|Δk|,|Δl| < 5e-4; sum_ratio 0.99–1.01; mean_peak_distance ≤ 1 px); nb-compare rerun archived under `phase_l/supervisor_command/`; docs/fix_plan Attempt log updated; plan ready for archival.
   234	
   235	| ID | Task Description | State | How/Why & Guidance |
   236	| --- | --- | --- | --- |
   237	| L1a | Run HKL ↔ Fdump parity script and capture metrics | [D] | Attempt #50 (2025-10-06) produced `summary_20251006175032.md` / `metrics_20251006175032.json`; this loop repeated the run with `Fdump_scaled_20251006175946.bin`, confirming max |ΔF|=5.22e2, 99,209 mismatched voxels, and storing `c_fdump_20251006175946.log`. Artefacts live under `reports/2025-10-cli-flags/phase_l/hkl_parity/`. |
   238	| L1b | Diagnose binary-layout mismatch between HKL and Fdump | [D] | ✅ Attempt #52 (2025-10-06) produced `scripts/validation/analyze_fdump_layout.py` plus `reports/2025-10-cli-flags/phase_l/hkl_parity/layout_analysis.md`. Confirmed nanoBragg writes `(h_range+2)×(k_range+2)×(l_range+2)` voxels via `for(h0=0; h0<=h_range; h0++)` loops (see `nanoBragg.c:2400-2486`), leaving 9,534 zero-padded entries. PyTorch reader must drop the padded plane per axis. |
   239	| L1c | Align PyTorch HKL cache reader/writer with confirmed layout | [D] | ✅ Attempt #53 (2025-10-17): Updated `src/nanobrag_torch/io/hkl.py` to honour `(range+1)` padding, added C-code docstring quotes per Rule #11, and introduced `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` (max |ΔF| = 0.0, mismatches = 0). |
   240	| L1d | Re-run parity script after fixes and update artefacts | [D] | ✅ Attempt #54 (2025-10-17): Regenerated C cache using supervisor command flags, achieved perfect HKL parity (max |ΔF|=0.0, 0 mismatches). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/{Fdump_c_20251109.bin, summary_20251109.md, metrics_20251109.json}`. Phase L2 prerequisites now met—structure factors match C exactly. |
   241	
   242	### Phase L2 — Scaling Chain Audit
   243	Goal: Capture a side-by-side breakdown of the intensity scaling pipeline (I_before_scaling → ω → polarization → r_e² → fluence → steps) for the supervisor command to isolate the first divergence that drives the 1.26×10⁵ sum ratio.
   244	Prereqs: Phase L1d artifacts in place; Ralph working under `prompts/debug.md`; C binary instrumentable via `golden_suite_generator/` per CLAUDE Rule #0.3.
   245	Exit Criteria: C and PyTorch traces capturing scaling factors are stored under `reports/2025-10-cli-flags/phase_l/scaling_audit/`, a comparison summary identifies the first mismatched term with quantitative deltas, and docs/fix_plan.md Attempt log references the findings.
   246	
   247	| ID | Task Description | State | How/Why & Guidance |
   248	| --- | --- | --- | --- |
   249	| L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
[2025-10-07T10:03:38] tokens used: 176,299
[2025-10-07T10:03:38] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:03:42] thinking

**Tracking line numbers for plans**
[2025-10-07T10:03:43] exec bash -lc "nl -ba docs/fix_plan.md | sed -n '450,530p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:03:43] bash -lc "nl -ba docs/fix_plan.md | sed -n '450,530p'" succeeded in 747ms:
   450	## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
   451	- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
   452	- Priority: High
   453	- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
   454	- Owner/Date: ralph/2025-10-05
   455	- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
   456	- Reproduction (C & PyTorch):
   457	  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   458	  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
   459	- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
   460	- Next Actions (2025-11-19 refresh):
   461	  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
   462	  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
   463	  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
   464	- Attempts History:
   465	  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
   466	    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
   467	    Artifacts:
   468	      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log:1-41` - Refreshed trace with live values (polar=0.91464138, omega_pixel=4.20404859e-07 sr, F_cell=0, I_before_scaling=0)
   469	      - `reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1-54` - Phase L2c comparison report showing first divergence and all divergent factors
[2025-10-07T10:03:43] tokens used: 179,221
[2025-10-07T10:03:43] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:04:00] thinking

**Preparing input.md with commands and artifacts**

I'm working out how to construct input.md, focusing on including a clear Do Now line with commands for creating directories, running the harness, and running pytest while delegating the doc generation to instructions in How-To Map due to command length limits. I'll also list the expected output artifacts so everything is aligned.
[2025-10-07T10:04:27] exec bash -lc "cat <<'EOF' > input.md
Summary: Capture φ=0 rotation vectors and frame hypotheses for the residual k_frac drift before touching simulator code.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: [tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics]
Artifacts:
- reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector.log
- reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md
- reports/2025-10-cli-flags/phase_l/rot_vector/analysis.md
Do Now: CLI-FLAGS-003 — mkdir -p reports/2025-10-cli-flags/phase_l/rot_vector && PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --config supervisor --device cpu --dtype float32 --out reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector.log && KMP_DUPLICATE_LIB_OK=TRUE pytest --collect-only -q tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
If Blocked: Capture harness stdout/stderr to reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector_raw.txt, note missing TRACE_PY rot lines plus commands in docs/fix_plan.md Attempt history, and halt for supervisor review.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md:264-267 elevates Phase L3f/L3g as the current blockers before touching normalization code.
- docs/fix_plan.md:450-463 lists the rotation-vector audit and hypothesis framing as the top next actions for CLI-FLAGS-003.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log anchors the C-side rot vectors that must be matched.
- reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json shows φ sweep data confirming convergence after the initial step—use it as context when analysing the φ=0 drift.
- specs/spec-a-cli.md §3 and docs/development/c_to_pytorch_config_map.md outline the normative rotation/pivot rules the audit must respect.
How-To Map:
- Ensure env vars: "'`export PYTHONPATH=src` and `export KMP_DUPLICATE_LIB_OK=TRUE` before running tooling.
- Harness capture: `PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --config supervisor --device cpu --dtype float32 --out reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector.log` (creates new main + per-φ traces alongside env metadata).
- Build comparison table: `python - <<'"'PY'
from pathlib import Path
import re
root = Path('reports/2025-10-cli-flags/phase_l')
py_lines = Path(root, 'rot_vector/trace_py_rot_vector.log').read_text().splitlines()
c_lines = Path(root, 'scaling_audit/c_trace_scaling.log').read_text().splitlines()
pattern = re.compile(r'TRACE_(?P<src>C|PY): (?P<key>rot_[abc](?:_star)?_angstroms|rot_[abc]_star_A_inv) (?P<x>-?\\S+) (?P<y>-?\\S+) (?P<z>-?\\S+)')
def extract(lines, prefix):
    out = {}
    for line in lines:
        m = pattern.match(line)
        if m and m.group('src') == prefix:
            out[m.group('key')] = [float(m.group('x')), float(m.group('y')), float(m.group('z'))]
    return out
py = extract(py_lines, 'PY')
c = extract(c_lines, 'C')
rows = [
    ('rot_a_angstroms', 'Å'),
    ('rot_b_angstroms', 'Å'),
    ('rot_c_angstroms', 'Å'),
    ('rot_a_star_A_inv', 'Å"'^{-1}'"'),
    ('rot_b_star_A_inv', 'Å"'^{-1}'"'),
    ('rot_c_star_A_inv', 'Å"'^{-1}'"')
]
lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
for key, units in rows:
    cx, cy, cz = c.get(key, (float('nan'),)*3)
    px, pyv, pz = py.get(key, (float('nan'),)*3)
    dx, dy, dz = (px-cx, pyv-cy, pz-cz)
    lines.append(f\"| {key} | {units} | ({cx:.9f}, {cy:.9f}, {cz:.9f}) | ({px:.9f}, {pyv:.9f}, {pz:.9f}) | ({dx:.3e}, {dy:.3e}, {dz:.3e}) |\")
Path(root, 'rot_vector/rot_vector_comparison.md').write_text('\\n'.join(lines) + '\\n')
PY"'`
- Hypothesis log: Summarize takeaways (δrot components, suspected causes, follow-up probes) in `reports/2025-10-cli-flags/phase_l/rot_vector/analysis.md`.
- Record findings + commands in docs/fix_plan.md Attempts once artifacts exist.
Pitfalls To Avoid:
- Do not edit production simulator code during this evidence pass.
- Keep harness commands on CPU/float32 unless explicitly testing GPU; avoid accidental dtype/device drift.
- Preserve existing trace artifacts—write new outputs under `rot_vector/` rather than overwriting `scaling_audit/` logs.
- Respect Protected Assets rule: never move/delete files listed in docs/index.md.
- Maintain vectorization guardrails—no new Python loops or `.item()` usage in production paths if later edits are required.
- Capture commands verbatim in Attempts history; no paraphrasing.
- Ensure all temporary scripts/snippets are stored under `reports/…/` (not at repo root) and kept ASCII.
- Run `pytest --collect-only` before leaving the loop to confirm the selector remains valid.
- Avoid committing artifacts until supervisor review; keep git status clean apart from expected report/doc updates.
- Follow trace schema from docs/debugging/debugging.md when adding new trace lines.
Pointers:
- plans/active/cli-noise-pix0/plan.md:264-267 — Phase L3f/L3g task definitions and success criteria.
- docs/fix_plan.md:450-463 — CLI-FLAGS-003 next actions and evidence expectations.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C reference rot vectors & lattice factors.
- reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json — PyTorch per-φ reference for context.
Next Up: 1) If rotation vectors align, draft `rot_vector/analysis.md` conclusions and update docs/fix_plan.md; 2) Prepare code-edit plan for the identified rotation discrepancy (Phase L3g follow-up).
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:04:28] bash -lc "cat <<'EOF' > input.md
Summary: Capture φ=0 rotation vectors and frame hypotheses for the residual k_frac drift before touching simulator code.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: [tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics]
Artifacts:
- reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector.log
- reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md
- reports/2025-10-cli-flags/phase_l/rot_vector/analysis.md
Do Now: CLI-FLAGS-003 — mkdir -p reports/2025-10-cli-flags/phase_l/rot_vector && PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --config supervisor --device cpu --dtype float32 --out reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector.log && KMP_DUPLICATE_LIB_OK=TRUE pytest --collect-only -q tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics
If Blocked: Capture harness stdout/stderr to reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector_raw.txt, note missing TRACE_PY rot lines plus commands in docs/fix_plan.md Attempt history, and halt for supervisor review.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md:264-267 elevates Phase L3f/L3g as the current blockers before touching normalization code.
- docs/fix_plan.md:450-463 lists the rotation-vector audit and hypothesis framing as the top next actions for CLI-FLAGS-003.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log anchors the C-side rot vectors that must be matched.
- reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json shows φ sweep data confirming convergence after the initial step—use it as context when analysing the φ=0 drift.
- specs/spec-a-cli.md §3 and docs/development/c_to_pytorch_config_map.md outline the normative rotation/pivot rules the audit must respect.
How-To Map:
- Ensure env vars: "'`export PYTHONPATH=src` and `export KMP_DUPLICATE_LIB_OK=TRUE` before running tooling.
- Harness capture: `PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --pixel 685 1039 --config supervisor --device cpu --dtype float32 --out reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector.log` (creates new main + per-φ traces alongside env metadata).
- Build comparison table: `python - <<'"'PY'
from pathlib import Path
import re
root = Path('reports/2025-10-cli-flags/phase_l')
py_lines = Path(root, 'rot_vector/trace_py_rot_vector.log').read_text().splitlines()
c_lines = Path(root, 'scaling_audit/c_trace_scaling.log').read_text().splitlines()
pattern = re.compile(r'TRACE_(?P<src>C|PY): (?P<key>rot_[abc](?:_star)?_angstroms|rot_[abc]_star_A_inv) (?P<x>-?\\S+) (?P<y>-?\\S+) (?P<z>-?\\S+)')
def extract(lines, prefix):
    out = {}
    for line in lines:
        m = pattern.match(line)
        if m and m.group('src') == prefix:
            out[m.group('key')] = [float(m.group('x')), float(m.group('y')), float(m.group('z'))]
    return out
py = extract(py_lines, 'PY')
c = extract(c_lines, 'C')
rows = [
    ('rot_a_angstroms', 'Å'),
    ('rot_b_angstroms', 'Å'),
    ('rot_c_angstroms', 'Å'),
    ('rot_a_star_A_inv', 'Å"'^{-1}'"'),
    ('rot_b_star_A_inv', 'Å"'^{-1}'"'),
    ('rot_c_star_A_inv', 'Å"'^{-1}'"')
]
lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
for key, units in rows:
    cx, cy, cz = c.get(key, (float('nan'),)*3)
    px, pyv, pz = py.get(key, (float('nan'),)*3)
    dx, dy, dz = (px-cx, pyv-cy, pz-cz)
    lines.append(f\"| {key} | {units} | ({cx:.9f}, {cy:.9f}, {cz:.9f}) | ({px:.9f}, {pyv:.9f}, {pz:.9f}) | ({dx:.3e}, {dy:.3e}, {dz:.3e}) |\")
Path(root, 'rot_vector/rot_vector_comparison.md').write_text('\\n'.join(lines) + '\\n')
PY"'`
- Hypothesis log: Summarize takeaways (δrot components, suspected causes, follow-up probes) in `reports/2025-10-cli-flags/phase_l/rot_vector/analysis.md`.
- Record findings + commands in docs/fix_plan.md Attempts once artifacts exist.
Pitfalls To Avoid:
- Do not edit production simulator code during this evidence pass.
- Keep harness commands on CPU/float32 unless explicitly testing GPU; avoid accidental dtype/device drift.
- Preserve existing trace artifacts—write new outputs under `rot_vector/` rather than overwriting `scaling_audit/` logs.
- Respect Protected Assets rule: never move/delete files listed in docs/index.md.
- Maintain vectorization guardrails—no new Python loops or `.item()` usage in production paths if later edits are required.
- Capture commands verbatim in Attempts history; no paraphrasing.
- Ensure all temporary scripts/snippets are stored under `reports/…/` (not at repo root) and kept ASCII.
- Run `pytest --collect-only` before leaving the loop to confirm the selector remains valid.
- Avoid committing artifacts until supervisor review; keep git status clean apart from expected report/doc updates.
- Follow trace schema from docs/debugging/debugging.md when adding new trace lines.
Pointers:
- plans/active/cli-noise-pix0/plan.md:264-267 — Phase L3f/L3g task definitions and success criteria.
- docs/fix_plan.md:450-463 — CLI-FLAGS-003 next actions and evidence expectations.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C reference rot vectors & lattice factors.
- reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json — PyTorch per-φ reference for context.
Next Up: 1) If rotation vectors align, draft `rot_vector/analysis.md` conclusions and update docs/fix_plan.md; 2) Prepare code-edit plan for the identified rotation discrepancy (Phase L3g follow-up).
EOF
' succeeded in 948ms:

[2025-10-07T10:04:28] tokens used: 183,696
[2025-10-07T10:04:28] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:04:34] thinking

**Correcting code snippet for ASCII compliance**
[2025-10-07T10:04:36] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/input.md
@@ -37,8 +37,8 @@
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
[2025-10-07T10:04:36] apply_patch(auto_approved=true) exited 0 in 29ms:
Success. Updated the following files:
M input.md
[2025-10-07T10:04:36] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:04:36] tokens used: 186,388
[2025-10-07T10:04:36] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:04:38] thinking

**Verifying snippet character usage**
[2025-10-07T10:04:38] exec bash -lc 'rg -n "Å" input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:04:39] bash -lc 'rg -n "Å" input.md' exited 1 in 942ms:

[2025-10-07T10:04:39] tokens used: 186,734
[2025-10-07T10:04:39] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:04:44] thinking

**Checking input.md and preparing git status**
[2025-10-07T10:04:45] exec bash -lc 'git status --short' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:04:45] bash -lc 'git status --short' succeeded in 784ms:
 M docs/fix_plan.md
 M input.md
 M plans/active/cli-noise-pix0/plan.md
[2025-10-07T10:04:45] tokens used: 210,844
[2025-10-07T10:04:45] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:04:49] thinking

**Reviewing code differences**
[2025-10-07T10:04:50] exec bash -lc 'git diff' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:04:51] bash -lc 'git diff' succeeded in 752ms:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c..548ef94 100644
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
[2025-10-07T10:04:51] tokens used: 211,804
[2025-10-07T10:04:51] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:04:55] thinking

**Verifying and replacing non-ASCII characters**
[2025-10-07T10:04:56] exec bash -lc 'rg -n "Å" plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:04:57] bash -lc 'rg -n "Å" plans/active/cli-noise-pix0/plan.md' succeeded in 894ms:
31:- Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
121:| G3 | Trace verification + parity rerun | [D] | ✅ 2025-10-06 Attempt #18 (ralph). Fixed MOSFLM matrix transpose bug (`src/nanobrag_torch/io/mosflm.py:88`). Reciprocal vectors now match C exactly (9/9 components to 16 decimals). Miller indices match: (2,2,-13). F_cell perfect match: 300.58. Real vectors within 0.05Å (<0.2% error). Artifacts: `reports/2025-10-cli-flags/phase_g/trace_summary_orientation_fixed.md`, `traces/trace_c.log`, `traces/trace_py_fixed.log`. Intensity parity still divergent (deferred to Phase H polarization). |
202:| K3a | Guard MOSFLM rescale path in `Crystal.compute_cell_tensors` | [D] | ✅ 2025-11-06 Attempt #43 (`a89f6fd`): added `mosflm_provided` guard; `mosflm_rescale.py` now reports Δa/b/c=0.0 Å against C (`orientation_delta.md`). |
265:| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
[2025-10-07T10:04:57] tokens used: 215,740
[2025-10-07T10:04:57] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:05:11] thinking

**Replacing non-ASCII characters with ASCII equivalents**

I’m carefully updating all newly added lines and changed text to replace non-ASCII symbols like `Å`, `φ`, `Δ`, and Unicode minus signs with their ASCII counterparts ("Angstrom," "phi," "delta," and "-") to align with instructions that prefer ASCII for edits and new content, while considering existing pre-ASCII usage and avoiding unnecessary big changes to old lines.
[2025-10-07T10:05:12] exec bash -lc 'rg -n "φ" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:05:13] bash -lc 'rg -n "φ" docs/fix_plan.md' succeeded in 985ms:
453:- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
461:  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
462:  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
512:  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
513:    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
515:      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
517:      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
521:      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
610:      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
614:      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
616:    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
617:  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
618:    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
621:      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
622:      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
627:      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
628:      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
630:      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
631:    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
923:  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
[2025-10-07T10:05:13] tokens used: 217,415
[2025-10-07T10:05:13] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..ec528c0b7ee9314b37333812a5a81434b4116da3
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:05:20] thinking

**Deciding on phi character usage**
[2025-10-07T10:05:27] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/plans/active/cli-noise-pix0/plan.md
@@ -28,3 +28,3 @@
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-φ instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual φ=0 mismatch (`k_frac` −0.589 in PyTorch vs −0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
@@ -263,4 +263,4 @@
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at φ=0. |
-| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for φ=0 using updated harness, and dump aligned values (Å + Å⁻¹) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight Δ components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
 | L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
[2025-10-07T10:05:27] apply_patch(auto_approved=true) exited 0 in 43ms:
Success. Updated the following files:
M plans/active/cli-noise-pix0/plan.md
[2025-10-07T10:05:27] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:05:27] tokens used: 220,857
[2025-10-07T10:05:27] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:05:30] thinking

**Verifying Unicode absence**
[2025-10-07T10:05:31] exec bash -lc 'rg -n "Å" plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:05:32] bash -lc 'rg -n "Å" plans/active/cli-noise-pix0/plan.md' succeeded in 1.16s:
31:- Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
121:| G3 | Trace verification + parity rerun | [D] | ✅ 2025-10-06 Attempt #18 (ralph). Fixed MOSFLM matrix transpose bug (`src/nanobrag_torch/io/mosflm.py:88`). Reciprocal vectors now match C exactly (9/9 components to 16 decimals). Miller indices match: (2,2,-13). F_cell perfect match: 300.58. Real vectors within 0.05Å (<0.2% error). Artifacts: `reports/2025-10-cli-flags/phase_g/trace_summary_orientation_fixed.md`, `traces/trace_c.log`, `traces/trace_py_fixed.log`. Intensity parity still divergent (deferred to Phase H polarization). |
202:| K3a | Guard MOSFLM rescale path in `Crystal.compute_cell_tensors` | [D] | ✅ 2025-11-06 Attempt #43 (`a89f6fd`): added `mosflm_provided` guard; `mosflm_rescale.py` now reports Δa/b/c=0.0 Å against C (`orientation_delta.md`). |
[2025-10-07T10:05:32] tokens used: 229,931
[2025-10-07T10:05:32] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:05:37] thinking

**Reviewing non-ASCII character usage**
[2025-10-07T10:05:42] tokens used: 233,117
[2025-10-07T10:05:42] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:05:51] thinking

**Updating patch approach for non-ASCII characters**
[2025-10-07T10:05:55] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -460,4 +460,4 @@
 - Next Actions (2025-11-19 refresh):
-  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
-  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
   3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
[2025-10-07T10:05:55] apply_patch(auto_approved=true) exited 0 in 37ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T10:05:55] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..d6902ff470303b51c3b6ee9703a4996f12012152
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:05:55] tokens used: 235,083
[2025-10-07T10:05:55] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..d6902ff470303b51c3b6ee9703a4996f12012152
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:05:59] thinking

**Preparing to replace substrings in file**
[2025-10-07T10:06:01] exec bash -lc "python - <<'PY'
from pathlib import Path
path = Path('docs/fix_plan.md')
text = path.read_text()
text = text.replace('Per-φ', 'Per-phi')
text = text.replace('φ=0', 'phi=0')
text = text.replace('φ steps', 'phi steps')
text = text.replace('φ step', 'phi step')
text = text.replace('φ ', 'phi ')
text = text.replace('φ', 'phi')
text = text.replace('Δ', 'delta')
text = text.replace('≈', 'approx')
path.write_text(text)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:02] bash -lc "python - <<'PY'
from pathlib import Path
path = Path('docs/fix_plan.md')
text = path.read_text()
text = text.replace('Per-φ', 'Per-phi')
text = text.replace('φ=0', 'phi=0')
text = text.replace('φ steps', 'phi steps')
text = text.replace('φ step', 'phi step')
text = text.replace('φ ', 'phi ')
text = text.replace('φ', 'phi')
text = text.replace('Δ', 'delta')
text = text.replace('≈', 'approx')
path.write_text(text)
PY" succeeded in 949ms:

[2025-10-07T10:06:02] tokens used: 236,300
[2025-10-07T10:06:02] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..8bfaf07193883c60d762278b100641eb1256a737
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedupapprox0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cellapprox190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox-7,kapprox-1,lapprox-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm deltaF error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. deltaF = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify deltaF/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (deltaF=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: deltaFast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (deltaFclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (deltakapprox6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). deltakapprox6.042 at phi=0°, persists at all phi_tic=0…9. deltaF_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the deltakapprox6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/deltal < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (deltakapprox6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox-0.217596 m, Sbeamapprox-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), deltaF = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with deltaF = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, deltal=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |deltaF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |deltaF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |deltaF| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |deltaF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx9,534 more than header ranges) and consistent index shifts (deltakapprox+10–12, deltalapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |deltaF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |deltaF| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |deltaF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |deltaF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing deltakapprox6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corrapprox0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:08] thinking

**Verifying replacement accuracy**
[2025-10-07T10:06:10] exec bash -lc 'rg -n "φ" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:11] bash -lc 'rg -n "φ" docs/fix_plan.md' exited 1 in 760ms:

[2025-10-07T10:06:11] tokens used: 236,694
[2025-10-07T10:06:11] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..8bfaf07193883c60d762278b100641eb1256a737
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedupapprox0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cellapprox190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox-7,kapprox-1,lapprox-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm deltaF error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. deltaF = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify deltaF/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (deltaF=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: deltaFast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (deltaFclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (deltakapprox6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). deltakapprox6.042 at phi=0°, persists at all phi_tic=0…9. deltaF_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the deltakapprox6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/deltal < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (deltakapprox6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox-0.217596 m, Sbeamapprox-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), deltaF = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with deltaF = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, deltal=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |deltaF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |deltaF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |deltaF| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |deltaF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx9,534 more than header ranges) and consistent index shifts (deltakapprox+10–12, deltalapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |deltaF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |deltaF| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |deltaF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |deltaF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing deltakapprox6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corrapprox0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:13] thinking

**Scanning for special characters**
[2025-10-07T10:06:14] exec bash -lc 'rg -n "Δ" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:14] bash -lc 'rg -n "Δ" docs/fix_plan.md' exited 1 in 902ms:

[2025-10-07T10:06:14] tokens used: 236,917
[2025-10-07T10:06:14] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..8bfaf07193883c60d762278b100641eb1256a737
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedupapprox0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cellapprox190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox-7,kapprox-1,lapprox-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm deltaF error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. deltaF = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify deltaF/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (deltaF=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: deltaFast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (deltaFclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (deltakapprox6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). deltakapprox6.042 at phi=0°, persists at all phi_tic=0…9. deltaF_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the deltakapprox6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/deltal < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (deltakapprox6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox-0.217596 m, Sbeamapprox-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), deltaF = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with deltaF = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, deltal=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |deltaF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |deltaF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |deltaF| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |deltaF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx9,534 more than header ranges) and consistent index shifts (deltakapprox+10–12, deltalapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |deltaF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |deltaF| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |deltaF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |deltaF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing deltakapprox6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corrapprox0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:17] thinking

**Questioning symbol meaning**
[2025-10-07T10:06:18] exec bash -lc 'rg -n "≈" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:19] bash -lc 'rg -n "≈" docs/fix_plan.md' exited 1 in 892ms:

[2025-10-07T10:06:19] tokens used: 239,771
[2025-10-07T10:06:19] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..8bfaf07193883c60d762278b100641eb1256a737
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedupapprox0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cellapprox190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox-7,kapprox-1,lapprox-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm deltaF error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. deltaF = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify deltaF/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (deltaF=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: deltaFast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (deltaFclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (deltakapprox6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). deltakapprox6.042 at phi=0°, persists at all phi_tic=0…9. deltaF_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the deltakapprox6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/deltal < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (deltakapprox6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox-0.217596 m, Sbeamapprox-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), deltaF = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with deltaF = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, deltal=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |deltaF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |deltaF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |deltaF| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |deltaF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx9,534 more than header ranges) and consistent index shifts (deltakapprox+10–12, deltalapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |deltaF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |deltaF| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |deltaF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |deltaF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing deltakapprox6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corrapprox0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:26] thinking

**Verifying character replacements**
[2025-10-07T10:06:26] exec bash -lc 'git diff' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:27] bash -lc 'git diff' succeeded in 885ms:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c..8bfaf07 100644
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
[2025-10-07T10:06:27] tokens used: 240,395
[2025-10-07T10:06:27] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..8bfaf07193883c60d762278b100641eb1256a737
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedupapprox0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cellapprox190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox-7,kapprox-1,lapprox-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm deltaF error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. deltaF = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify deltaF/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (deltaF=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: deltaFast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (deltaFclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (deltakapprox6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). deltakapprox6.042 at phi=0°, persists at all phi_tic=0…9. deltaF_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the deltakapprox6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/deltal < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (deltakapprox6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox-0.217596 m, Sbeamapprox-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), deltaF = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with deltaF = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, deltal=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |deltaF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |deltaF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |deltaF| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |deltaF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx9,534 more than header ranges) and consistent index shifts (deltakapprox+10–12, deltalapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |deltaF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |deltaF| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |deltaF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |deltaF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing deltakapprox6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corrapprox0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:35] thinking

**Correcting spacing after global replacements**
[2025-10-07T10:06:36] exec bash -lc 'rg -n "approx[0-9]" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:37] bash -lc 'rg -n "approx[0-9]" docs/fix_plan.md' succeeded in 788ms:
113:- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
240:    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
250:    Metrics: Profiling (512² warm) shows compiled region approx66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
279:  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedupapprox0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
501:    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx0.000002%).
585:      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cellapprox190.27 matching Phase L3b probe results (tolerance 1%)
741:      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
898:      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx0.9126)
923:  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (deltakapprox6.0 at all phi steps), NOT a phi-sampling offset.**
924:    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). deltakapprox6.042 at phi=0°, persists at all phi_tic=0…9. deltaF_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
955:      - **Miller index impact:** Massive lattice error explains the deltakapprox6 mismatch from Phase K3e
989:      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (deltakapprox6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
1184:      - Intensity ratio approx256 suspicious (2⁸) but likely coincidental given geometry mismatch
1532:  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |deltaF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx9,534 more than header ranges) and consistent index shifts (deltakapprox+10–12, deltalapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
1634:      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox0.508 for cos(2θ)=-0.130070.
1655:      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing deltakapprox6.04 units).
1719:    Metrics: corrapprox0.99963; metric duality matches within 1e-12.
[2025-10-07T10:06:37] tokens used: 243,802
[2025-10-07T10:06:37] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..8bfaf07193883c60d762278b100641eb1256a737
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warmapprox0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedupapprox0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cellapprox190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox-7,kapprox-1,lapprox-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm deltaF error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. deltaF = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify deltaF/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (deltaF=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: deltaFast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (deltaFclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (deltakapprox6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). deltakapprox6.042 at phi=0°, persists at all phi_tic=0…9. deltaF_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the deltakapprox6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/deltal < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (deltakapprox6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox-0.217596 m, Sbeamapprox-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), deltaF = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with deltaF = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, deltal=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |deltaF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |deltaF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |deltaF| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |deltaF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx9,534 more than header ranges) and consistent index shifts (deltakapprox+10–12, deltalapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |deltaF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |deltaF| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |deltaF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |deltaF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing deltakapprox6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corrapprox0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:41] thinking

**Improving readability of "approx" replacements**
[2025-10-07T10:06:44] exec bash -lc "python - <<'PY'
from pathlib import Path
import re
path = Path('docs/fix_plan.md')
text = path.read_text()
text = re.sub(r'approx(?=[0-9-])', 'approx ', text)
text = re.sub(r'deltakapprox', 'delta k approx', text)
text = re.sub(r'deltaF', 'delta F', text)
text = re.sub(r'speedup_warmapprox', 'speedup_warm approx', text)
text = re.sub(r'Correlations approx', 'Correlations approx ', text)
text = re.sub(r'speedupapprox', 'speedup approx ', text)
text = re.sub(r'F_cellapprox', 'F_cell approx', text)
text = re.sub(r'deltaF_latt', 'delta F_latt', text)
text = re.sub(r'delta k', 'delta k', text)
text = re.sub(r'deltaF', 'delta F', text)
text = re.sub(r'deltaF', 'delta F', text)
text = re.sub(r'deltal', 'delta l', text)
text = re.sub(r'corrapprox', 'corr approx', text)
path.write_text(text)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:45] bash -lc "python - <<'PY'
from pathlib import Path
import re
path = Path('docs/fix_plan.md')
text = path.read_text()
text = re.sub(r'approx(?=[0-9-])', 'approx ', text)
text = re.sub(r'deltakapprox', 'delta k approx', text)
text = re.sub(r'deltaF', 'delta F', text)
text = re.sub(r'speedup_warmapprox', 'speedup_warm approx', text)
text = re.sub(r'Correlations approx', 'Correlations approx ', text)
text = re.sub(r'speedupapprox', 'speedup approx ', text)
text = re.sub(r'F_cellapprox', 'F_cell approx', text)
text = re.sub(r'deltaF_latt', 'delta F_latt', text)
text = re.sub(r'delta k', 'delta k', text)
text = re.sub(r'deltaF', 'delta F', text)
text = re.sub(r'deltaF', 'delta F', text)
text = re.sub(r'deltal', 'delta l', text)
text = re.sub(r'corrapprox', 'corr approx', text)
path.write_text(text)
PY" succeeded in 789ms:

[2025-10-07T10:06:45] tokens used: 245,672
[2025-10-07T10:06:45] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..dd593de2a0285707e48c97cd8baae0525876cc1e
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm approx 0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx  1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx 66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup approx  0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx 0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell approx 190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox -7,kapprox -1,lapprox -14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx 0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm delta F error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. delta F = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify delta F/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (delta F=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: delta Fast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (delta Fclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx 0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (delta k approx 6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). delta k approx 6.042 at phi=0°, persists at all phi_tic=0…9. delta F_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the delta k approx 6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/delta l < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (delta k approx 6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox -0.217596 m, Sbeamapprox -0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), delta F = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with delta F = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx 256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, delta l=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |delta F|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |delta F|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |delta F| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |delta F|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx 9,534 more than header ranges) and consistent index shifts (delta k approx+10–12, delta lapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |delta F| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |delta F| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |delta F| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |delta F|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox 0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing delta k approx 6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corr approx 0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:48] thinking

**Verifying replacement accuracy**
[2025-10-07T10:06:48] exec bash -lc 'rg -n "approx[0-9-]" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:50] bash -lc 'rg -n "approx[0-9-]" docs/fix_plan.md' exited 1 in 1.13s:

[2025-10-07T10:06:50] tokens used: 251,381
[2025-10-07T10:06:50] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..dd593de2a0285707e48c97cd8baae0525876cc1e
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm approx 0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx  1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx 66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup approx  0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx 0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell approx 190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox -7,kapprox -1,lapprox -14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx 0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm delta F error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. delta F = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify delta F/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (delta F=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: delta Fast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (delta Fclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx 0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (delta k approx 6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). delta k approx 6.042 at phi=0°, persists at all phi_tic=0…9. delta F_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the delta k approx 6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/delta l < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (delta k approx 6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox -0.217596 m, Sbeamapprox -0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), delta F = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with delta F = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx 256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, delta l=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |delta F|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |delta F|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |delta F| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |delta F|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx 9,534 more than header ranges) and consistent index shifts (delta k approx+10–12, delta lapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |delta F| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |delta F| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |delta F| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |delta F|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox 0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing delta k approx 6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corr approx 0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:54] thinking

**Searching for impacted code snippets**
[2025-10-07T10:06:55] exec bash -lc 'rg -n "deltaF" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:06:56] bash -lc 'rg -n "deltaF" docs/fix_plan.md' exited 1 in 830ms:

[2025-10-07T10:06:56] tokens used: 251,927
[2025-10-07T10:06:56] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..dd593de2a0285707e48c97cd8baae0525876cc1e
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm approx 0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx  1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx 66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup approx  0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx 0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell approx 190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox -7,kapprox -1,lapprox -14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx 0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm delta F error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. delta F = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify delta F/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (delta F=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: delta Fast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (delta Fclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx 0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (delta k approx 6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). delta k approx 6.042 at phi=0°, persists at all phi_tic=0…9. delta F_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the delta k approx 6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/delta l < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (delta k approx 6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox -0.217596 m, Sbeamapprox -0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), delta F = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with delta F = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx 256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, delta l=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |delta F|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |delta F|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |delta F| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |delta F|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx 9,534 more than header ranges) and consistent index shifts (delta k approx+10–12, delta lapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |delta F| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |delta F| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |delta F| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |delta F|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox 0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing delta k approx 6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corr approx 0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:06:59] thinking

**Reviewing documentation diff for consistency**
[2025-10-07T10:06:59] exec bash -lc 'git diff docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:07:00] bash -lc 'git diff docs/fix_plan.md' succeeded in 876ms:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c..dd593de 100644
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm approx 0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx  1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
[2025-10-07T10:07:00] tokens used: 252,067
[2025-10-07T10:07:00] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..dd593de2a0285707e48c97cd8baae0525876cc1e
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -110,7 +110,7 @@
   * C: `NB_C_BIN=./golden_suite_generator/nanoBragg python scripts/benchmarks/benchmark_detailed.py --sizes 256,512,1024 --device cpu --iterations 2`
   * PyTorch: `env KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/investigate_compile_cache.py --instances 5 --size 256 --devices cpu,cuda --dtypes float64,float32 --sources 1`
   * Shapes/ROI: 256²–1024² detectors, pixel 0.1 mm, oversample 1, full-frame ROI
-- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm≈0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
+- First Divergence (if known): 4096² warm runs still trail the C binary (latest speedup_warm approx 0.30 per reports/benchmarks/20251001-025148/ after 1.77 s PyTorch vs 0.53 s C) and prior 1.11× measurements appear non-reproducible without warmed torch.compile caches; additionally, PyTorch now honours `source_weights` whereas C ignores them (golden_suite_generator/nanoBragg.c:2604-3278), so weighted-source parity remains unresolved pending a decision memo (src/nanobrag_torch/simulator.py:310-360, 420-570).
 - Immediate Next Actions (2025-10-15):
   * Implement plan task B7 so benchmark logs record the actual compile mode, push/pop any prior `NANOBRAGG_DISABLE_COMPILE` value, and stop reusing cached simulators across mode switches; rerun the 4096² regression command both compiled and eager to populate reports/benchmarks/<stamp>-env-toggle-fix/.
   * After the harness fix, redo Phase B task B6: run ten cold-process warm benchmarks with the new compile-mode field, record cache/state metadata, and reconcile the slowdown in reports/benchmarks/20251001-025148/ against the prior 1.11× runs from reports/benchmarks/20251001-014819-measurement-reconciliation/.
@@ -237,7 +237,7 @@
     Observations/Hypotheses: Script now defaults to `reports/benchmarks/YYYYMMDD-HHMMSS-multi-source-normalization/` pattern, ensuring successive runs produce unique directories. CLI flag `--outdir` allows explicit override (relative or absolute paths). Fix unblocks Phase A baseline capture.
     Next Actions: Mark plan.md task A0 as `[X]` complete. Proceed with A1-A4 baseline benchmarks.
   * [2025-09-30] Attempt #23 (ralph loop BC) — Result: Phase 3 decision memo written BEFORE Attempt #22 invalidation. Wrote comprehensive memo integrating P3.0–P3.4 deliverables and CPU/CUDA benchmarks assuming P3.0c was valid. Recommendation: DEFER Phase 4 (graph optimization) given CUDA meets all targets (1.55–3.33× faster than C across 256²–1024²) and only large CPU detectors (1024²) show deficit (2.4× slower).
-    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations ≈1.0 throughout.
+    Metrics: CPU — 256²: 4.07× faster, 512²: 1.23× slower (within tolerance), 1024²: 2.43× slower. CUDA — 256²: 1.55× faster, 512²: 1.69× faster, 1024²: 3.33× faster. Cache: 37–6428× setup speedup. Correlations approx  1.0 throughout.
     Artifacts: `reports/benchmarks/20250930-perf-summary/PHASE_3_DECISION.md` (decision memo - **PROVISIONAL pending P3.0c correction**), `reports/benchmarks/20250930-perf-summary/cpu/P3.2_summary.md` (CPU), `reports/benchmarks/20250930-220739/benchmark_results.json` (CUDA).
     Observations/Hypotheses: CUDA performance validates torch.compile effectiveness. 1024² CPU deficit likely stems from memory bandwidth (L3 cache pressure) and C compiler advantage for sequential loops. Production users prioritize GPU for large detectors. Phase 4 graph work has uncertain ROI given CUDA already exceeds targets. **NOTE:** Phase 3 decision memo is PROVISIONAL because Attempt #22 invalidated P3.0c weighted-source validation.
     Next Actions: Re-execute Attempt #22 corrective actions (fix validation script with BeamConfig-based weights), regenerate P3.0c artifacts, then update Phase 3 decision memo if weighted-source semantics change. Keep memo marked PROVISIONAL until P3.0c re-validated.
@@ -247,7 +247,7 @@
     Observations/Hypotheses: The old Attempt #21 validation was invalid because it set `simulator.source_*` attributes post-construction; these are never read—only `_source_*` caches populated from `BeamConfig` during `__init__` are used. Corrected approach passes sources through BeamConfig before construction, ensuring `_source_directions/_source_wavelengths/_source_weights` are properly cached. CPU/CUDA self-consistency now proven with unequal weights exercised. Phase 3 decision memo can be finalized.
     Next Actions: Mark P3.0c as `[X]` in plan.md; finalize Phase 3 decision memo (P3.5) removing PROVISIONAL status. Phase 3 complete—all tasks delivered with valid parity evidence.
   * [2025-10-11] Attempt #25 — Result: diagnostic blockers surfaced. Re-running `KMP_DUPLICATE_LIB_OK=TRUE python scripts/validate_weighted_source_normalization.py` on macOS fails with `FileNotFoundError` because the script still writes to `/home/ollie/.../validation_results.json`. P3.0c evidence cannot be regenerated until the path is made repo-relative. Warm-run CPU profiling after compile (512² detector, `torch.compile` cached) reports `Torch-Compiled Region` dominating (~3.8 ms) with `aten::mul`/`aten::sum` leading operations. Need eager-mode traces to quantify `Crystal._nearest_neighbor_lookup` advanced indexing cost and to measure the ~200 MB `_cached_pixel_coords_meters` tensor that likely drives the 4096² slowdown. Updated plan Phase A with task A0 (fix script path) and added Phase B task B5 plus Phase C tasks C6/C7 to capture these diagnostics before attempting new optimizations.
-    Metrics: Profiling (512² warm) shows compiled region ≈66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
+    Metrics: Profiling (512² warm) shows compiled region approx 66 % of CPU time; validation script stack trace terminates at `/home/ollie/.../validation_results.json`.
     Artifacts: Inline profiling output (no trace saved yet); updated plan `plans/active/perf-pytorch-compile-refactor/plan.md`.
     Observations/Hypotheses: Multi-source tooling must be path-agnostic; structure-factor gather and pixel-coordinate cache size remain top suspects for the 3.4× warm gap.
     Next Actions: Repair validation script output path (plan task A0), record eager-mode trace per B5, then execute C6/C7 experiments prior to any performance code changes.
@@ -276,7 +276,7 @@
     Artifacts: `reports/benchmarks/20251001-030128-4096-warm-repro/{B6_summary.md, reproducibility_results.json, run1.log...run10.log}`.
     Observations/Hypotheses: (1) Performance is highly stable (CV<5%) confirming measurements are reliable. (2) Mean slowdown 1.21× exceeds ≤1.2× target by small margin (0.01×). (3) No runs achieved speedup ≥1.0 (range: 0.78–0.88). (4) **Phase B complete—target NOT met, proceed to Phase C diagnostics.** This addresses supervisor's Attempt #32 concern—B6 provides the requested reproducibility data showing stable 1.21× slowdown, not the 3.3× regression detected in 025148. Core geometry tests 31/31 passed.
     Next Actions: Execute Phase C diagnostic experiments (C1-C7) to identify remaining bottlenecks before considering optimization work in Phase D.
-  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup≈0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
+  * [2025-10-13] Attempt #34 (galph loop CG) — Result: Phase B re-opened. New benchmark log `reports/benchmarks/20251001-025148/benchmark_results.json` shows warm speedup approx  0.30 (PyTorch 1.7738 s vs C 0.5306 s) even with cache hits, contradicting Attempt #33’s 0.828±0.033 average. Investigation found `scripts/benchmarks/benchmark_detailed.py` mutates `NB_DISABLE_COMPILE` when `--disable-compile` is toggled while the simulator actually reads `NANOBRAGG_DISABLE_COMPILE`, so the flag is ignored and cached compiled simulators bleed across "eager" runs. Because the env var is also unset unconditionally, downstream callers inherit indeterminate compile state and benchmark logs lack mode metadata.
     Next Actions: (1) Implement Plan task B7 (push/pop `NANOBRAGG_DISABLE_COMPILE`, cache keyed by mode, compile-mode metadata). (2) Re-run the ten-run cold-process study per reopened B6 after patching the harness, capturing cache-hit + compile-mode flags in each artifact. (3) Draft reconciliation memo comparing the new roll-up against both the 025148 regression and 030128 reproducibility datasets, then update plan status before entering Phase C.
 - Risks/Assumptions: 1024² CPU performance acceptable given GPU alternative; Phase 4 revisit only if future profiling identifies specific bottlenecks. Weighted-source tooling must be path-agnostic before P3.0c evidence is considered durable.
 - Exit Criteria (quote thresholds from spec):
@@ -450,17 +450,17 @@
 ## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
 - Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
 - Priority: High
-- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
+- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-phi evidence + normalization closure)
 - Owner/Date: ralph/2025-10-05
 - Plan Reference: `plans/active/cli-noise-pix0/plan.md`
 - Reproduction (C & PyTorch):
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
-- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hklapprox(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for phi=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify delta components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the phi=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -470,7 +470,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` - Updated environment snapshot (git_sha current, torch version, device/dtype)
     Observations/Hypotheses:
       - **Harness functional:** Attempt #70 audit was stale; Attempt #69 already resolved the `(F_grid, metadata)` API change
-      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hkl≈(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
+      - **First divergence confirmed:** `I_before_scaling=0` in PyTorch vs `943654.809` in C stems from the targeted pixel (685,1039) mapping to hklapprox(−7,−1,−14), where PyTorch reports `F_cell=0` but C’s trace records `F_cell=190.27`.
       - **Root cause hypothesis (updated 2025-11-15):** Harness instrumentation is now correct; the discrepancy arises from structure-factor ingestion (HKL grid vs generated Fdump) rather than MOSFLM vector wiring.
       - **Scaling infrastructure correct:** r_e², fluence, steps, capture_fraction, polar, omega_pixel all match C within tolerances (<0.002% delta)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants
@@ -498,7 +498,7 @@
       - **Attempt #75 misdiagnosis explained:** Previous analysis confused scaled.hkl with a different file or misread metadata; actual file is full binary grid
     Next Actions: Phase L3c — Audit `trace_harness.py` to find why HKL data isn't loaded despite file existing; fix harness to mirror probe's successful attachment pattern (crystal.hkl_data = F_grid, crystal.hkl_metadata = metadata); verify CLI __main__.py follows same pattern when -hkl provided; NO simulator.py changes needed.
   * [2025-11-17] Attempt #77 (ralph loop) — Result: **SUCCESS** (Phase L3a coverage re-verification COMPLETE). **Re-executed probe on current workstation, confirmed HKL coverage with timestamped artifacts.**
-    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (≈0.000002%).
+    Metrics: Evidence-only loop; no production code changed. Pytest collection successful (tests/test_cli_scaling.py collected cleanly). Probe confirms F_cell=190.27 retrieved from scaled.hkl with delta 4.27e-06 (approx 0.000002%).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log` - Fresh probe execution log with current git SHA acab43b, Torch 2.8.0+cu128, Linux platform
       - `reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md` - Updated analysis documenting HKL coverage confirmation and L3c next actions
@@ -509,21 +509,21 @@
       - **Root cause remains configuration/loading:** The F_cell=0 divergence in trace_py_scaling.log stems from harness/CLI not loading/attaching HKL data, NOT from missing reflections or broken lookup logic
       - **Phase L3a exit criteria satisfied:** Fresh probe log and analysis captured with current environment metadata; HKL coverage ranges documented and reconciled
     Next Actions: Phase L3c harness audit per plan guidance (review trace_harness.py lines 206-236, compare against probe successful pattern, fix HKL attachment). NO simulator.py changes needed (scaling math correct per L2c, lookup logic works per L3a probe).
-  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all φ steps, enabling per-φ parity validation.**
-    Metrics: 10 TRACE_PY_PHI lines captured (φ=0.0° to φ=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
+  * [2025-10-07] Attempt #85 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi instrumentation COMPLETE). **Extended Simulator and trace harness to emit TRACE_PY_PHI entries for all phi steps, enabling per-phi parity validation.**
+    Metrics: 10 TRACE_PY_PHI lines captured (phi=0.0° to phi=0.1°), structured JSON generated, test suite collection passes (652 tests), targeted scaling trace test passes 4/4 variants.
     Artifacts:
-      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-φ tracing block emitting k_frac, F_latt_b, F_latt for each phi step
+      - `src/nanobrag_torch/simulator.py:1424-1460` - Per-phi tracing block emitting k_frac, F_latt_b, F_latt for each phi step
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:260-333` - Harness updates to capture and parse TRACE_PY_PHI lines
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-φ trace (10 entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_per_phi_20251007.log` - PyTorch per-phi trace (10 entries)
       - `reports/2025-10-cli-flags/phase_l/per_phi/per_phi_py_20251007.json` - Structured JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/per_phi/comparison_summary_20251119.md` - Comparison summary documenting instrumentation and next steps
     Observations/Hypotheses:
-      - **Instrumentation complete:** Simulator now emits per-φ lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
+      - **Instrumentation complete:** Simulator now emits per-phi lattice entries reusing existing rotated vectors from Crystal.get_rotated_real_vectors
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
-      - **k_frac mismatch confirmed:** PyTorch k_frac ≈ -0.59 vs C reference k_frac ≈ -3.86 (Δk ≈ 3.27), consistent with Phase K3e findings
+      - **k_frac mismatch confirmed:** PyTorch k_frac approx -0.59 vs C reference k_frac approx -3.86 (deltak approx 3.27), consistent with Phase K3e findings
       - **Design decision:** Reuses production helpers (rotated vectors, sincg) per CLAUDE Rule #0.3 (no parallel trace-only physics reimplementation)
       - **Structured output:** JSON format enables comparison tooling integration per input.md Step 4
-    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if Δk persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
+    Next Actions: Phase L3e comparison — Run scripts/compare_per_phi_traces.py to generate delta analysis; if deltak persists, revisit base lattice parity (Phase K3f) before proceeding to scaling validation (Phase L3e full chain).
   * [2025-10-17] Attempt #78 (ralph loop) — Result: **SUCCESS** (Phase L3c CLI ingestion audit COMPLETE). **Audit confirms CLI successfully loads HKL grid with correct metadata; root cause of F_cell=0 is downstream from parsing.**
     Metrics: CLI loads HKL grid (49×57×62) with dtype=float32, device=cpu; all 9 metadata keys present (h_min/max/range, k_min/max/range, l_min/max/range); target reflection (-7,-1,-14) is IN RANGE; pytest collection successful (2 tests collected from test_cli_scaling.py).
     Artifacts:
@@ -547,7 +547,7 @@
       - **Fix confirmed:** Switching from `mosflm_a_star=[a*, b*, c*]` (list) to individual tensor assignments (mosflm_a_star=tensor(a*), mosflm_b_star=tensor(b*), mosflm_c_star=tensor(c*)) enables Crystal.compute_cell_tensors to use the MOSFLM orientation properly
       - **F_latt now non-zero:** Lattice shape factors are computed correctly (F_latt_a=-2.405, F_latt_b=-0.861, F_latt_c=0.652, product=1.351)
       - **F_cell still zero:** This is expected—pixel (685,1039) computes h=-7,k=-1,l=-14 but scaled.hkl only contains (1,12,3), so structure factor lookup correctly returns 0
-      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (Δ<0.002%)
+      - **Scaling factors verified:** All intermediate scaling terms (r_e², fluence, steps, capture_fraction, polar, omega_pixel) match C within tolerance (delta<0.002%)
       - **Test coverage maintained:** tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics passes all parametrized variants (4/4)
     Next Actions: Phase L2b complete; L2c evidence captured. Supervisor to proceed with Phase L3 structure-factor investigation based on comparison summary showing I_before_scaling as first divergence.
   * [2025-10-07] Attempt #79 (ralph loop) — Result: **SUCCESS** (Phase L3c device audit COMPLETE per supervisor "Do Now"). **Evidence-only loop; confirmed device handling gap: HKL tensor remains on CPU despite `-device cuda` flag.**
@@ -582,7 +582,7 @@
       - Git SHA at commit: 279aa3c
     Observations/Hypotheses:
       - **Fix confirmed working:** HKL tensor device matches requested device (validated via `.device.type` comparison to handle cuda vs cuda:0)
-      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell≈190.27 matching Phase L3b probe results (tolerance 1%)
+      - **Structure factor lookup validated:** Target reflection (-7,-1,-14) returns F_cell approx 190.27 matching Phase L3b probe results (tolerance 1%)
       - **Test parametrization correct:** Uses pytest.mark.skipif for CUDA variants when GPU unavailable; CPU tests run unconditionally
       - **Device comparison quirk handled:** `.device.type` comparison used instead of direct `.device ==` to handle PyTorch's cuda:0 resolution
       - **Phase L3d exit criteria satisfied:** Device transfer patch landed, regression test covers CLI→Crystal HKL flow, smoke tests pass
@@ -607,28 +607,28 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Updated markdown table using the new TRACE_PY capture
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Refreshed structured metrics (overwrites prior Phase L2c output with latest deltas)
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Updated run metadata (git SHA 1de11df, torch 2.7.1, command/path snapshot)
-      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-φ TRACE instrumentation
+      - `reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md` — Narrative linking residual error to `F_latt` sign/magnitude mismatch and recommending per-phi TRACE instrumentation
     Observations/Hypotheses:
       - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
       - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
-      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
+      - **Per-phi evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
       - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
-    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
-  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
-    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
+    Next Actions: Instrument per-phi `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
+  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-phi scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
+    Metrics: Harness captured 10 TRACE_PY_PHI lines (phi=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
-      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-phi trace (10 TRACE_PY_PHI entries)
+      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-phi JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
-      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **Harness operational:** Per-phi instrumentation from Attempt #85 working correctly; captured k_frac approx -0.59 (phi=0°) to -0.607 (phi=0.1°), F_latt ranges 1.351 to -2.351
+      - **phi=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows phi=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later phi steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
-      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+      - **Phase L3e exit criteria satisfied:** Per-phi JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for phi=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
@@ -639,7 +639,7 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json` — Updated environment snapshot
     Observations/Hypotheses:
       - **Fix validated:** Crystal.get_structure_factor now consults proper attributes; lookup for (1,12,3) returns F=100 as expected
-      - **F_cell=0 is correct:** Pixel (685,1039) maps to h≈-7,k≈-1,l≈-14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
+      - **F_cell=0 is correct:** Pixel (685,1039) maps to happrox -7,kapprox -1,lapprox -14; scaled.hkl only contains (1,12,3), so out-of-range lookup correctly returns 0
       - **Scaling chain validated:** All intermediate factors (r_e²=7.94e-30, fluence=1e24, steps=10, polar=0.914641, omega=4.204e-07) match C within tolerance
       - **First divergence confirmed:** I_before_scaling (C=943654.809, Py=0) diverges because pixel maps to reflection not in HKL grid
       - **Phase L2b/L2c complete:** Harness functional, comparison executed, first divergence documented
@@ -738,7 +738,7 @@
       - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
       - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
       - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
-      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
+      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C approx 0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
     Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
   * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
     Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
@@ -759,11 +759,11 @@
     Observations/Hypotheses:
       - **Root Cause Confirmed:** `Detector._configure_geometry` was using `self.beam_center_f/s` which are in pixels (converted in __init__), but the correct approach is to use `self.config.beam_center_f/s` directly (in mm) and convert mm→m explicitly
       - **C Reference:** nanoBragg.c:1220-1221 for MOSFLM: `Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000`
-      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm ΔF error that this fix resolves
+      - **Documentation:** Reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md documents the baseline 1.1mm delta F error that this fix resolves
       - **Phase H5 Tasks:** H5a (C precedence) ✅, H5b (revert override for custom vectors) ✅, H5d (fix_plan update) ✅, H5e (unit correction) ✅; H5c (post-fix trace capture) pending
     Next Actions: Execute Phase H5c trace harness to capture post-fix PyTorch traces showing pix0 parity with C, then proceed to Phase K1 (F_latt SQUARE sincg fix)
   * [2025-10-24] Attempt #35 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H5c trace capture). **Critical finding: Attempt #33's beam-center mm→m fix had NO impact on pix0 calculation. Pix0 deltas IDENTICAL to 2025-10-22 baseline.**
-    Metrics: Pix0 parity NOT achieved. ΔF = -1136.4 μm (exceeds <50 μm threshold), ΔS = +139.3 μm (exceeds threshold), ΔO = -5.6 μm (within threshold). Total magnitude Δ = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
+    Metrics: Pix0 parity NOT achieved. delta F = -1136.4 μm (exceeds <50 μm threshold), deltaS = +139.3 μm (exceeds threshold), deltaO = -5.6 μm (within threshold). Total magnitude delta = 1.145 mm. All deltas identical to pre-Attempt #33 baseline.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/trace_py.stdout` - Fresh PyTorch trace post-Attempt #33 (wall-clock: ~3s)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-24/README.md` - Trace command, environment, git context
@@ -777,7 +777,7 @@
       - **Systematic bias pattern:** Fast-axis error (1136 μm) is 10× larger than slow-axis error (139 μm), suggesting targeted calculation error rather than global scaling issue.
       - **Hypothesis:** `_calculate_pix0_vector()` likely has unit conversion error when using custom vectors OR pivot mode (SAMPLE) calculation differs from C implementation despite identical detector_pivot setting.
       - **Phase K blocked:** The 1.1mm pix0 error will cascade into incorrect Miller index calculations (h/k/l off by several tenths) and invalidate F_latt comparisons.
-    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify ΔF/ΔS/ΔO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
+    Next Actions: **Phase H6 required** — Instrument `_calculate_pix0_vector()` with targeted print statements, generate comparative pix0 trace (Python vs C), identify first divergence, fix root cause, verify delta F/deltaS/deltaO all below 50μm. Phase K normalization work cannot proceed until pix0 parity is achieved.
   * [2025-10-24] Attempt #36 (ralph) — Result: **EVIDENCE COMPLETE** (Phase H6a C trace capture). **C pix0 calculation fully instrumented and captured for SAMPLE pivot with custom vectors.**
     Metrics: Evidence-only loop. Wall-clock ~3s. C binary rebuilt with SAMPLE pivot tracing. Supervisor command executed successfully with max_I=446.254.
     Artifacts:
@@ -808,8 +808,8 @@
     Observations/Hypotheses:
       - **PyTorch pix0:** (-0.216337, 0.215207, -0.230198) meters
       - **C pix0 (reference):** (-0.216476, 0.216343, -0.230192) meters
-      - **Deltas:** ΔX = +139 µm, ΔY = -1136 µm, ΔZ = -6 µm
-      - **Large ΔS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
+      - **Deltas:** deltaX = +139 µm, deltaY = -1136 µm, deltaZ = -6 µm
+      - **Large deltaS persists:** 1.1 mm slow-axis error remains after Phase H5e unit fix, indicating deeper divergence in pix0 calculation
       - **Instrumentation complete:** Captured detector_convention, angles (radians), beam_center_m (Xclose/Yclose), initial basis vectors, Fclose/Sclose, close_distance, ratio, distance, term_fast/slow/close_before_rot, pix0 (before/after rotations), final basis vectors, twotheta_axis
       - **Harness improvements:** Removed hard-coded DetectorPivot.BEAM (line 114) to allow natural SAMPLE pivot selection; patched Detector.__init__ instead of non-existent _configure_geometry method; added term_* emission for all CUSTOM cases with custom vectors (not pivot-gated)
       - **Editable install confirmed:** PYTHONPATH=src used throughout (per Phase H6b guardrails); trace reflects current detector.py implementation
@@ -823,7 +823,7 @@
     Observations/Hypotheses:
       - **Root cause:** PyTorch trace harness logs beam_center config values (in mm) directly under the "beam_center_m" label, while C logs the pixel-based derived values (in meters). This is a trace instrumentation mismatch, NOT a calculation error.
       - **Key evidence:** Fclose_m and Sclose_m match exactly between C and PyTorch (0.217742295 m, 0.21390708 m), confirming the mm→m conversion happens correctly in the actual pix0 calculation.
-      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (ΔF=-1136μm, ΔS=+139μm, ΔO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
+      - **Pix0 delta persists:** Despite correct Fclose/Sclose, final pix0 still differs by (delta F=-1136μm, deltaS=+139μm, deltaO=-6μm). This indicates the divergence occurs downstream in the term_fast/term_slow/term_close combination or a missing CUSTOM convention transform.
       - **No code changes:** Evidence-only loop per supervisor directive (input.md:13). Documentation artifacts captured for next debugging phase.
     Next Actions: **Phase H6d required** — Fix the trace logging to show consistent units (meters) for beam_center_m in PyTorch, then investigate why pix0 differs when Fclose/Sclose are identical. Likely causes: (1) MOSFLM +0.5 pixel offset applied/unapplied incorrectly in CUSTOM mode, (2) missing transform in custom-vector path, or (3) pivot calculation differs despite identical detector_pivot setting. Phase K1 remains blocked until pix0 parity achieved.
   * [2025-10-06] Attempt #39 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase H6e pivot parity proof). **Confirmed C uses SAMPLE pivot while PyTorch defaults to BEAM pivot for supervisor command.**
@@ -839,7 +839,7 @@
       - **Impact quantified:** BEAM pivot formula (`pix0 = -Fbeam·fdet - Sbeam·sdet + distance·beam`) differs from SAMPLE pivot (`pix0 = -Fclose·fdet - Sclose·sdet + close_distance·odet`) → different pix0 vectors → cascading into 1.14mm delta
       - **Specification alignment:** specs/spec-a-cli.md and docs/architecture/detector.md §5.2 both require custom vectors → SAMPLE pivot
       - **Phase H6e exit criteria met:** Evidence documented, parity mismatch confirmed, spec references cited
-    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |Δpix0| < 5e-5 m before closing H6).
+    Next Actions: **Phase H6f required** — Implement custom-vector-to-SAMPLE-pivot forcing rule in `DetectorConfig.__post_init__`/`from_cli_args` (detect any of `custom_fdet_vector`, `custom_sdet_vector`, `custom_odet_vector`, `custom_beam_vector`, or `pix0_override_m` and force `detector_pivot=DetectorPivot.SAMPLE`), add regression test `tests/test_cli_flags.py::test_custom_vectors_force_sample_pivot` (CPU+CUDA), document in `reports/2025-10-cli-flags/phase_h6/pivot_fix.md`, then proceed to Phase H6g (re-run PyTorch trace harness and require |deltapix0| < 5e-5 m before closing H6).
   * [2025-10-06] Attempt #40 (ralph loop) — Result: **SUCCESS** (Phase H6f complete). **Custom detector basis vectors now force SAMPLE pivot matching C behavior.**
     Metrics: Core test suite 61/61 passed in 5.41s (cli_flags 30/30 including new pivot test, detector_geometry 12/12, crystal_geometry 19/19). New regression test validates pivot forcing across 4 parametrized variants (cpu/cuda × float32/float64). pix0 delta improved from 1.14 mm (BEAM pivot, wrong) to ~5 μm (SAMPLE pivot, correct).
     Artifacts:
@@ -856,9 +856,9 @@
       - **C reference update:** Phase H6a C trace shows SAMPLE pivot produces pix0=(-0.216476, 0.216343, -0.230192); updated JSON expected values from old BEAM values
       - **Pivot override behavior:** Even explicit `detector_pivot=DetectorPivot.BEAM` is overridden to SAMPLE when custom basis vectors present (matching C)
       - **Phase H6f complete:** Implementation, tests, and documentation artifacts all landed; ready for Phase H6g parity verification
-    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |Δpix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
+    Next Actions: **Phase H6g required** — Re-run PyTorch trace harness with updated pivot logic (`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_h/trace_harness.py --out reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log`), diff against C trace to verify |deltapix0| < 5e-5 m threshold, run nb-compare smoke test for visual parity, archive artifacts under `phase_h6/post_fix/`, and record results in Attempt #41. Once pix0 parity confirmed, resume Phase K2 normalization with refreshed traces.
   * [2025-10-06] Attempt #41 (ralph loop) — Result: **SUCCESS** (Phase H6g complete). **pix0 alignment validated; max delta 2.85 μm << 50 μm threshold.**
-    Metrics: pix0 deltas: ΔFast=0.123 μm, ΔSlow=0.114 μm, ΔClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (ΔFclose=0, ΔSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
+    Metrics: pix0 deltas: delta Fast=0.123 μm, deltaSlow=0.114 μm, deltaClose=2.851 μm (max=2.851 μm < 50 μm threshold). Beam center perfect match (delta Fclose=0, deltaSclose=0). close_distance delta=2.837 μm, r_factor delta=6.8e-8. All geometry metrics align to sub-micron precision.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py.log` - PyTorch trace with SAMPLE pivot
       - `reports/2025-10-cli-flags/phase_h6/post_fix/trace_py_<stamp>.log` - Timestamped backup
@@ -872,7 +872,7 @@
       - **Beam center consistency:** Fclose/Sclose match exactly (0.0 delta), confirming CLI parsing and mm→m conversion are correct.
       - **Geometry stability:** close_distance delta of 2.84 μm and r_factor delta of 6.8e-8 demonstrate numerical stability across the detector geometry pipeline.
       - **No unit regressions:** All trace values in meters (as required by detector hybrid unit system); no millimeter/Angstrom confusion.
-      - **Phase H6g exit criteria met:** |Δpix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
+      - **Phase H6g exit criteria met:** |deltapix0| = 2.85 μm << 50 μm threshold; evidence artifacts archived; ready for Phase K2.
     Next Actions: **Phase K2 required** — With pix0 parity confirmed, regenerate scaling-chain analysis with corrected SAMPLE pivot traces. Execute Phase K2 checklist from `plans/active/cli-noise-pix0/plan.md`: (1) rerun PyTorch trace harness for pixel (1039,685) with final geometry, (2) diff F_latt components against C baseline, (3) update `reports/2025-10-cli-flags/phase_j/scaling_chain.md` with post-H6 comparison, (4) archive refreshed traces under `phase_k/f_latt_fix/`, (5) proceed to Phase K3 regression test once F_latt parity is restored.
   * [2025-10-06] Attempt #42 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K2/K2b). **Critical finding: MOSFLM rescaling is NOT the root cause of F_latt_b error.**
     Metrics: Evidence-only loop (per supervisor directive). Scaling ratios: F_latt_b C=38.63 vs Py=46.98 (21.6% error), polar C=0.913 vs Py=1.0 (9.6% error), I_final C=446 vs Py=497 (11.4% error). MOSFLM lattice vector analysis reveals perfect magnitude match (|b|_C = |b|_Py = 31.310 Å), ruling out rescale hypothesis.
@@ -887,15 +887,15 @@
       - **Rescale hypothesis disproven:** C with user_cell=0 (no rescale) produces |b| = 31.310 Å; PyTorch (always rescales) also produces |b| = 31.310 Å. Vectors match identically regardless of rescale path.
       - **F_latt_b still diverges:** Despite identical lattice vector magnitudes, F_latt_b shows 21.6% error (C=38.63, Py=46.98). This proves the error is NOT in vector_rescale logic.
       - **Polarization remains secondary issue:** C polar=0.9126 vs Py polar=1.0 (9.6% error). BeamConfig defaults still need alignment per Phase K3 task.
-      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared ≈ 57% mismatch).
+      - **First divergence confirmed:** I_before_scaling shows 87.8% lower in PyTorch due to F_latt squaring (25.5% error squared approx 57% mismatch).
       - **Root cause location narrowed:** F_latt_b error must originate in either (a) Miller index (h,k,l) calculation upstream of sincg, or (b) sincg function evaluation itself, NOT in lattice vector construction.
     Next Actions: **Phase K3 blocked** pending root-cause identification. Execute K2.1 (new subtask): compare fractional Miller indices (h_frac, k_frac, l_frac) from C vs PyTorch traces for pixel (1039, 685) to isolate whether the 21.6% F_latt_b error originates in scattering-vector → reciprocal-space projection or in the sincg lattice shape factor evaluation. Once first divergence is pinpointed, proceed with targeted fix in Phase K3.
   * [2025-11-06] Attempt #43 (ralph loop) — Result: **PARTIAL SUCCESS** (Phase K3a-K3b complete). **MOSFLM rescale guard implemented + polarization default aligned with C.**
-    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: Δa=Δb=Δc=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
+    Metrics: Core test suite 61/61 passed (crystal_geometry 19/19, detector_geometry 12/12, cli_flags 30/30). MOSFLM rescale diagnostic (mosflm_rescale.py) confirms perfect parity: deltaa=deltab=deltac=0.0000 Å. Parity matrix tests (AT-PARALLEL-001) failing with correlation~0.47, sum_ratio~0.42 - pre-existing issue not caused by these changes.
     Artifacts:
       - `src/nanobrag_torch/models/crystal.py:668-720` - Added conditional to skip cross-product rescaling when MOSFLM reciprocal vectors provided via -mat flag
       - `src/nanobrag_torch/config.py:504-512` - Changed BeamConfig.polarization_factor default from 1.0 to 0.0 to match C per-pixel reset behavior
-      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation ≈0.9126)
+      - `tests/test_cli_flags.py:594-634` - Updated test_default_polarization_parity to reflect correct C semantics (polar resets to 0.0 per pixel, triggering dynamic computation approx 0.9126)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/mosflm_rescale.py` - Diagnostic output shows PyTorch matches C with user_cell=0 (0.0 Å delta for all vectors)
     Observations/Hypotheses:
       - **K3a (MOSFLM rescale guard):** C code sets user_cell=0 when MOSFLM matrices supplied via -mat, skipping vector_rescale path. PyTorch now replicates this: rescale only when cell parameters provided WITHOUT MOSFLM orientation.
@@ -904,7 +904,7 @@
       - **Parity test failures appear pre-existing:** test_cli_scaling and test_parity_matrix failures not caused by K3a-K3b changes; likely require additional investigation beyond these tasks.
     Next Actions: **Phase K3c investigation required** - test_cli_scaling::test_f_latt_square_matches_c shows correlation=0.174 (< 0.999) and sum_ratio=1.45 (>1e-3). This appears unrelated to MOSFLM rescale or polarization defaults (test uses -cell without -mat, so K3a doesn't apply). Investigate root cause separately; K3a-K3b changes are correct per plan and diagnostic evidence.
   * [2025-10-06] Attempt #44 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3d dtype sensitivity). **Dtype precision NOT root cause of F_latt_b discrepancy.**
-    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac Δ=1.09e-06 (4.12e-06% error), k_frac Δ=1.38e-07 (1.40e-06% error), l_frac Δ=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
+    Metrics: Evidence-only loop. Float32 and float64 produce nearly identical results: F_latt_b=2.33 (both dtypes), vs C F_latt_b=38.63 → 93.98% error for both. Float64 improves F_latt precision by only 0.39% (relative dtype error 3.86e-01%), ruling out rounding as the root cause. Miller index precision: h_frac delta=1.09e-06 (4.12e-06% error), k_frac delta=1.38e-07 (1.40e-06% error), l_frac delta=4.13e-06 (3.73e-05% error). All precision deltas 6-7 orders of magnitude smaller than indices themselves.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/analyze_scaling.py` - Dtype sweep script with production-path instrumentation (compliant with CLAUDE.md Rule #0.3)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/float32_run.log` - Full float32 trace output
@@ -914,32 +914,32 @@
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.json` - Automated comparison summary
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/dtype_sensitivity.md` - Complete evidence report
     Observations/Hypotheses:
-      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b ≈ 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
+      - **Dtype precision ruled out:** Both float32 and float64 produce F_latt_b approx 2.33, which is 16.6× smaller than C's 38.63. This is a systematic error, not a precision issue.
       - **Miller index precision excellent:** Fractional h/k/l precision is 6-7 orders of magnitude better than needed, ruling out index rounding errors.
       - **F_latt component precision adequate:** Float64 improves F_latt by < 0.4%, negligible compared to the 93.98% systematic error.
       - **Root cause remains:** Geometric/orientation discrepancy (likely MOSFLM rescaling mismatch per Phase K2b) is the driver, not numerical precision.
       - **Phase K3a still critical:** The MOSFLM rescale guard is the next blocking step; dtype sweep confirms this is the right path.
     Next Actions: **Phase K3a remains blocking** - implement MOSFLM rescale guard per plan.md task, then rerun dtype sweep to verify F_latt_b moves closer to C's 38.63. If still divergent after K3a, investigate MOSFLM → real-space conversion formula and reciprocal vector recalculation sequence (CLAUDE.md Rule #13).
-  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-φ Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (Δk≈6.0 at all φ steps), NOT a φ-sampling offset.**
-    Metrics: Evidence-only loop. Per-φ traces captured for pixel (133, 134) across φ ∈ [0°, 0.1°] with 10 steps. C reports k_frac≈−3.857 (constant across all φ) while PyTorch reports k_frac≈−9.899 (varying −9.899 to −9.863). Δk≈6.042 at φ=0°, persists at all φ_tic=0…9. ΔF_latt_b ranges 0.27–1.91. First divergence occurs at φ_tic=0, ruling out φ-grid mismatch as the root cause.
+  * [2025-10-06] Attempt #45 (ralph loop) — Result: **EVIDENCE COMPLETE** (Phase K3e per-phi Miller index parity). **Root cause identified: fundamental lattice/geometry mismatch (delta k approx 6.0 at all phi steps), NOT a phi-sampling offset.**
+    Metrics: Evidence-only loop. Per-phi traces captured for pixel (133, 134) across phi ∈ [0°, 0.1°] with 10 steps. C reports k_fracapprox−3.857 (constant across all phi) while PyTorch reports k_fracapprox−9.899 (varying −9.899 to −9.863). delta k approx 6.042 at phi=0°, persists at all phi_tic=0…9. delta F_latt_b ranges 0.27–1.91. First divergence occurs at phi_tic=0, ruling out phi-grid mismatch as the root cause.
     Artifacts:
-      - `scripts/trace_per_phi.py` - Per-φ trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-φ trace (metadata + 10 φ steps)
+      - `scripts/trace_per_phi.py` - Per-phi trace generation tool (reuses production paths per CLAUDE.md Rule #0.3)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_pytorch_20251006-151228.json` - PyTorch per-phi trace (metadata + 10 phi steps)
       - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_summary_20251006-151228.md` - PyTorch trace summary table
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-φ trace (TRACE_C_PHI instrumentation)
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/per_phi_c_20251006-151228.log` - C per-phi trace (TRACE_C_PHI instrumentation)
       - `scripts/compare_per_phi_traces.py` - C vs PyTorch comparison tool
-      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with Δk table
-      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per φ step
+      - `reports/2025-10-cli-flags/phase_k/f_latt_fix/per_phi/comparison_summary.md` - Full comparison with deltak table
+      - `golden_suite_generator/nanoBragg.c:3156-3160` - Added TRACE_C_PHI instrumentation to log phi_tic, phi_deg, k_frac, F_latt_b, F_latt per phi step
     Observations/Hypotheses:
-      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all φ steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY φ rotation is applied.
-      - **φ-sampling ruled out:** The supervisor steering hypothesis of a φ-grid mismatch is incorrect. The 6-unit k offset exists at φ=0° where no rotation has occurred yet.
+      - **Critical finding:** C k_frac is CONSTANT (−3.857) across all phi steps while PyTorch k_frac VARIES (−9.899 → −9.863). This indicates the base reciprocal lattice vectors or scattering geometry differ fundamentally before ANY phi rotation is applied.
+      - **phi-sampling ruled out:** The supervisor steering hypothesis of a phi-grid mismatch is incorrect. The 6-unit k offset exists at phi=0° where no rotation has occurred yet.
       - **Likely root causes:** (1) MOSFLM matrix loading produces different a_star/b_star/c_star, (2) reciprocal→real conversion differs, or (3) scattering vector S calculation uses different geometry.
-      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before φ rotation) instead of φ-sampling adjustments.
-    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE φ rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm Δk<5e-4 at all φ steps.
+      - **Plan pivot required:** Phase K3f must now debug base lattice vectors (compare a_star/b_star/c_star from MOSFLM matrix loading, verify reciprocal→real formula, trace a/b/c before phi rotation) instead of phi-sampling adjustments.
+    Next Actions: **Phase K3f redirected** - Compare base lattice vectors between C and PyTorch: (1) Log MOSFLM a_star/b_star/c_star after matrix loading, (2) Trace reciprocal→real conversion (cell_a/b/c calculation), (3) Log a/b/c vectors BEFORE phi rotation, (4) Verify scattering vector S = (d - i)/λ uses identical geometry. Once base lattice parity achieved, regenerate scaling-chain memo and confirm deltak<5e-4 at all phi steps.
   * [2025-10-06] Attempt #46 (ralph) — Result: **EVIDENCE COMPLETE** (Phase K3f base lattice traces). **First divergence: reciprocal vectors scaled 40.51× too large.**
     Metrics: Reciprocal vector ratio (Py/C) = 40.514916 (a_star, b_star, c_star all identical), real vector ratio = ~405,149×, V_cell mismatch (C=24,682 vs Py=1.64e-9 with wrong units).
     Artifacts:
-      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + φ=0 scattering)
+      - `reports/2025-10-cli-flags/phase_k/base_lattice/c_stdout.txt` — C trace (291 lines, includes base vectors + phi=0 scattering)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/trace_py.log` — PyTorch trace (37 lines, matching structure)
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — Automated comparison showing first divergence
       - `reports/2025-10-cli-flags/phase_k/base_lattice/README.md` — Executive summary with root cause hypothesis
@@ -952,7 +952,7 @@
       - **MOSFLM matrix files already contain λ-scaled vectors:** C code shows `scaling factor = 1e-10/lambda0 = 1.02375`
       - **PyTorch may be re-scaling in `read_mosflm_matrix()` or `Crystal.compute_cell_tensors()`:** Need to verify whether function expects raw or pre-scaled input
       - **Real vector cascade:** 405,149× error in real vectors derives from reciprocal error via cross-product calculation
-      - **Miller index impact:** Massive lattice error explains the Δk≈6 mismatch from Phase K3e
+      - **Miller index impact:** Massive lattice error explains the delta k approx 6 mismatch from Phase K3e
     Next Actions (Phase K3f4):
       1. Investigate `src/nanobrag_torch/io/mosflm.py:read_mosflm_matrix()` — verify λ-scaling expectations
       2. Review `src/nanobrag_torch/models/crystal.py:compute_cell_tensors()` — check MOSFLM branch for double-scaling
@@ -960,14 +960,14 @@
       4. Test hypothesis: remove λ-scaling from reader output OR adjust compute_cell_tensors() to skip scaling when MOSFLM vectors provided
       5. Document chosen fix approach in base_lattice/README.md and update plan before implementing
   * [2025-11-08] Attempt #47 (galph) — Result: **EVIDENCE COMPLETE** (Phase K3f4 root-cause documentation). **MOSFLM rescale fix validated; cell tensors now match C.**
-    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (Δ=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
+    Metrics: PyTorch float64 CPU run reported V_cell=24682.256630 Å³ vs C 24682.3 (delta=4.3e-5), |a|=26.751388 Å, |b|=31.309964 Å, |c|=33.673354 Å (all <1.7e-6 relative); |a*|=0.042704 Å⁻¹ (matches C reciprocal magnitude).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt` — Inline `KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'` reproduction and outputs.
       - `reports/2025-10-cli-flags/phase_k/base_lattice/summary.md` — 2025-11-08 update capturing placeholder-volume root cause and commit 46ba36b remediation.
       - `tests/test_cli_scaling.py::TestMOSFLMCellVectors::test_mosflm_cell_vectors` — Regression test asserting |a|/|b|/|c| and V_cell within 5e-4 of C reference.
     Observations/Hypotheses:
       - MOSFLM branch now mirrors C pipeline (V_star dot, V_cell inversion, real-vector rebuild, reciprocal dual recompute).
-      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect Δh/Δk/Δl < 5e-4 once logs refreshed.
+      - Base lattice traces still show historical 40× deltas until the harness is rerun; expect deltah/deltak/delta l < 5e-4 once logs refreshed.
       - Normalization parity remains to be confirmed via Phase K3g3 scaling rerun.
     Next Actions: Execute Phase K3g3 — rerun `tests/test_cli_scaling.py::test_f_latt_square_matches_c` with `NB_RUN_PARALLEL=1`, refresh `phase_k/f_latt_fix/` scaling_chain artifacts + nb-compare, then regenerate `trace_py.log` so summary diff reflects corrected vectors before moving to Phase L.
   * [2025-10-06] Attempt #48 (galph) — Result: **PARITY FAILURE** (Phase K3g3 evidence collection). **MOSFLM rescale fix insufficient; correlation 0.174 indicates fundamental physics divergence.**
@@ -986,7 +986,7 @@
       - **Peak displacement (122–272 px)** means Bragg peaks appear in completely different locations, not just intensity scaling
       - **Dtype-independent error** rules out float32 precision; F_latt_b C=38.63 vs Py=2.33 (93.98% error) persists in float64
       - **MOSFLM rescale fix (commit 46ba36b) verified** via Attempt #47, but parity test shows it's insufficient
-      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (Δk≈6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
+      - **Hypotheses:** (1) Base lattice vectors still diverge despite rescale fix (K3f traces needed), (2) Fractional Miller indices misaligned (delta k approx 6.0 from K3e), (3) Additional geometry/physics bugs upstream of F_latt calculation
     Next Actions: **Phase K3f base-lattice traces REQUIRED** — The MOSFLM rescale implementation is incomplete or the divergence occurs in a different component. Capture fresh C baseline for lattice + scattering (K3f1), extend PyTorch trace harness (K3f2), diff traces to isolate first divergence (K3f3), document root cause (K3f4). Phase L parity sweep remains blocked until correlation >0.999.
   * [2025-10-06] Attempt #49 (ralph) — Result: **SUCCESS** (Phase K3g3 test isolation fix). **Fdump.bin contamination identified and resolved; scaling parity test now PASSES.**
     Metrics: Test `test_f_latt_square_matches_c` PASSED after removing stale Fdump.bin. CLI regression suite shows 31 passed (14 pass, 17 pre-existing failures unrelated to this work, 1 skipped). K3g3 exit criteria MET.
@@ -1028,7 +1028,7 @@
       - `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md` - Authoritative evidence report with dot-product derivation
     Observations/Hypotheses:
       - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
-      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeam≈-0.217596 m, Sbeam≈-0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
+      - **Dot-product derivation performed:** If override (-0.216336, 0.215206, -0.230201) were projected onto fdet/sdet, expected Fbeamapprox -0.217596 m, Sbeamapprox -0.212772 m. Actual C values: Fbeam=0.217889 m, Sbeam=0.215043 m. **Mismatch proves override not applied.**
       - **Precedence logic:** `if(custom_vectors) {compute_from_xbeam_ybeam(); ignore_override();} else if(override) {apply_override();} else {use_convention();}`
       - **PyTorch parity status:** Phase H3b2 (commit d6f158c) correctly skips override when custom vectors present, **matching C behavior exactly**
       - **1.14 mm pix0 delta root cause:** Not due to override precedence (that logic is correct). Must trace to different issue in Phase K normalization or Phase L parity sweep.
@@ -1048,14 +1048,14 @@
       - **Matches C behavior:** Precedence logic aligns with golden_suite_generator/nanoBragg.c
     Next Actions: Capture PyTorch traces (Phase H5c) to verify pix0/Fbeam/Sbeam/F_latt parity with C reference traces from 2025-10-22.
   * [2025-10-22] Attempt #32 (ralph loop) — Result: **EVIDENCE CAPTURED, PARITY GAP IDENTIFIED** (Phase H5c PyTorch trace). **pix0 divergence exceeds threshold despite revert.**
-    Metrics: pix0 ΔS = +1.393e-04 m (139 μm), ΔF = -1.136e-03 m (**1.1 mm**), ΔO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
+    Metrics: pix0 deltaS = +1.393e-04 m (139 μm), delta F = -1.136e-03 m (**1.1 mm**), deltaO = -5.594e-06 m. All three components exceed <5e-5 m threshold. Basis vectors (fdet, sdet) match C exactly.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.log` - PyTorch trace for pixel (1039, 685)
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/trace_py.stdout` - Full harness output
       - `reports/2025-10-cli-flags/phase_h5/py_traces/2025-10-22/diff_notes.md` - Component-by-component comparison
       - `reports/2025-10-cli-flags/phase_h5/parity_summary.md` - Updated with metrics table and analysis
     Observations/Hypotheses:
-      - **Primary finding:** pix0 divergence persists after revert, with ΔF = 1.1mm being most significant
+      - **Primary finding:** pix0 divergence persists after revert, with delta F = 1.1mm being most significant
       - **Basis vector parity:** fdet/sdet match C exactly (0.0 delta), ruling out orientation issues
       - **Pattern analysis:** Deltas show systematic bias (S: +139μm, F: -1136μm, O: -5.6μm), not random error
       - **Possible causes:**
@@ -1181,12 +1181,12 @@
       - **Root cause likely geometry/scattering vector error, NOT normalization:** Zero correlation proves outputs are fundamentally different diffraction patterns
       - PyTorch peaks at (1145, 2220) vs C peaks at (1039, 685) - 1538 pixel displacement
       - PyTorch produces sparse concentrated peaks (1.88% non-zero), C produces dense diffuse pattern (99.22% non-zero)
-      - Intensity ratio ≈256 suspicious (2⁸) but likely coincidental given geometry mismatch
+      - Intensity ratio approx 256 suspicious (2⁸) but likely coincidental given geometry mismatch
       - Steps normalization verified correct: `steps = 1×10×1×1 = 10` (line 831)
       - **Likely causes:** (1) pix0_vector override not applied, (2) CUSTOM basis vectors wrong, (3) SAMPLE pivot calculation error, (4) incident_beam_direction wrong, (5) Miller index/scattering vector bug
     Next Actions: **MANDATORY parallel trace comparison** per `docs/debugging/debugging.md` §2.1 - instrument C to print pix0_vector, incident_beam_direction, scattering_vector, h/k/l, F_cell, F_latt, omega for pixel (1039,685); generate matching PyTorch trace; compare line-by-line to find first divergence. Only after trace identifies root cause should implementation fixes begin.
   * [2025-10-06] Phase E Complete (ralph) — Result: success. **First divergence identified: pix0_vector mismatch at line 1 of trace.**
-    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px ≈ 264mm geometry shift).
+    Metrics: C trace (40 vars), PyTorch trace (40 vars). C pix0_vector = `-0.216475836, 0.216343050, -0.230192414` m; PyTorch pix0_vector = `-0.216336293, 0.215205512, -0.230200866` m. Y-component error = **1.14 mm** (explains 1535-pixel horizontal displacement: 1535px × 0.172mm/px approx 264mm geometry shift).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_e/c_trace.log` - C reference trace (40 TRACE_C lines)
       - `reports/2025-10-cli-flags/phase_e/pytorch_trace.log` - PyTorch trace (40 TRACE_PY lines)
@@ -1285,7 +1285,7 @@
       - 2025-10-17 supervisor audit: mark plan Phase F2 as partial until the MOSFLM transform and `distance_corrected` recomputation land.
     Next Actions: Superseded by Attempt #13 (Phase F2 completion). No additional follow-up under this attempt.
   * [2025-10-05] Attempt #12 (ralph) — Result: evidence-only (Phase F3 partial). **Parity validation reveals severe geometry divergence after detector fixes.**
-    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |Δ| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
+    Metrics: Correlation = -5.03e-06 (near zero, indicating completely different diffraction patterns). RMSE = 75.44. Max |delta| = 1.150e+05. C sum = 6.491e+03, PyTorch sum = 7.572e+05 (sum_ratio = 116.65×). Pytest collection: 624 items / 1 skipped (successful).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/img.bin` - C float image (24M, max_I=446.254)
       - `reports/2025-10-cli-flags/phase_f/parity_after_detector_fix/c_reference/command.log` - C execution log (7.9K)
@@ -1401,7 +1401,7 @@
     Next Actions: (1) Execute plan Phase H2 to investigate where Detector.beam_vector should feed simulator.incident_beam_direction; (2) Check if Simulator.__init__ should consume detector.beam_vector or if apply_custom_vectors() needs to set it; (3) Fix the beam vector propagation chain; (4) Re-run Phase H1 trace to verify incident_vec matches C; (5) With beam vector fixed, advance to Phase H3 (lattice structure factor parity).
   * [2025-10-05] Attempt #20 (ralph) — Result: success (Phase H2 complete). **Simulator now consumes detector.beam_vector for incident beam direction.**
   * [2025-10-05] Attempt #21 (ralph loop i=22) — Result: evidence-only (Phase H3 trace captured). **Incident beam vector parity achieved; lattice factor divergence isolated.**
-    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h Δ=0.097, k Δ=0.024, l Δ=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
+    Metrics: incident_vec perfect match (0.00051387949 0 -0.99999986 vs C 0.000513879494 0 -0.999999868). Miller indices diverge: h delta=0.097, k delta=0.024, l delta=0.120 (exceeds 1e-3 threshold). F_latt components critical: Py [-3.29, 10.74, -1.78] vs C [35.89, 38.63, 25.70]; product 62.68 vs C 35636 (568× difference). Pytest: test_custom_beam_vector_propagates 1/1 passed.
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.log` - PyTorch trace after beam fix (49 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H2.stderr` - Harness stderr
@@ -1416,7 +1416,7 @@
       - **Primary Suspect:** sincg() argument order in `src/nanobrag_torch/models/crystal.py` get_structure_factor()
         - C uses sincg(π·h, Na) pattern (nanoBragg.c:3063-3178)
   * [2025-10-06] Attempt #21 (evidence-only loop) — Result: success (Phase H3 evidence complete). **pix0 vector divergence root cause identified and quantified.**
-    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: Δh=0.00008, Δk=0.00003, Δl=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
+    Metrics: pix0 delta = 1.14 mm Y-component. Miller index deltas: deltah=0.00008, deltak=0.00003, delta l=0.0003 (within 1e-3 threshold but compound through sincg). F_latt component differences: 0.3-3.4%. Pytest collection: successful (all tests importable).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.log` - Fresh PyTorch trace (50 lines)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3_refresh.stderr` - Harness stderr (clean run)
@@ -1450,7 +1450,7 @@
       - No regression: All existing geometry and CLI tests pass
     Next Actions: (1) Re-run Phase H1 trace harness (`reports/2025-10-cli-flags/phase_h/trace_harness.py`) to verify `incident_vec` now matches C reference; (2) Once trace confirms beam vector parity, proceed to Phase H3 (diagnose lattice structure factor mismatch per plan); (3) After H3/H4 complete, execute Phase F3 parity rerun with full lattice+beam fixes in place.
   * [2025-10-06] Attempt #22 (ralph loop i=23) — Result: evidence-only (Phase H3 investigation complete). **sincg argument hypothesis REJECTED - Miller indices divergence is the root cause.**
-    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (Δ ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h Δ=0.097, k Δ=0.024, l Δ=0.120).
+    Metrics: Pytest collection 625 tests successful in 2.71s. Manual sincg validation proves both formulations (C: `sincg(π*h, Na)` and PyTorch: `sincg(π*(h-h0), Na)`) produce identical results (delta ~2.8e-12, float64 noise level). Trace comparison confirms F_latt components still diverge due to upstream Miller index calculation errors (h delta=0.097, k delta=0.024, l delta=0.120).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.log` - PyTorch trace after H2 beam vector fix (regenerated)
       - `reports/2025-10-cli-flags/phase_h/trace_py_after_H3.stderr` - Stderr from trace harness run
@@ -1458,7 +1458,7 @@
       - `reports/2025-10-cli-flags/phase_h/implementation_notes.md` - Updated with 2025-10-06 section rejecting sincg hypothesis and identifying Miller index divergence as root cause
       - `reports/2025-10-cli-flags/phase_h/pytest_collect.log` - Pytest collection log (625 tests, no breakage)
     Observations/Hypotheses:
-      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) ≈ sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
+      - **sincg Hypothesis REJECTED:** Manual calculation proves `sincg(π*2.001203, 36) approx sincg(π*0.001203, 36)` (both produce ~35.889, matching C expected value)
       - **ROOT CAUSE IDENTIFIED:** Miller indices (h,k,l) themselves differ between C and PyTorch implementations
         - C trace: h=2.001, k=1.993, l=-12.991
         - PyTorch: h=2.098, k=2.017, l=-12.871 (from earlier grep)
@@ -1504,8 +1504,8 @@
       - **Final I_pixel 11.4% higher:** Cascading effect from F_latt + polarization errors
       - **Steps/r_e²/fluence/omega perfect:** All normalization constants match exactly, confirming infrastructure is correct
     Next Actions: Phase K3 — Compare fractional Miller indices (h,k,l_frac) in C vs PyTorch traces to determine if F_latt_b error originates in Miller index calculation (scattering vector → reciprocal space) or in sincg function itself. Then debug polarization to apply Kahn factor correctly (reference PERF-PYTORCH-004 P3.0b fixes). Once both resolved, run targeted regression `pytest tests/test_cli_scaling.py::test_f_latt_square_matches_c -v` and proceed to Phase L final parity sweep.
-  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |ΔF|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
-    Metrics: Max |ΔF|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
+  * [2025-10-06] Attempt #50 (ralph) — Result: **PARITY FAILURE** (Phase L1 HKL ingestion evidence complete). **Critical discrepancy: PyTorch and C load HKL/Fdump data with significant numerical differences (max |delta F|=522 electrons, 99k mismatched voxels), but investigation reveals this is NOT a fundamental ingestion bug.**
+    Metrics: Max |delta F|=5.224e2 electrons (>> 1e-6 target), Relative RMS error=1.181 (>> 1e-8 target), mismatched voxels=99,209 (expect 0), metadata match ✅, shape match ✅. HKL has 64,333 non-zero entries, Fdump has 64,117 (216 fewer). Value ranges: both sources show min=0, max=522.35.
     Artifacts:
       - `scripts/validation/compare_structure_factors.py` — New validation script implementing HKL parity comparison per Phase L1 requirements
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251006175032.md` — Markdown evidence report with parity failure documentation
@@ -1527,9 +1527,9 @@
       1. Read the C code Fdump write logic (nanoBragg.c around "writing dump file") to understand exact array layout
       2. Compare with `src/nanobrag_torch/io/hkl.py` read_fdump() implementation to find the mismatch
       3. Add a targeted unit test that writes a small known HKL grid, generates Fdump via C, reads it back via PyTorch, and verifies exact value-for-value match
-      4. Fix the indexing bug and rerun this Phase L1 script to verify max |ΔF| < 1e-6
+      4. Fix the indexing bug and rerun this Phase L1 script to verify max |delta F| < 1e-6
       5. Only then proceed to Phase L2 scaling chain traces, as Fdump parity is a prerequisite for trustworthy structure factor comparisons
-  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |ΔF|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (≈9,534 more than header ranges) and consistent index shifts (Δk≈+10–12, Δl≈−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
+  * [2025-11-09] Attempt #51 (galph) — Result: **EVIDENCE UPDATE** (Phase L1a rerun). Regenerated the cache (`Fdump_scaled_20251006175946.bin`, size 1,461,622 bytes) via the supervisor command (ROI-trimmed) and re-executed `compare_structure_factors.py`. Metrics remain max |delta F|=5.22e2, RMS=5.10e1, mismatches=99,209, but inspection shows the binary encodes 182,700 doubles (approx 9,534 more than header ranges) and consistent index shifts (delta k approx+10–12, delta lapprox−2…−8). Artifacts: `reports/2025-10-cli-flags/phase_l/hkl_parity/c_fdump_20251006175946.log`, `run_20251006175946.log`, and the new cache alongside the original evidence. Recommended action: Phase L1b must reconcile C's dump layout with PyTorch IO before resuming scaling traces.
   * [2025-10-06] Attempt #52 (ralph) — Result: **SUCCESS** (Phase L1b complete). **Fdump layout fully characterized; C off-by-one allocation confirmed.**
     Metrics: Evidence-only loop. Generated fresh Fdump cache (`Fdump_scaled_20251006181401.bin`, SHA256=29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5, 1.4 MB). Analysis script found 0 mismatches (100% parity) after correcting for C's `(h_range+1) × (k_range+1) × (l_range+1)` allocation. Grid: h∈[-24,24], k∈[-28,28], l∈[-31,30] → 173,166 nominal voxels, but C writes 182,700 voxels (9,534 padding). Wall-clock: ~2s (cache gen), ~1s (analysis).
     Artifacts:
@@ -1547,7 +1547,7 @@
       - **Phase L1b Exit Criteria Met:** Layout documented with C line-number citations, index deltas captured in JSON, hypotheses confirmed via 100% parity after correcting reader logic.
     Next Actions: **Phase L1c required** — Update `src/nanobrag_torch/io/hkl.py::read_fdump()` to match C loop bounds (`range(h_range+1)` per line 131-132 of analysis script), update `write_fdump()` to replicate C writer exactly (lines 2484-2487), add regression test `test_hkl_fdump_equivalence` ensuring `read_hkl_file(scaled.hkl)` == `read_fdump(Fdump_scaled_*.bin)` for all (h,k,l) within bounds, then execute Phase L1d parity rerun and archive layout_analysis.md findings in plan notes.
   * [2025-10-17] Attempt #53 (ralph) — Result: **SUCCESS** (Phase L1c complete). **HKL/Fdump IO aligned with C binary format; perfect roundtrip parity achieved.**
-    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |ΔF| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
+    Metrics: HKL I/O tests 18/18 passed (test_at_io_003.py, test_at_io_004.py, test_at_str_004.py). New parity test TestHKLFdumpParity::test_scaled_hkl_roundtrip passed with max |delta F| = 0.0. Roundtrip: scaled.hkl → PyTorch grid (49×57×62) → Fdump.bin (182,700 doubles with padding) → PyTorch grid (49×57×62) with zero error.
     Artifacts:
       - `src/nanobrag_torch/io/hkl.py:86-107` - Fixed h_range calculation to C-style (h_max - h_min + 1), updated grid allocation from (range+1)³ to range³ for in-memory representation
       - `src/nanobrag_torch/io/hkl.py:131-183` - Rewrote write_fdump() to allocate padded (range+1)³ array matching C writer loop (nanoBragg.c:2484-2490)
@@ -1559,11 +1559,11 @@
       - **Root Cause (Fixed):** PyTorch calculated h_range = h_max - h_min (span) while C uses h_range = h_max - h_min + 1 (count). This caused PyTorch to allocate 49×57×62=173,166 elements while C writes 50×58×63=182,700. The mismatch created systematic offsets in all structure factors.
       - **C Range Semantics:** nanoBragg.c:2405 shows `h_range = h_max - h_min + 1` giving the NUMBER of h values, then line 2427 allocates `(h_range+1)` for padding, and line 2484 writes via `for(h0=0; h0<=h_range; h0++)` (h_range+1 iterations).
       - **PyTorch Solution:** Changed range calculation to match C, kept in-memory grid at range³ (no padding) for efficiency, padded only during write_fdump(), trimmed padding in read_fdump().
-      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |ΔF| = 0.0 and zero mismatches (100% parity).
+      - **Parity Evidence:** Roundtrip test loads scaled.hkl (64,333 reflections), reads C-generated Fdump (182,700 doubles), achieves max |delta F| = 0.0 and zero mismatches (100% parity).
       - **Phase L1c Exit Criteria Met:** read_fdump/write_fdump match C binary layout exactly, regression test proves equivalence, all existing HKL I/O tests pass.
-    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |ΔF| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
+    Next Actions: **Phase L1d required** — Regenerate Fdump.bin via PyTorch write_fdump(), load with scripts/validation/compare_structure_factors.py, verify max |delta F| ≤ 1e-6 and zero mismatches against both C cache and HKL text, then proceed to Phase L2 scaling traces.
   * [2025-10-17] Attempt #54 (ralph) — Result: **SUCCESS** (Phase L1d complete). **HKL parity verified with fresh C cache; perfect structure-factor match achieved.**
-    Metrics: Max |ΔF|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
+    Metrics: Max |delta F|=0.0 electrons, relative RMS error=0.0, 0 mismatched voxels. Metadata match ✅, Shape match ✅ ([49, 57, 62]). Value ranges identical (min=0.0, max=522.35 electrons). Test collection confirmed `tests/test_cli_flags.py::TestHKLFdumpParity::test_scaled_hkl_roundtrip` collects successfully (1 test in 2.12s).
     Artifacts:
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_c_20251109.bin` - Fresh C cache generated from supervisor command (1,461,622 bytes, SHA256: 29a427209e14ce2f1fc8e4f5f61eacc194f6c4cb1f5c6a6ca20bf19c244978c5)
       - `reports/2025-10-cli-flags/phase_l/hkl_parity/summary_20251109.md` - Parity validation summary (64 lines, status: ✅ PASS)
@@ -1574,7 +1574,7 @@
       - **Phase L2 prerequisites met:** Structure-factor grids now identical between C and PyTorch; any remaining intensity delta must originate from scaling chain, not F lookup
       - **HKL file unchanged:** SHA256 65b668b3fa5c4de14c9049d9262945453d982ff193b572358846f38b173855ba (same as prior runs)
       - **C code honoured layout:** 49×57×62 shape confirms C binary respects (range+1)³ padding format discovered in Phase L1b
-    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-φ traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
+    Next Actions: **Phase L2 required** — Proceed to scaling-chain diagnostics with refreshed structure factors. Run per-phi traces comparing C vs PyTorch scaling factors (steps, r_e², fluence, omega, polarization) using instrumented supervisor command. Expected outcome: intensity sum_ratio 0.99–1.01 once all factors align.
   * [2025-10-17] Attempt #55 (ralph) — Result: **EVIDENCE COMPLETE** (Phase L2a C scaling trace capture). **Instrumentation already present; C trace successfully captured for pixel (685,1039).**
     Metrics: Evidence-only loop. C runtime: ~1s. Trace pixel (spixel=685, fpixel=1039) selected for max_I location from Phase I. No additional C code changes needed; instrumentation lines 3367-3382 already comprehensive.
     Artifacts:
@@ -1584,7 +1584,7 @@
       - **Instrumentation already complete:** C code at lines 3367-3382 logs I_before_scaling, r_e_sqr, fluence, steps, oversample_*, capture_fraction, polar, omega_pixel, I_pixel_final
       - **Scaling formula verified:** Line 3358 implements `test = r_e_sqr * fluence * I / steps` followed by last-value corrections (lines 3361-3363)
       - **Key values for pixel (685,1039):** I_before_scaling=943654.809, r_e_sqr=7.941e-30 m², fluence=1e24 photons/m², steps=10, capture_fraction=1.0, polar=0.9146, omega_pixel=4.204e-7 sr, I_pixel_final=2.881e-7 photons
-      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 ≈ 2.881e-7 ✓ (matches trace)
+      - **Manual verification:** base=(7.941e-30 × 1e24 × 943654.809 / 10)=7.495e-7, with corrections: final=base × 1.0 × 0.9146 × 4.204e-7 approx 2.881e-7 ✓ (matches trace)
       - **Trace pixel selection rationale:** Pixel (1039,685) had max_I=446.254 in Phase I supervisor run; (spixel,fpixel) convention in C is (685,1039)
       - **No recompilation needed:** Used existing golden_suite_generator/nanoBragg binary; `-trace_pixel` CLI flag activates instrumentation at runtime
     Next Actions: **Phase L2b required** — Create PyTorch trace harness at `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` that logs identical quantities (I_before_scaling, omega, polar, capture_fraction, steps, r_e², fluence, I_pixel_final) for same pixel. Execute with supervisor command, store as `trace_py_scaling.log`, then proceed to Phase L2c diff analysis.
@@ -1610,13 +1610,13 @@
       - `src/nanobrag_torch/simulator.py:1050-1080` - Cache capture_fraction by computing ratio before/after _apply_detector_absorption when trace_pixel is set
       - `src/nanobrag_torch/simulator.py:1127-1171` - Recompute polarization for trace pixel using polarization_factor() from utils/physics
       - `src/nanobrag_torch/simulator.py:1177-1199` - Extended _apply_debug_output() signature to accept steps and capture_fraction parameters
-      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×φ×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
+      - `src/nanobrag_torch/simulator.py:1369-1398` - Updated TRACE_PY output to print actual values: steps (sources×mosaic×phi×oversample²), capture_fraction (per-pixel absorption), polar (Kahn factor)
       - `tests/test_trace_pixel.py` - New regression test suite (197 lines, 2 test methods with parametrization covering cpu/cuda × float32/float64)
     Observations/Hypotheses:
       - **Implementation Strategy:** For trace_pixel only, recomputed polarization separately (matching compute_physics_for_position calculation) to avoid breaking torch.compile caching; cached capture_fraction as before/after ratio from _apply_detector_absorption; threaded actual steps value through _apply_debug_output
       - **Device/Dtype Neutrality Maintained:** No .cpu()/.item() calls in differentiable paths; all trace computations reuse tensors on original device/dtype before extracting .item() for logging
       - **Backward Compatibility:** steps parameter defaults to None with fallback to phi_steps only; capture_fraction defaults to 1.0 when absorption not configured; polarization defaults to 1.0 when nopolar is set
-      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×φ×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
+      - **Test Coverage:** test_scaling_trace_matches_physics validates steps calculation matches expected value (sources×mosaic×phi×oversample²), capture_fraction is 1.0 when no absorption configured; test_scaling_trace_with_absorption validates capture_fraction < 1.0 when absorption enabled
     Next Actions: Execute Phase L2b Step 3 - run updated trace harness (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py`) to capture `trace_py_scaling.log` for pixel (685,1039) using supervisor command parameters, then proceed to Phase L2c diff analysis.
   * [2025-10-17] Attempt #58 (ralph loop i=64) — Result: **PHASE L2b COMPLETE** (evidence-only pass). **PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.**
     Metrics: PyTorch steps=160 vs C steps=10 (16× auto-oversample factor); fluence=1.26e+29 vs C=1e+24 (~100,000× error); polar=1.0 vs C=0.9146 (9.3% error); omega=4.18e-7 vs C=4.20e-7 (0.56%, within tolerance); r_e_sqr match within 0.0003%.
@@ -1630,8 +1630,8 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_audit/pytest_collect_final.log` - Test collection check (4 tests collected)
     Observations/Hypotheses:
       - **First Divergence: Fluence calculation** - PyTorch fluence is ~100,000× higher than C. Calculation check suggests beamsize unit error (treated as 1e-5 m instead of 1 mm).
-      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 φ × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
-      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polar≈0.508 for cos(2θ)=-0.130070.
+      - **Steps normalization mismatch** - PyTorch auto-selects 4× oversample when none specified, yielding steps=160 (10 phi × 1 mosaic × 4²). C defaults to oversample=1, yielding steps=10. This creates an additional 16× dimming factor.
+      - **Polarization bypass** - PyTorch reports polar=1.0 instead of applying Kahn formula. Even with K=0, geometric term should yield polarapprox 0.508 for cos(2θ)=-0.130070.
       - **Previous L2b blocker resolved** - Ralph's earlier attempt (#56, documented in `L2b_blocker.md`) failed because the trace harness tried to construct Simulator directly from configs. Current approach uses the CLI with `-trace_pixel` flag, which handles all object construction internally.
       - **Trace support already implemented** - The `Simulator` class already emits TRACE_PY lines when `debug_config['trace_pixel']` is set (see `simulator.py:1243-1410`). No production code changes needed for evidence gathering.
     Next Actions:
@@ -1652,7 +1652,7 @@
     Observations/Hypotheses:
       - **Fluence/Steps Parity Achieved**: With `-beam_vector`, `-oversample 1`, `-flux 1e18`, and `-beamsize 1.0` flags now present in harness config, PyTorch matches C for fluence and steps (resolving Attempt #58 divergences).
       - **Placeholder Scaling Values**: Current trace harness emits hardcoded placeholders for `polar` (0.0), `omega_pixel` (4.20412684465831e-07), and `I_before_scaling` (NOT_EXTRACTED) because it cannot access internal Simulator state without instrumentation. These must be fixed per Phase L2b proper (pending instrumentation work).
-      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing Δk≈6.04 units).
+      - **Zero Final Intensity**: I_pixel_final=0 indicates upstream physics divergence (likely k_frac mismatch documented in Phase K3e evidence showing delta k approx 6.04 units).
       - **Harness Fix Applied**: Fixed bug where harness passed DetectorConfig/CrystalConfig objects to Simulator.__init__ instead of Detector/Crystal instances, causing AttributeError on `self.detector.beam_vector`.
       - **Input.md "Do Now" Satisfied**: Executed rerun command with full supervisor flag set; confirmed beam_vector, oversample, flux, and beamsize parameters are now present and match C trace exactly.
     Next Actions:
@@ -1716,7 +1716,7 @@
 - First Divergence (if known): Metric duality violation (V_actual vs formula volume) at `Crystal.compute_cell_tensors`
 - Attempts History:
   * [2025-09-30] Attempt #16 — Result: success. Restored Core Rule #13 exact duality (V_actual) and tightened tolerances back to 1e-12.
-    Metrics: corr≈0.99963; metric duality matches within 1e-12.
+    Metrics: corr approx 0.99963; metric duality matches within 1e-12.
     Artifacts: reports/2025-09-30-AT-012-debug/ (rotation traces).
     Observations/Hypotheses: Formula-volume shortcut caused 0.9658 correlation; V_actual fix resolves.
     Next Actions: Monitor while peak-match work proceeds.
@@ -1902,7 +1902,7 @@
 - Reproduction (C & PyTorch):
   * C: `"$NB_C_BIN" -mat A.mat -floatfile c_abs.bin -hkl scaled.hkl -nonoise -nointerpolate -oversample 2 -oversample_thick -detector_abs 320 -detector_thick 500 -detector_thicksteps 5 -distance 231.274660 -lambda 0.9768 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527` (restrict ROI in plan to keep runtime manageable).
   * PyTorch: `KMP_DUPLICATE_LIB_OK=TRUE nanoBragg --config supervisor.yaml --oversample 2 --oversample-thick` with ROI matching C run.
-  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI ≈ 64×64 around a Bragg peak.
+  * Shapes/ROI: Oversample 2, oblique detector vectors from supervisor command, ROI approx 64×64 around a Bragg peak.
 - First Divergence (if known): PyTorch intensity identical whether `oversample_thick` is enabled; C nanoBragg increases capture fraction on oblique pixels by ≥3%, confirming PyTorch path skips per-subpixel geometry.
 - Next Actions:
   1. Execute plan Phase A (A1–A3) to capture failing pytest evidence, C comparison, and contract summary under `reports/2025-11-oversample-thick/`.
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:12] thinking

**Reverting unintended widespread changes**

I want to undo the many accidental replacements that spread beyond the intended CLI-FLAGS section—especially those messing with symbols like “approx” and causing large diffs. My plan is to revert the docs/fix_plan.md file to its original state and then carefully reapply just the specific, necessary changes using the original non-ASCII symbols like phi and delta to keep readability intact while minimizing impact on unrelated text.
[2025-10-07T10:07:14] exec bash -lc 'git checkout -- docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:07:15] bash -lc 'git checkout -- docs/fix_plan.md' succeeded in 895ms:

[2025-10-07T10:07:15] tokens used: 255,543
[2025-10-07T10:07:15] turn diff:

diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:19] thinking

**Reapplying targeted fixes carefully**
[2025-10-07T10:07:20] exec bash -lc 'rg -n "Next Actions (2025-11-19 evidence refresh)" docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:07:20] bash -lc 'rg -n "Next Actions (2025-11-19 evidence refresh)" docs/fix_plan.md' exited 1 in 706ms:

[2025-10-07T10:07:20] tokens used: 256,397
[2025-10-07T10:07:20] turn diff:

diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:23] thinking

**Reviewing previous HEAD text snippet**
[2025-10-07T10:07:24] exec bash -lc "sed -n '450,520p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:07:25] bash -lc "sed -n '450,520p' docs/fix_plan.md" succeeded in 722ms:
## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
- Priority: High
- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
- Owner/Date: ralph/2025-10-05
- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
- Reproduction (C & PyTorch):
  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
- Next Actions (2025-11-19 evidence refresh):
  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
- Attempts History:
  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
    Artifacts:
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log:1-41` - Refreshed trace with live values (polar=0.91464138, omega_pixel=4.20404859e-07 sr, F_cell=0, I_before_scaling=0)
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1-54` - Phase L2c comparison report showing first divergence and all divergent factors
[2025-10-07T10:07:25] tokens used: 256,588
[2025-10-07T10:07:25] turn diff:

diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:27] thinking

**Updating next actions with new tasks**
[2025-10-07T10:07:31] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -459,6 +459,6 @@
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
[2025-10-07T10:07:31] apply_patch(auto_approved=true) exited 0 in 36ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T10:07:31] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..fefb19a6c88e717cae807007649aea4bb43282a6
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:07:31] tokens used: 259,543
[2025-10-07T10:07:31] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..fefb19a6c88e717cae807007649aea4bb43282a6
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:36] thinking

**Locating attempt #86 block**
[2025-10-07T10:07:37] exec bash -lc 'rg -n "Attempt #86" -n docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:07:38] bash -lc 'rg -n "Attempt #86" -n docs/fix_plan.md' succeeded in 996ms:
617:  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
[2025-10-07T10:07:38] tokens used: 260,566
[2025-10-07T10:07:38] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..fefb19a6c88e717cae807007649aea4bb43282a6
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:41] thinking

**Updating observation bullets**
[2025-10-07T10:07:45] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -627,6 +627,6 @@
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
[2025-10-07T10:07:45] apply_patch(auto_approved=true) exited 0 in 32ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T10:07:45] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:07:46] tokens used: 261,489
[2025-10-07T10:07:46] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:50] thinking

**Verifying planned documentation updates**
[2025-10-07T10:07:51] exec bash -lc 'git diff docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:07:52] bash -lc 'git diff docs/fix_plan.md' succeeded in 925ms:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c..548ef94 100644
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
[2025-10-07T10:07:52] tokens used: 262,468
[2025-10-07T10:07:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:55] thinking

**Verifying diff cleanliness**
[2025-10-07T10:07:55] exec bash -lc 'git diff plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:07:56] bash -lc 'git diff plans/active/cli-noise-pix0/plan.md' succeeded in 749ms:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179..4cb8f8f 100644
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@ Exit Criteria: PyTorch source changes with inline C-code docstring references ar
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
[2025-10-07T10:07:56] tokens used: 264,151
[2025-10-07T10:07:56] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:07:59] thinking

**Reviewing documentation updates**
[2025-10-07T10:07:59] exec bash -lc 'git status --short' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:00] bash -lc 'git status --short' succeeded in 754ms:
 M docs/fix_plan.md
 M input.md
 M plans/active/cli-noise-pix0/plan.md
[2025-10-07T10:08:00] tokens used: 266,275
[2025-10-07T10:08:00] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:04] thinking

**Staging and preparing commit**
[2025-10-07T10:08:05] exec bash -lc 'git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:06] bash -lc 'git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md input.md' succeeded in 791ms:

[2025-10-07T10:08:06] tokens used: 266,574
[2025-10-07T10:08:06] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:13] thinking

**Committing with descriptive message**
[2025-10-07T10:08:14] exec bash -lc 'git commit -m "SUPERVISOR: CLI-FLAGS plan refresh - tests: not run"' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:14] bash -lc 'git commit -m "SUPERVISOR: CLI-FLAGS plan refresh - tests: not run"' succeeded in 791ms:
[feature/spec-based-2 80ab804] SUPERVISOR: CLI-FLAGS plan refresh - tests: not run
 3 files changed, 73 insertions(+), 101 deletions(-)
[2025-10-07T10:08:14] tokens used: 266,724
[2025-10-07T10:08:14] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:17] thinking

**Preparing to push changes**
[2025-10-07T10:08:17] exec bash -lc 'git push' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:20] bash -lc 'git push' succeeded in 2.42s:
To github.com:hoidn/nanoBragg.git
   09d2ee2..80ab804  feature/spec-based-2 -> feature/spec-based-2
[2025-10-07T10:08:20] tokens used: 266,904
[2025-10-07T10:08:20] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:22] thinking

**Updating galph_memory.md with summary**
[2025-10-07T10:08:25] exec bash -lc "cat <<'EOF' >> galph_memory.md
## 2025-11-19 (galph loop — CLI rotation audit setup)
- Updated CLI-FLAGS-003 next actions + plan Phase L3 to reflect Attempt #86 per-phi evidence and new φ=0 rotation mismatch; L3e marked done, new tasks L3f (rot-vector comparison) and L3g (hypothesis framing) added.
- Input.md now directs Ralph to capture rotation vectors via trace_harness, build "'`rot_vector_comparison.md`, and log hypotheses before touching simulator code.
- Key observation logged: PyTorch rot_b at φ=0 uses k_frac ≈ -0.589 vs C -0.607, flipping F_latt_b sign; rotation-vector audit required before normalization changes.

EOF' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:26] bash -lc "cat <<'EOF' >> galph_memory.md
## 2025-11-19 (galph loop — CLI rotation audit setup)
- Updated CLI-FLAGS-003 next actions + plan Phase L3 to reflect Attempt #86 per-phi evidence and new φ=0 rotation mismatch; L3e marked done, new tasks L3f (rot-vector comparison) and L3g (hypothesis framing) added.
- Input.md now directs Ralph to capture rotation vectors via trace_harness, build "'`rot_vector_comparison.md`, and log hypotheses before touching simulator code.
- Key observation logged: PyTorch rot_b at φ=0 uses k_frac ≈ -0.589 vs C -0.607, flipping F_latt_b sign; rotation-vector audit required before normalization changes.

EOF' succeeded in 712ms:

[2025-10-07T10:08:26] tokens used: 267,234
[2025-10-07T10:08:26] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:29] thinking

**Replacing non-ASCII characters**
[2025-10-07T10:08:30] exec bash -lc "python - <<'PY'
from pathlib import Path
path = Path('galph_memory.md')
text = path.read_text()
text = text.replace('φ', 'phi')
path.write_text(text)
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:31] bash -lc "python - <<'PY'
from pathlib import Path
path = Path('galph_memory.md')
text = path.read_text()
text = text.replace('φ', 'phi')
path.write_text(text)
PY" succeeded in 773ms:

[2025-10-07T10:08:31] tokens used: 267,648
[2025-10-07T10:08:31] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:34] thinking

**Checking git status and considering commits**
[2025-10-07T10:08:35] exec bash -lc 'git status --short' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:35] bash -lc 'git status --short' succeeded in 754ms:
 M galph_memory.md
[2025-10-07T10:08:35] tokens used: 267,964
[2025-10-07T10:08:35] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:43] thinking

**Preparing new commit for memory updates**
[2025-10-07T10:08:44] exec bash -lc 'git add galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:45] bash -lc 'git add galph_memory.md' succeeded in 755ms:

[2025-10-07T10:08:45] tokens used: 268,184
[2025-10-07T10:08:45] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

[2025-10-07T10:08:48] exec bash -lc 'git commit -m "SUPERVISOR: log rotation audit prep - tests: not run"' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:48] bash -lc 'git commit -m "SUPERVISOR: log rotation audit prep - tests: not run"' succeeded in 699ms:
[feature/spec-based-2 24e490b] SUPERVISOR: log rotation audit prep - tests: not run
 1 file changed, 21 insertions(+), 16 deletions(-)
[2025-10-07T10:08:48] tokens used: 268,380
[2025-10-07T10:08:48] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:51] thinking

**Preparing to push changes**
[2025-10-07T10:08:51] exec bash -lc 'git push' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:08:54] bash -lc 'git push' succeeded in 2.80s:
To github.com:hoidn/nanoBragg.git
   80ab804..24e490b  feature/spec-based-2 -> feature/spec-based-2
[2025-10-07T10:08:54] tokens used: 268,563
[2025-10-07T10:08:54] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:08:57] thinking

**Preparing final summary**
[2025-10-07T10:08:59] exec bash -lc 'git status --short' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:09:00] bash -lc 'git status --short' succeeded in 729ms:

[2025-10-07T10:09:00] tokens used: 268,730
[2025-10-07T10:09:00] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:09:08] thinking

**Preparing final update message**
[2025-10-07T10:09:09] exec bash -lc "nl -ba docs/fix_plan.md | sed -n '450,470p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:09:10] bash -lc "nl -ba docs/fix_plan.md | sed -n '450,470p'" succeeded in 722ms:
   450	## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
   451	- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
   452	- Priority: High
   453	- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
   454	- Owner/Date: ralph/2025-10-05
   455	- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
   456	- Reproduction (C & PyTorch):
   457	  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   458	  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
   459	- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
   460	- Next Actions (2025-11-19 refresh):
   461	  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
   462	  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
   463	  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
   464	- Attempts History:
   465	  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
   466	    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
   467	    Artifacts:
   468	      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log:1-41` - Refreshed trace with live values (polar=0.91464138, omega_pixel=4.20404859e-07 sr, F_cell=0, I_before_scaling=0)
   469	      - `reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1-54` - Phase L2c comparison report showing first divergence and all divergent factors
[2025-10-07T10:09:10] tokens used: 270,038
[2025-10-07T10:09:10] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:09:13] thinking

**Adding line numbers for reference**
[2025-10-07T10:09:14] exec bash -lc "nl -ba docs/fix_plan.md | sed -n '612,626p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:09:15] bash -lc "nl -ba docs/fix_plan.md | sed -n '612,626p'" succeeded in 743ms:
   612	      - **Structure factors now aligned:** `F_cell=190.270004272461` in TRACE_PY (`trace_py_scaling_20251117.log:26`) matches C within 2.4e-6, proving HKL ingestion fixes held
   613	      - **First divergence moves downstream:** Raw accumulation gap is driven by lattice factor parity — C logs `F_latt=-2.3832` (`c_trace_scaling.log:288`) while PyTorch reports `F_latt=+1.35136` (`trace_py_scaling_20251117.log:24`)
   614	      - **Per-φ evidence missing:** C trace shows oscillating `TRACE_C_PHI` entries (lines 280-287) that sum to the negative lattice factor; the current PyTorch artifact (`reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_per_phi.log`) contains no `TRACE_PY_PHI` lines despite simulator hooks, so the harness/instrumentation needs to be rerun or repaired
   615	      - **Downstream scalars validated:** Fluence, steps, capture_fraction, polarization, omega remain within L3e tolerances, narrowing focus to lattice accumulation
   616	    Next Actions: Instrument per-φ `TRACE_PY_PHI` logging for `_compute_structure_factor_components`, compare against `TRACE_C_PHI` (same pixel), and update plan tasks L3e/L3f before touching simulator code.
   617	  * [2025-10-07] Attempt #86 (ralph loop) — Result: **SUCCESS** (Phase L3e per-φ scaling validation COMPLETE per input.md "Do Now"). **Trace harness regenerated with 10 TRACE_PY_PHI entries, scaling comparison rerun with 2025-11-19 artifacts, pytest instrumentation test passes 4/4 variants.**
   618	    Metrics: Harness captured 10 TRACE_PY_PHI lines (φ=0° to 0.1°) + 40 TRACE_PY lines; comparison script identifies I_before_scaling as first divergence (C=943654.809, PyTorch=713431, -24.4% rel); 5 divergent factors total (I_before_scaling, polar, omega_pixel, cos_2theta, I_pixel_final); r_e², fluence, steps, capture_fraction all PASS (≤1e-6 tolerance). Pytest 4/4 passed in 7.86s.
   619	    Artifacts:
   620	      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251119.log` — Fresh main trace (40 lines, F_cell=190.27, I_before_scaling=713431)
   621	      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.log` — Per-φ trace (10 TRACE_PY_PHI entries)
   622	      - `reports/2025-10-cli-flags/phase_l/per_phi/trace_py_scaling_20251119_per_phi.json` — Structured per-φ JSON with phi_tic, phi_deg, k_frac, F_latt_b, F_latt
   623	      - `reports/2025-10-cli-flags/phase_l/scaling_validation/scaling_validation_summary_20251119.md` — Phase L3e markdown summary with ≤1e-6 tolerance enforcement
   624	      - `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json` — Structured per-factor metrics with first_divergence="I_before_scaling"
   625	      - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
   626	    Observations/Hypotheses:
[2025-10-07T10:09:15] tokens used: 271,290
[2025-10-07T10:09:15] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:09:20] thinking

**Requesting additional lines**
[2025-10-07T10:09:22] exec bash -lc "nl -ba docs/fix_plan.md | sed -n '626,636p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:09:23] bash -lc "nl -ba docs/fix_plan.md | sed -n '626,636p'" succeeded in 878ms:
   626	    Observations/Hypotheses:
   627	      - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
   628	      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
   629	      - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
   630	      - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
   631	    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   632	  * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
   633	    Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
   634	    Artifacts:
   635	      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:233-236` — Fixed HKL attachment (crystal.hkl_data = F_grid_tensor, crystal.hkl_metadata = hkl_metadata)
   636	      - `reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_fixed.txt` — Post-fix verification showing hkl_data/metadata properly attached
[2025-10-07T10:09:23] tokens used: 272,611
[2025-10-07T10:09:23] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:09:26] thinking

**Seeking line numbers for plan**
[2025-10-07T10:09:31] exec bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '20,40p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:09:31] bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '20,40p'" succeeded in 730ms:
    20	- Gap snapshot update (2025-11-17 late): Attempt #78 completed the Phase L3c harness rerun; TRACE_PY now reports F_cell=190.27 with matching metadata, so the immediate focus moves to auditing CLI ingestion (L3c) and adding regression coverage (L3d).
    21	- Gap snapshot refresh (2025-11-18): Attempt #80 (evidence loop) closed Phase L3c — `cli_hkl_audit.md` now documents the missing `.to(device=device)` transfer at `__main__.py:1073`, confirmed probe artifacts, and queued the device fix + regression test for L3d.
    22	- Gap snapshot update (2025-11-18 late): Attempt #81 implemented `.to(device=device, dtype=dtype)` and added the `TestHKLDevice::test_hkl_tensor_respects_device` regression (CPU/CUDA × float32/float64). Next blocker: Phase L3e scaling validation to prove all factors match C (≤1e-6) before rerunning nb-compare in Phase L4.
    23	- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
    24	- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
    25	- Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
    26	- Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
    27	- Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
    28	- Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
    29	- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
    30	- Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
    31	- Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
    32	- Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
    33	- Evidence status: Phase E artifacts (`reports/2025-10-cli-flags/phase_e/`) hold C/PyTorch traces, diffs, and beam-vector checks. Phase H3b1 also stashes WITH/without override traces under `reports/2025-10-cli-flags/phase_h/implementation/` for reference.
    34	- Documentation anchors for this focus:
    35	  * `specs/spec-a-core.md` — canonical lattice-factor formulas (SQUARE uses sincg(π·h, Na); ROUND/GAUSS definitions used to verify Phase K).
    36	  * `golden_suite_generator/nanoBragg.c` (≈2993-3151) — definitive implementation of the sincg calls and rounding logic.
    37	  * `docs/architecture/pytorch_design.md` — simulator reference; docstring currently claims full-h usage and must be updated post-fix.
    38	  * `docs/development/testing_strategy.md` — authoritative pytest/trace commands for CLI parity evidence.
    39	
[2025-10-07T10:09:31] tokens used: 273,484
[2025-10-07T10:09:31] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:09:36] thinking

**Preparing lines for table**
[2025-10-07T10:09:37] exec bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '258,270p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:09:38] bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '258,270p'" succeeded in 772ms:
   258	| ID | Task Description | State | How/Why & Guidance |
   259	| --- | --- | --- | --- |
   260	| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
   261	| L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
   262	| L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
   263	| L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
   264	| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
   265	| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
   266	| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
   267	| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
   268	
   269	### Phase L4 — Supervisor Command Parity Rerun
   270	Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.
[2025-10-07T10:09:38] tokens used: 276,335
[2025-10-07T10:09:38] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:09:43] thinking

**Considering documentation updates**
[2025-10-07T10:09:43] exec bash -lc 'nl -ba galph_memory.md | tail -n 10' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T10:09:44] bash -lc 'nl -ba galph_memory.md | tail -n 10' succeeded in 764ms:
  1564	- Reviewed Phase L3 evidence: `analysis_20251119.md` still shows F_cell parity yet `trace_py_scaling_per_phi.log` lacks any `TRACE_PY_PHI`, indicating the harness output is stale.
  1565	- Updated `docs/fix_plan.md` next actions to call for a 2025-11-19 per-phi trace rerun and noted the empty log under Attempt #83 observations.
  1566	- Refreshed `plans/active/cli-noise-pix0/plan.md` gap snapshot and revised the L3e task description to emphasise regenerating per-phi artifacts before scaling validation can pass.
  1567	- Authored new `input.md` (Parity mode) instructing Ralph to rerun `trace_harness.py` with `--out trace_py_scaling_20251119.log`, regenerate per-phi comparison data, rerun `compare_scaling_traces.py`, and capture the targeted pytest output.
  1568	- Follow-up: expect refreshed logs/JSON under `reports/2025-10-cli-flags/phase_l/per_phi/`, updated scaling_validation metrics, and a docs/fix_plan.md attempt summarising whether the Δk≈6 offset persists.
  1569	## 2025-11-19 (galph loop — CLI rotation audit setup)
  1570	- Updated CLI-FLAGS-003 next actions + plan Phase L3 to reflect Attempt #86 per-phi evidence and new phi=0 rotation mismatch; L3e marked done, new tasks L3f (rot-vector comparison) and L3g (hypothesis framing) added.
  1571	- Input.md now directs Ralph to capture rotation vectors via trace_harness, build `rot_vector_comparison.md`, and log hypotheses before touching simulator code.
  1572	- Key observation logged: PyTorch rot_b at phi=0 uses k_frac ≈ -0.589 vs C -0.607, flipping F_latt_b sign; rotation-vector audit required before normalization changes.
  1573	
[2025-10-07T10:09:44] tokens used: 277,487
[2025-10-07T10:09:44] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.


[2025-10-07T10:09:51] thinking

**Composing final message with summary and next steps**

I'll assemble the final response starting with a brief explanation, then add a bullet list pointing to relevant files and line ranges, and finish with natural next steps including a numeric list. I'll also note that tests haven't been run yet to keep things transparent.
[2025-10-07T10:09:52] codex

Refocused the CLI parity effort on the φ=0 rotation mismatch and handed Ralph concrete tracing work.

- docs/fix_plan.md:460-463 — Next actions now call for the rot-vector audit (L3f) and hypothesis write-up (L3g) before the parity rerun.  
- docs/fix_plan.md:626-631 — Attempt #86 observations updated to flag the rot_b_y drift and to point Ralph at the new rot_vector artifact queue.  
- plans/active/cli-noise-pix0/plan.md:29-267 — Gap snapshot cites the k_frac sign flip; Phase L3 records L3e as done and adds L3f/L3g deliverables with guidance on where to park the comparison + analysis files.  
- input.md — Steering memo now directs Ralph to rerun the harness into `rot_vector/`, build `rot_vector_comparison.md`, and log hypotheses before touching simulator code.  
- galph_memory.md:1569-1572 — Logged the rotation-audit pivot so future loops know why L3f/L3g are gating parity.

Tests: not run (documentation/planning only).

Next steps (for Ralph):  
1. Execute the harness and comparison commands from input.md to capture the φ=0 rot vectors.  
2. Draft `rot_vector/analysis.md` with ranked hypotheses once the comparison table is in place.
[2025-10-07T10:09:55] tokens used: 278,503
[2025-10-07T10:09:55] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 06c311c43a704c36481a591a51402c228304c62b..548ef94b7b690490d375c9c927eca911903af512
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -457,10 +457,10 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
-- Next Actions (2025-11-19 evidence refresh):
-  1. Phase L3e per-φ + scaling validation — Refresh the trace harness so the current supervisor run emits `TRACE_PY_PHI` lines (existing `trace_py_scaling_per_phi.log` contains none), capture `trace_py_per_phi_20251119.log`/`.json`, and rerun `compare_scaling_traces.py` (≤1e-6 tolerances) with outputs stored under `reports/2025-10-cli-flags/phase_l/per_phi/` and `.../scaling_validation/`.
-  2. Phase L3f documentation sync — Update `analysis.md`, `docs/fix_plan.md` attempts, and architectural docs with the finalized ingestion/normalization flow once the refreshed scaling validation passes.
-  3. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+- Next Actions (2025-11-19 refresh):
+  1. Phase L3f rotation-vector audit — Capture aligned `TRACE_C` vs `TRACE_PY` rot-vector dumps for φ=0 using the updated harness, write `rot_vector_comparison.md` under `reports/2025-10-cli-flags/phase_l/rot_vector/`, and quantify Δ components that explain the `k_frac` offset (target <5e-4).
+  2. Phase L3g hypothesis framing — Summarize likely causes of the φ=0 drift (spindle axis, reciprocal reconstitution, phi step sizing) in `rot_vector/analysis.md`, cite supporting traces, and update this plan/fix_plan before touching simulator code.
+  3. Phase L4 parity rerun — After alignment fixes land and documentation is refreshed, execute the supervisor command end-to-end, capture nb-compare metrics (correlation ≥0.9995, sum_ratio 0.99–1.01), archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
@@ -625,10 +625,10 @@
       - `reports/2025-10-cli-flags/phase_l/scaling_validation/run_metadata.json` — Run metadata (SHA f3e65b8, torch 2.8.0+cu128, command snapshot)
     Observations/Hypotheses:
       - **Harness operational:** Per-φ instrumentation from Attempt #85 working correctly; captured k_frac ≈ -0.59 (φ=0°) to -0.607 (φ=0.1°), F_latt ranges 1.351 to -2.351
-      - **First divergence remains lattice-driven:** I_before_scaling delta (-24.4% rel) stems from F_latt mismatch (PyTorch summed F_latt=1.351 at φ=0° vs C trace showing F_latt=-2.3832)
+      - **φ=0 drift isolated:** Comparing `TRACE_C` vs `TRACE_PY` shows φ=0 vectors misaligned (`rot_b_y` 0.7173 Py vs 0.6716 C), yielding k_frac -0.589 instead of -0.607 and flipping F_latt_b’s sign. Later φ steps converge, so the error occurs before/at the initial phi orientation.
       - **Scaling factors validated:** r_e², fluence, steps, capture_fraction match C exactly (PASS status ≤1e-6); polar/omega/cos_2theta show MINOR deltas (≤2e-5 rel)
       - **Phase L3e exit criteria satisfied:** Per-φ JSON + scaling validation artifacts captured with current git SHA; comparison script enforces ≤1e-6 tolerance per plan guidance; pytest confirms instrumentation stability
-    Next Actions: Phase L3f documentation sync — Update analysis.md + architectural docs with finalized per-φ evidence and scaling validation summary; Phase L4 parity rerun after lattice factor root cause is addressed (F_latt sign/magnitude discrepancy blocking nb-compare correlation ≥0.9995).
+    Next Actions: Phase L3f rotation-vector audit — capture aligned rot vectors for φ=0 under the new harness, document deltas in `reports/2025-10-cli-flags/phase_l/rot_vector/`, and frame hypotheses (L3g) before modifying simulator normalization or rotation code.
   * [2025-10-07] Attempt #74 (ralph loop) — Result: **SUCCESS** (Phase L2b harness fix + L2c comparison complete). **Corrected harness to attach `crystal.hkl_data` and `crystal.hkl_metadata` instead of ad-hoc attributes; structure factor lookup now functional.**
     Metrics: Harness probe confirms hkl_data attached (lookup returns F=100 for (1,12,3), F=0 for out-of-range). Comparison identifies I_before_scaling as first divergence. Test suite passes 4/4 variants (tests/test_trace_pixel.py::TestScalingTrace::test_scaling_trace_matches_physics).
     Artifacts:
diff --git a/input.md b/input.md
index 3b425360a3677354002d34bb278ca2113e6c66e3..f82b37c9a3714f9e2728788fc086543fbdb22bce
--- a/input.md
+++ b/input.md
@@ -35,12 +35,12 @@
 py = extract(py_lines, 'PY')
 c = extract(c_lines, 'C')
 rows = [
-    ('rot_a_angstroms', 'Å'),
-    ('rot_b_angstroms', 'Å'),
-    ('rot_c_angstroms', 'Å'),
-    ('rot_a_star_A_inv', 'Å^{-1}'),
-    ('rot_b_star_A_inv', 'Å^{-1}'),
-    ('rot_c_star_A_inv', 'Å^{-1}')
+    ('rot_a_angstroms', 'Angstrom'),
+    ('rot_b_angstroms', 'Angstrom'),
+    ('rot_c_angstroms', 'Angstrom'),
+    ('rot_a_star_A_inv', 'Angstrom^-1'),
+    ('rot_b_star_A_inv', 'Angstrom^-1'),
+    ('rot_c_star_A_inv', 'Angstrom^-1')
 ]
 lines = ['| Vector | Units | C | PyTorch | Δ (Py-C) |', '| --- | --- | --- | --- | --- |']
 for key, units in rows:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 2c09179d4d5b32ebad884001919f822d9844bb03..4cb8f8f2bf331422933e839070e56829acd820e6
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -26,7 +26,7 @@
 - Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
 - Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
 - Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
-- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored but the newly captured `trace_py_scaling_per_phi.log` still lacks `TRACE_PY_PHI` lines, so per-φ evidence must be regenerated before Phase L3e can close the lattice-factor gap.
+- Gap snapshot refresh (2025-11-19): Latest scaling audit (`reports/2025-10-cli-flags/phase_l/scaling_validation/analysis_20251119.md`) shows `F_cell` parity restored while per-phi instrumentation (Attempt #86) now emits the expected `TRACE_PY_PHI` entries. The refreshed log exposes a residual phi=0 mismatch (`k_frac` -0.589 in PyTorch vs -0.607 in C) that flips `F_latt_b`’s sign, so Phase L3 needs a dedicated rotation-vector audit before touching normalization again.
 - Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
 - Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
 - Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
@@ -261,8 +261,10 @@
 | L3b | Mirror probe attachment in harness | [D] | ✅ 2025-11-17 Attempt #78: `trace_harness.py` now assigns both `hkl_data` and `hkl_metadata` before simulator construction (see `trace_py_scaling_20251117.log`, `harness_hkl_state_20251117.txt`). Keeps harness behavior aligned with probe results and removes F_cell=0 divergence. |
 | L3c | Audit CLI HKL loading path | [D] | ✅ 2025-10-18 Attempt #80: Refreshed `cli_hkl_audit.md` with precise line refs (line 1073) showing dtype-only transfer; confirmed probe artifacts and logged device gap blocking CUDA parity. |
 | L3d | Add targeted regression tests | [D] | ✅ Attempt #81 (2025-11-18): Added `TestHKLDevice::test_hkl_tensor_respects_device` covering CPU/CUDA × float32/float64; ensures HKL tensors honor requested device/dtype and lookups return F≈190.27 for (-7,-1,-14). |
-| L3e | Refresh per-φ scaling validation | [ ] | Re-run the trace harness so the current supervisor pixel emits `TRACE_PY_PHI` lines (`trace_py_per_phi_20251119.log/.json`), then execute `scripts/validation/compare_scaling_traces.py` with ≤1e-6 tolerances and archive refreshed metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/`. |
-| L3f | Update documentation & plans | [ ] | Refresh `docs/fix_plan.md` with the implementation attempt, link the new artifacts, and edit architectural docs (`docs/architecture/pytorch_design.md`, `docs/development/testing_strategy.md`) to describe the final normalization + HKL ingestion flow. Mark L3 tasks `[D]` once artifacts are archived. |
+| L3e | Refresh per-phi scaling validation | [D] | ✅ Attempt #86 (2025-10-07 timestamp, replayed 2025-11-19 artifacts): Harness now captures 10 `TRACE_PY_PHI` lines plus JSON, reruns `compare_scaling_traces.py`, and stores outputs under `reports/2025-10-cli-flags/phase_l/{per_phi,scaling_validation}/`. Remaining delta isolated to `k_frac`/`F_latt_b` at phi=0. |
+| L3f | Rotation-vector audit (new) | [ ] | Capture side-by-side `TRACE_C: rot_{a,b,c}` vs `TRACE_PY: rot_{a,b,c}` for phi=0 using updated harness, and dump aligned values (Angstrom + Angstrom^-1) into `reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md`. Highlight delta components >5e-4 and confirm whether the drift originates pre- or post-phi rotation (reference `nanoBragg.c:3006-3098`, `Crystal.get_rotated_real_vectors`). |
+| L3g | Fractional Miller alignment plan | [ ] | From the rot-vector audit, derive hypotheses for the `k_frac` offset (e.g., spindle-axis normalization, reciprocal-vector reconstitution, phi step size). Document hypotheses + confirming probes in `rot_vector/analysis.md`, then update this plan + docs/fix_plan with the chosen corrective action before any simulator edits. |
+| L3h | Update documentation & plans | [ ] | Once L3f/L3g evidence lands and the corrective code path is defined, refresh `docs/fix_plan.md`, architectural docs (`docs/architecture/pytorch_design.md`), and this plan to capture the normalization + rotation data flow. Mark Phase L3 `[D]` after implementation + doc sync. |
 
 ### Phase L4 — Supervisor Command Parity Rerun
 Goal: Demonstrate end-to-end parity for the supervisor command (correlation ≥ 0.9995, sum_ratio 0.99–1.01, mean_peak_distance ≤ 1 px) and close CLI-FLAGS-003.

Reading prompt from stdin...
To github.com:hoidn/nanoBragg.git
   24e490b..d398014  HEAD -> feature/spec-based-2
Waiting for Ralph to complete i=87...
Already up to date.
[sync] state: actor=ralph status=waiting-ralph iter=87
Updating d398014..ece7ee4
Fast-forward
 sync/state.json | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
From github.com:hoidn/nanoBragg
   d398014..ece7ee4  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=ralph status=running-ralph iter=87
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating ece7ee4..597d144
Fast-forward
 docs/fix_plan.md                                   |  14 +++
 .../per_phi/trace_py_rot_vector_per_phi.json       |  82 +++++++++++++++
 .../per_phi/trace_py_rot_vector_per_phi.log        |  10 ++
 .../phase_l/rot_vector/analysis.md                 | 111 +++++++++++++++++++++
 .../phase_l/rot_vector/rot_vector_comparison.md    |   8 ++
 .../phase_l/rot_vector/trace_py_rot_vector.log     |  40 ++++++++
 .../phase_l/scaling_audit/trace_py_env.json        |   4 +-
 7 files changed, 267 insertions(+), 2 deletions(-)
 create mode 100644 reports/2025-10-cli-flags/phase_l/per_phi/trace_py_rot_vector_per_phi.json
 create mode 100644 reports/2025-10-cli-flags/phase_l/per_phi/trace_py_rot_vector_per_phi.log
 create mode 100644 reports/2025-10-cli-flags/phase_l/rot_vector/analysis.md
 create mode 100644 reports/2025-10-cli-flags/phase_l/rot_vector/rot_vector_comparison.md
 create mode 100644 reports/2025-10-cli-flags/phase_l/rot_vector/trace_py_rot_vector.log
From github.com:hoidn/nanoBragg
   ece7ee4..597d144  feature/spec-based-2 -> origin/feature/spec-based-2
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating 597d144..043ae15
Fast-forward
 sync/state.json | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)
From github.com:hoidn/nanoBragg
   597d144..043ae15  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=galph status=complete iter=88
Ralph completed iteration 87; proceeding to 88
