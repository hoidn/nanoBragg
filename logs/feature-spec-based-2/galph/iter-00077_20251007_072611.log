Already up to date.
Waiting for expected_actor=galph...
Already up to date.
[sync] state: actor=galph status=complete iter=77
To github.com:hoidn/nanoBragg.git
   a68ee62..a77ea6e  HEAD -> feature/spec-based-2
$ codex exec -m gpt-5-codex -c model_reasoning_effort=high --dangerously-bypass-approvals-and-sandbox
[2025-10-07T07:26:15] OpenAI Codex v0.38.0 (research preview)
--------
workdir: /Users/ollie/Documents/nanoBragg3
model: gpt-5-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
--------
[2025-10-07T07:26:15] User instructions:
<role>
planning, review and analysis. do not make implementation code changes.
</role>
<current long-term goals>
in order of decreasing priority:
<1>
run a successful parallel test of pytorch nanobragg against nanobragg c with this command:

nanoBragg  -mat A.mat -floatfile img.bin -hkl scaled.hkl  -nonoise  -nointerpolate -oversample 1  -exposure 1  -flux 1e18 -beamsize 1.0  -spindle_axis -1 0 0 -Xbeam 217.742295 -Ybeam 213.907080  -distance 231.274660 -lambda 0.976800 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527 -odet_vector -0.000088 0.004914 -0.999988 -sdet_vector -0.005998 -0.999970 -0.004913 -fdet_vector 0.999982 -0.005998 -0.000118 -pix0_vector_mm -216.336293 215.205512 -230.200866  -beam_vector 0.00051387949 0.0 -0.99999986  -Na 36  -Nb 47 -Nc 29 -osc 0.1 -phi 0 -phisteps 10 -detector_rotx 0 -detector_roty 0 -detector_rotz 0 -twotheta 0

this will require first adding support for the following cli params to pytorch nanobragg:
-nonoise
-pix0_vector_mm

</1>
<2>
review and firm up if needed plans/active/vectorization.md. then implement it (or rather, delegate its implementation to ralph), starting with the tricubic interpolation vectorization 
</2>
</current long-term goals>
</past long term goals>
(just for archival / reference purposes):
- error-correcting the engineer agent 
- profiling pytorch nanobragg to find performance bottlenecks. analyzing to understand the CRITICAL slowness of the pytorch implementation compared to reference C, which persists after our recent improvement attempts
- finding performance issues in the PyTorch reimplementation of nanobragg (originally a C code) and speeding it up. It should be efficiently vectorized and faster than C nanobragg, but currently it's slower 
- ensuring that the pytorch implementation uses fp32 (not fp64) by default 
- understanding why pytorch is slower than C in the warm condition at 4096x4096 resolution and fixing the underlying performance issue, if one is found. THIS IS A CRITICAL PERFORMANCE ISSUE.
- once all the above are taken care of: building a user-facing showcase of autodiff-based parameter refinement, with plots / visuals

</past long-term goals>
<task>
You are galph, a planner / supervisor agent. you are overseeing the work of an agent (ralph) that is running prompts/main.md in a loop, using docs/fix_plan.md as its instruction set and long term memory. 

You will get invoked repeatedly, for multiple iterations. Use galph_memory.md to communicate with your future self. You'll plans under plans/, when needed, to help steer multi-turn efforts by the coder agent (ralph). Those plans will be cross referenced from docs/fix_plan.md so that ralph can find / read them. 

At the start of every invocation:
- Run `timeout 30 git pull --rebase` with a hard 30-second timeout  to sync with origin before reviewing context. If the command times out, immediately abort any partial rebase (`git rebase --abort`) and fall back to a normal merge via `git pull --no-rebase`. Whatever path you take, resolve resulting conflicts (docs/fix_plan.md is a frequent hotspot) and document key decisions in galph_memory.md. If the pull reports conflicts:
  * Run `git status --short` to list conflicted files.
  * Fix each file (remove conflict markers, keep the intended content) and stage it with `git add`.
  * When the index is clean, resume with `timeout 30 git rebase --continue --no-edit`. IMPORTANT: NEVER RUN THIS COMMAND WITHOUT TIMEOUT
- After resolving any conflicts, read the latest galph_memory.md entry (and any linked plan files) so you do not lose past context.
- Prune or summarize older entries when they become stale or redundant.

During the main, central phase of invocation you will be doing essentially three things:
<main analysis step 1> 
An initial analysis pass in which you gather and review information in order to choose a <focus issue> for the turn, corresponding to <instructions> steps <0> through <3>. This will involve a multi-category selection process. You will choose both a <focus issue> and an associated <Action type> for the remainder of the iteration. The available <Action type>s are the following:
<Action type> list:
<1>
<Evidence collection> (i.e. Evidence task)
No full pytest runs. Allowed: non-mutating probes and CLI validation tools (e.g., scripts/validation/*, nb-compare, pixel trace); pytest --collect-only to validate selectors; and in TDD mode only, a single targeted pytest run to confirm the newly authored test fails. Do not change production code. This gate applies only to galph. Ralph is exempt. Do not include any Phase label in input.md.

Evidence collection includes the following subtype:
    Callchain Tracing (Evidence subtype)
    - When to use: before code edits when the pipeline/factor order relevant to the current issue is unclear; when onboarding a new surface; or upon parity failures where the locus is unknown.
    - Directive: first, emit descriptions for the following things:
       - <analysis_question> (required): Free‑text description of the behavior/bug/perf issue to investigate. This will be closely related to <focus issue>, but viewed from the point of view of relevant code execution paths and their entry points.
       - <initiative_id> and <scope_hints> 

    - Directive: then read and carefully follow the instructions in `prompts/callchain.md` with the question‑driven invocation (<analysis_question>) and produce standardized artifacts under `reports/<initiative_id>/`.
    - Example invocation variables (fill at run time):
      analysis_question: "<what are we trying to understand or fix?>"
      initiative_id: "<short‑slug>"
      scope_hints: ["CLI flags", "normalization", "scaling"]
      roi_hint: "<minimal input/ROI>"
      namespace_filter: "<project primary package>"
      time_budget_minutes: 30
    - Expected outputs:
      - `reports/<initiative_id>/callchain/static.md` (entry→sink with file:line anchors)
      - `reports/<initiative_id>/callgraph/dynamic.txt` (optional, module‑filtered)
      - `reports/<initiative_id>/trace/tap_points.md` (proposed numeric taps with owners)
      - `reports/<initiative_id>/summary.md` (question‑oriented narrative + next steps)
      - `reports/<initiative_id>/env/trace_env.json`
    - Guardrails: evidence‑only (no prod edits), module/device/dtype neutrality, small ROI, respect Protected Assets, stable key names in traces.
</Evidence collection>
</1>
<2>
<Debug> i.e. Debugging — Hypothesis + Triage (initial pass):
- pairs with <focus issue>s that are related to debugging or parity analysis.
<steps>
- Formulate 1–3 plausible hypotheses for the observed gap/issue.
- Triage each with checks appropriate to scope using existing artifacts or quick, documented reproductions; this triage may constitute the primary work for the iteration. Record outcomes succinctly in the evidence.
- For the top hypothesis, state confidence and the single confirming step to run next. Deliver 1–3 ranked hypotheses with supporting evidence, artifact paths, and the next confirming step.
</steps>
</Debug>
</2>
<3>
<Planning>
- we will be calling the plan topic the <focus issue> of this turn.
- based on which long term <goal> and sub-goal is that effort / plan? 
- Which existing docs/fix_plan.md items does it (i.e. the <focus issue>) relate to? 
- Documentation review for <focus issue>: Using `docs/index.md` as the map, identify and read the specific documents relevant to the chosen <focus issue> (e.g., component contracts, architecture ADRs, parity/testing strategy). List the file paths you will rely on (with a one‑line rationale each) before drafting or updating the plan.
- think deeply. draft / redraft the plan and save it to a .md under plans/active/. Structure the write-up as a phased implementation document (see `plans/archive/general-detector-geometry/implementation.md` for tone/shape): begin with context + phase overviews, then outline each phase’s intent, prerequisites, and exit criteria. When a phase benefits from explicit tracking, embed a checklist table using the `ID | Task Description | State | How/Why & Guidance` format (with `[ ]`, `[P]`, `[D]` markers) inside that phase section.
  • Include reproduction commands, owners (if known), and decision rules in the guidance column.
  • Favor narrative flow first; layer checklists only where they clarify verification steps or deliverables.
  • Mini-template to crib when drafting:
    ```md
    ## Context
    - Initiative: <initiative>
    - Phase Goal: <outcome>
    - Dependencies: <docs/tests>

    ### Phase A — <short title>
    Goal: <what this phase proves or delivers>
    Prereqs: <artifacts or measurements required before starting>
    Exit Criteria: <verifiable completion signal>

    | ID | Task Description | State | How/Why & Guidance (including API / document / artifact / source file references) |
    | --- | --- | --- | --- |
    | A1 | <Key diagnostic or implementation step> | [ ] | Run `<command>`; capture results under `reports/<date>/...`. |
    | A2 | <Follow-up validation> | [ ] | Compare against `<artifact>`; stop if deviation > threshold. |
    ```
- When refreshing an existing plan, retrofit it to this phased format before adding or editing tasks.
- review docs/fix_plan.md. edit if needed. cross reference the new plans .md so that ralph can find it.
</Planning>
</3>

<4>
<Review or housekeeping>
- Review and evaluate ralph's work. Scrutinize the commit history. Look at the diffs. 
- Are the docs/fix_plan.md contents and priorities sane? things to consider:
  - if docs/fix_plan.md is longer than 1000 lines it should be housecleaned. If it's disorganized and / or internally inconsistent, consider how this could be addressed. 
- after considering the above, you have enough information to choose a <focus issue> for this turn. do so. Consider the nature of the <focus issue>:
    - Do we need a new docs/fix_plan item to put ralph back on course, fix one of his mistakes, or instruct him to do something that he overlooked? If so, draft it and add it to docs/fix_plans
    - does the <focus issue> involve identified issues in docs/fix_plan.md? If so, fix them. If you decide to shorten docs/fix_plan.md, the least relevant portions should be moved to archive/fix_plan_archive.md (with summary + archive cross reference if appropriate)
</Review or housekeeping>
</4>

In addition to selecting an <Action type> and <focus issue> you will choose a <Mode> flag. 
 - Mode flags are combinable and refine execution, and are applicable to both the supervisor (you) and ralph; do not include any Phase label in input.md.
 - available modes: <TDD | Parity | Perf | Docs | none>
<mode descriptions>
TDD mode (supervisor-scoped): When operating in supervisor evidence mode, author/update a single minimal failing test that encodes the acceptance criterion. Confirm it fails via a targeted pytest selector; record the selector and expected failure text in input.md. Do not change production code, and do not include any Phase label in input.md.
</mode descriptions>

</main analysis step 1>

<main analysis step 2>
Here you will do a deep analysis. You will follow the guidelines of the selected <Mode> and <Action type>, applying them to the selected <focus issue>. This corresponds to <instructions> step <3.2>.


</main analysis step 2>

<main analysis step 3>
Here you synthesize the analysis findings into output files (<instructions> steps <3.5> to <4>)
</main analysis step 3>


Before concluding each invocation:
- Append a concise update to galph_memory.md capturing key findings, decisions, and open questions (reference file paths or plan names).
- Note any follow-up actions you expect Ralph to take.
- If no substantial update is needed, explicitly log that fact so future runs know context was reviewed.
</task>

<instructions>
<0>
READ the following files:
- Index of project documentation: `./docs/index.md`
- $SPECS: `./specs/spec-a.md`
- $ARCH: `./arch.md` (ADR-backed implementation architecture; reconcile design with spec, surface conflicts)
- docs/development/c_to_pytorch_config_map.md — C↔Py config parity and implicit rules
-- docs/debugging/debugging.md — Parallel trace-driven debugging SOP
- $PLAN: `./fix_plan.md` (living, prioritized to‑do; keep it up to date)
- $TESTS: `./docs/development/testing_strategy.md` (testing philosophy, tiers, seeds/tolerances, commands)
- Set `AUTHORITATIVE_CMDS_DOC=./docs/development/testing_strategy.md` (or project‑specific path) and consult it for authoritative reference commands and test discovery.
<0>
<1>
do a deep analysis of the codebase in light of the <current long term goals>. What are some current issues / gaps and possible approaches to resolving them? Review docs/fix_plan.md and plans/active/, as previous iterations of you may have already done some legwork.
 


At this point, you can do a <Callchain tracing> or <Debug> action.
</1>
<2>
flip a coin using python. if it comes up <heads>:
review ralph's work over the last N (~10 but can be more or less - you decide) iterations. Check the commit history. Has the work been productive? Have there been regressions? Do we need to provide any feedback / course-correction?
</heads>
if it comes up <tails>: proceed to step <3>
</2>
<3>
Given your findings in <1> and <2>, think about whether there's any need for a multi-turn planning effort -- i.e. ralph can't see the forest for the trees and may struggle with major refactorings and multi-turn implementation efforts unless they are coordinated by you. Is there a need for such planning *right now*? If such a plan is already spoken for (plans/active/ or wherever else past galph might have saved to), does that plan require updates or is it up to date / high quality and simply pending? IFF you determine the need for a new plan or modification of an existing one:
<yes case>
follow the <Planning> <Action type> instructions
</yes case>
</no case>
Since you are not <Planning>, you must select one of the other <Action type>s and follow its instructions.
</no case>
</3>
<3.2>
- Documentation review for <focus issue>: From `docs/index.md`, enumerate and read the documents that are most relevant to the chosen <focus issue>`; note the key file paths you will rely on for the impending analysis.
- Then carefully follow the guidelines of the selected <Mode> and <Action type>, applying them to the selected <focus issue>. Complete the task that you selected in step <3>.
</3.2>
<3.5>
Render and write ./input.md (supervisor→engineer steering memo). Overwrite the entire file every invocation. Keep length ~100–200 lines. Include these sections in order:
Header:
- Summary: <one‑sentence goal for this loop>
 - Mode: <TDD | Parity | Perf | Docs | none>
- Focus: <plan item ID/title from docs/fix_plan.md>
- Branch: <expected branch>
- Mapped tests: <validated list | none — evidence‑only>
- Artifacts: <key paths to produce under reports/>
- Do Now: 1 line naming the exact docs/fix_plan.md item (ID and title) to execute this loop, plus the exact pytest command/env to reproduce it (only when an authoritative mapping exists). If no such test exists, the Do Now MUST be to author the minimal targeted test first and then run it. If you intend to delegate the choice, write “Do Now: delegate” and provide decision guidance below.
  - Note: If operating in supervisor <Evidence collection> (internal-only), do not run the full suite. Allowed: pytest --collect-only to validate selectors; in TDD mode only, run one targeted selector to confirm a new failing test. All other test execution is deferred to Ralph. Do not include any Phase label in input.md.
<Do Now guidelines>
Verification scripts: You may run nb-compare and scripts/validation/* to collect metrics and artifacts (no code changes). Record outputs under reports/.
Mapped tests under supervisor evidence mode: Include exact selectors in input.md; validate with --collect-only; do not run them (except the TDD failure confirmation). Do not include any Phase label in input.md.
Command Sourcing (tests): Only include an exact pytest command in Do Now when sourced from an authoritative mapping (e.g., docs/development/testing_strategy.md) or an existing, known test file/identifier. If no authoritative mapping exists, set the Do Now task to author the minimal targeted test first; do not guess at a pytest selector here.
Implementation: refer to phased plans, checklists, and all associated artifacts
</Do Now guidelines>
- If Blocked: fallback action (what to run/capture, how to record it in Attempts History).
- Priorities & Rationale: 3–6 bullets with file pointers (specs/arch/tests/plan) justifying the Do Now.
- How-To Map: exact commands (pytest, nb-compare), env vars, ROI/thresholds, and where to store artifacts; prefer using `scripts/validation/*` when available.
- Pitfalls To Avoid: 5–10 crisp do/don’t reminders (device/dtype neutrality, vectorization, Protected Assets, two‑message loop policy, no ad‑hoc scripts).
- Pointers: file paths with line anchors to the most relevant spec/arch/testing_strategy/fix_plan entries for this loop.
- Next Up (optional): 1–2 candidates Ralph may choose next loop if he finishes early (still one thing per loop).

 Rules:
 - Ralph must not edit input.md. You (galph) are the single writer.
 - If input.md’s Do Now conflicts with an existing in_progress selection in fix_plan.md, Ralph is allowed to switch; he must record the change in Attempts History.
- Do not instruct Ralph to run the full test suite more than once per turn. Require targeted tests first; run the full suite once at the end only if code changed. For prompt/docs‑only loops, require `pytest --collect-only -q` instead.
- Prefer referencing reusable scripts under `scripts/validation/` from Do Now when available.
- Commit input.md each run as part of step <4> (commit only if the content changed).
 </3.5>

<4>
Before finishing the loop, enforce git hygiene:
- Run `git status --short` to inspect new or modified files that appeared during this invocation.
- Review diffs for each change; revert only if you created it this loop and it was accidental.
- Stage intentional updates with `git add -A` and commit via `git commit -m "SUPERVISOR: <scope> - <tests or rationale>"`, noting any tests run (use `tests: not run` when you skip them).
- After committing, run `git push` to share the updates. If the push is rejected, `timeout 30 git pull --rebase`, resolve conflicts (capture decisions—especially for docs/fix_plan.md—in galph_memory.md), then push again.
- If you deliberately leave the tree dirty, document the rationale in galph_memory.md so the next invocation knows why.
</4>
</instructions>
<notes>
- ignore 'routing violations'. these are out of scope.
- ignore AT parallel 012-related items for now
</notes>
Now carefully and exhaustively follow your <instructions>, while keeping in mind the <task> directives.


[2025-10-07T07:26:17] thinking

**Preparing to update repository**
[2025-10-07T07:26:18] exec bash -lc 'timeout 30 git pull --rebase' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:21] bash -lc 'timeout 30 git pull --rebase' succeeded in 2.38s:
Already up to date.
[2025-10-07T07:26:21] tokens used: 13,973

[2025-10-07T07:26:23] thinking

**Preparing to review memory and plans**
[2025-10-07T07:26:24] exec bash -lc 'cat galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:25] bash -lc 'cat galph_memory.md' succeeded in 740ms:
## 2025-09-30 (loops 1-4 summary)
- Established parity backlog: AT-020/021/022/024 failing; created first supervisor plan for AT-021 and enforced prompts/debug.md routing.
- Diagnosed AT-024 random misset cap mismatch (π/2 vs 90 radians) and opened dedicated plan; insisted Ralph stop verification-only loops.
- Flagged torch.compile warm-up overhead (per-instance compilation, inline tensor factories) and opened AT-012 rotation parity plan (Phases A–E).
- Documented performance hotspots (`_compute_physics_for_position` guard tensors, `Crystal.compute_cell_tensors` clamps) and chronic dirty worktree artifacts blocking `git pull --rebase`.

## 2025-09-30 (loops 5-10 summary)
- Authored repo hygiene checklist (REPO-HYGIENE-002) to undo accidental `golden_suite_generator/nanoBragg.c` churn and purge stray reports.
- Created PERF-PYTORCH-004 plan to hoist constants, introduce compile caching, and remove Dynamo graph breaks; reiterated need for prompts/debug.md while parity fails.
- Repeatedly documented routing violations from Ralph and emphasized restoring Core Rule #13 metric duality after WIP commit 058986f swapped `V_actual` for formula volume.
- Noted that `_compute_physics_for_position` still recompiles each Simulator and that benchmarks in `reports/benchmarks/20250930-004916/` show ≥200× slowdown vs C for small detectors.

## 2025-09-30 (ralph debugging loop — AT-012 float32 regression)
- **Context**: AT-PARALLEL-012 test_simple_cubic_correlation failing (43/50 peaks matched, need ≥95%) after DTYPE-DEFAULT-001 (commit 8c2ceb4) switched simulator to native float32.
- **Investigation**:
  * Reproduced: 43/50 peaks matched (86%), corr≈1.0 (perfect physics) ✅
  * Parity matrix test: PASSES perfectly ✅ (live C↔Py comparison with current float32)
  * **Root cause identified**: PyTorch float32 produces 7× more unique values (4901 vs C's 669) in hot pixels, fragmenting intensity plateaus. Peak detection algorithm (scipy.ndimage.maximum_filter) is sensitive to plateau structure.
  * Parallel comparison: C float32 beam center (20×20 ROI) has 91 unique values; PyTorch has 331 (3.6× fragmentation). Max absolute diff tiny (0.0017).
  * **First Divergence**: Numerical accumulation patterns differ (likely FMA, compiler opts, vectorized accumulation order, or torch.compile kernel fusion) → perfect correlation but incompatible plateau structure.
[2025-10-07T07:26:25] tokens used: 14,198

[2025-10-07T07:26:26] thinking

**Listing recent memory entries**
[2025-10-07T07:26:28] exec bash -lc 'ls plans/active' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:28] bash -lc 'ls plans/active' succeeded in 657ms:
cli-noise-pix0
gradcheck-tier2-completion
perf-pytorch-compile-refactor
routing-loop-guard
supervisor-loop-guard
unification.md
vectorization.md
[2025-10-07T07:26:28] tokens used: 17,034

[2025-10-07T07:26:32] thinking

**Preparing to read key documentation files**
[2025-10-07T07:26:33] exec bash -lc 'cat docs/index.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:33] bash -lc 'cat docs/index.md' succeeded in 690ms:
# Project Documentation Index

Welcome to the central index for all `nanoBragg-PyTorch` project documentation. This guide provides a map to the key technical specifications, development processes, and historical records that define the project.

---

## 🏛️ Permanent Documentation

These are the core, living documents that guide the project.

### Core Project Guides
### PyTorch Guardrails
* **[PyTorch Runtime Checklist](./development/pytorch_runtime_checklist.md)** – quick do/don’t list for vectorization and device placement. Always review before editing simulator code.
* **[PyTorch Design](./architecture/pytorch_design.md#vectorization-strategy)** – detailed explanation of the required broadcast shapes and batched flows.
* **[Testing Strategy §1.4](./development/testing_strategy.md#14-pytorch-device--dtype-discipline)** – required CPU/GPU smoke tests and compile checks.

* **[CLAUDE.md](../CLAUDE.md)** - The primary instruction set for the AI agent.
* **[README.md](../README.md)** - The main project entry point.
* **[README_PYTORCH.md](../README_PYTORCH.md)** - Comprehensive user guide for the PyTorch implementation, including CLI usage, parallel comparison tools, and visualization.
* **[PROJECT_STATUS.md](../PROJECT_STATUS.md)** - Tracks the current active development initiative.
[2025-10-07T07:26:33] tokens used: 17,384

[2025-10-07T07:26:34] thinking

**Opening next specification file**
[2025-10-07T07:26:35] exec bash -lc 'cat specs/spec-a.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:36] bash -lc 'cat specs/spec-a.md' succeeded in 709ms:
# nanoBragg Spec A — Sharded Specification

Spec A is now organized across multiple shards. Read them together for the full normative contract.

- [spec-a-core.md](spec-a-core.md): Core physics, geometry, sampling, data, and associated acceptance tests.
- [spec-a-cli.md](spec-a-cli.md): CLI surface area, precedence rules, and CLI-focused acceptance tests.
- [spec-a-parallel.md](spec-a-parallel.md): Conformance profiles, parallel validation suites, and developer tooling requirements.

Each shard cross-references the others; updates must keep them consistent.
[2025-10-07T07:26:36] tokens used: 19,062

[2025-10-07T07:26:38] thinking

**Considering which specs to read**
[2025-10-07T07:26:39] exec bash -lc 'cat arch.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:39] bash -lc 'cat arch.md' succeeded in 714ms:
# nanoBragg PyTorch — Implementation Architecture

Document version: 1.0 (aligned with specs/spec-a.md)
Target spec: specs/spec-a.md (single-file normative spec)
Implementation target: Python (>=3.11) + PyTorch (float32 default). Type blocks may use TypeScript-style for clarity; implement with Python dataclasses and torch tensors.

## Running The CLI

- Install (recommended):
  - `pip install -e .` from repo root (provides `nanoBragg` console script).
- Run (installed):
  - `nanoBragg -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Run (without install):
  - `PYTHONPATH=src python -m nanobrag_torch -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Minimal quick test (no HKL):
  - `nanoBragg -default_F 1 -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -detpixels 128 -pgmfile test.pgm`

Notes
- Entry point: `src/nanobrag_torch/__main__.py` (module runner `python -m nanobrag_torch`).
- Required inputs: provide `-hkl` (or existing `Fdump.bin`) or set `-default_F > 0`, and either `-mat` (MOSFLM A*) or `-cell`.
[2025-10-07T07:26:39] tokens used: 19,474

[2025-10-07T07:26:41] thinking

**Opening development documentation**
[2025-10-07T07:26:42] exec bash -lc 'cat docs/development/c_to_pytorch_config_map.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:43] bash -lc 'cat docs/development/c_to_pytorch_config_map.md' succeeded in 697ms:
# C-CLI to PyTorch Configuration Map

## Critical Importance Notice

**This document is the authoritative source of truth for configuration parity between nanoBragg.c and the PyTorch implementation.**

Before writing any test or implementation that involves C-code validation, you **MUST** consult this document. Failure to ensure 1:1 configuration parity is the most common source of bugs, particularly with:
- Implicit pivot mode logic
- Convention-dependent beam center calculations
- Rotation axis defaults
- Unit conversions

## Quick Reference Table

### Crystal Parameters

| C-CLI Flag | PyTorch Config Field | C Variable | Units/Convention | Critical Notes |
|------------|---------------------|------------|------------------|----------------|
| `-cell a b c al be ga` | `CrystalConfig.cell_a/b/c/alpha/beta/gamma` | `a[0], b[0], c[0], alpha, beta, gamma` | Å and degrees → radians | Must convert degrees to radians internally |
| `-N <val>` | `CrystalConfig.N_cells` | `Na, Nb, Nc` | Number of unit cells | Sets all three axes to same value |
[2025-10-07T07:26:43] tokens used: 22,138

[2025-10-07T07:26:44] thinking

**Opening debugging documentation**
[2025-10-07T07:26:45] exec bash -lc 'cat docs/debugging/debugging.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:45] bash -lc 'cat docs/debugging/debugging.md' succeeded in 691ms:
# nanoBragg PyTorch Debugging Guidelines

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** PyTorch Development Team

## Overview

This document outlines the debugging methodology and tools for the nanoBragg PyTorch implementation. The debugging approach is built around a parallel trace comparison between the C code and the PyTorch implementation to ensure correctness and facilitate rapid issue resolution.

## Core Debugging Philosophy

### Parallel Trace Comparison Rule

**CRITICAL:** All debugging of physics discrepancies MUST begin with a parallel trace comparison. This involves:
1. Generating a detailed, step-by-step log of intermediate variables from the original C code for a specific pixel (the "Golden Trace").
2. Generating an identical log from the PyTorch implementation using `scripts/debug_pixel_trace.py`.
3. Comparing these two logs line-by-line to find the first point of divergence.

This methodology is mandatory to avoid guesswork and ensure bugs are found deterministically.
[2025-10-07T07:26:45] tokens used: 24,041

[2025-10-07T07:26:47] thinking

**Opening fix_plan.md**
[2025-10-07T07:26:49] exec bash -lc 'cat docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:50] bash -lc 'cat docs/fix_plan.md' succeeded in 688ms:
# Fix Plan Ledger

**Last Updated:** 2025-10-17 (galph loop)
**Active Focus:**
- ROUTING: Close the reopened guard plan by capturing a fresh regression audit referencing commit `c49e3be` and re-confirming the guarded `loop.sh` flow (`plans/active/routing-loop-guard/plan.md` Phases A–C) before automation resumes.
- ROUTING-SUPERVISOR: Launch Phase A of `plans/active/supervisor-loop-guard/plan.md`, then drive Phase B guard work (B2–B4) and new task B5 to add `supervisor.sh` to docs/index.md so Protected-Asset policy covers the script before automation runs again.
- AT-012: Plan archived (`plans/archive/at-parallel-012-plateau-regression/plan.md`); monitor for regressions using `reports/2025-10-AT012-regression/phase_c_validation/` artifacts and re-open only if peak matches drop below spec.
- GRADIENT: Execute `plans/active/gradcheck-tier2-completion/plan.md` Phase A (A1–A3 baseline audit + env alignment) before adding misset/beam gradchecks from Phases B/C; once pass logs exist, close `[AT-TIER2-GRADCHECK]` with Phase D documentation updates.
- DTYPE: ✅ Complete. Plan archived to `plans/archive/dtype-default-fp32/`. All phases (A-D) finished; float32 defaults documented in arch.md, pytorch_runtime_checklist.md, CLAUDE.md, prompts/debug.md.
- PERF: Land plan task B7 (benchmark env toggle fix), rerun Phase B6 ten-process reproducibility with the new compile metadata, capture the weighted-source parity memo feeding C5, then execute Phase C diagnostics (C1/C2 plus C8/C9 pixel-grid & rotated-vector cost probes, and new C10 mosaic RNG timing) ahead of Phase D caching work (D5/D6/D7) and detector-scalar hoisting (D8).

## Index
| ID | Title | Priority | Status |
| --- | --- | --- | --- |
| [PROTECTED-ASSETS-001](#protected-assets-001-docsindexmd-safeguard) | Protect docs/index.md assets | Medium | in_progress |
| [REPO-HYGIENE-002](#repo-hygiene-002-restore-canonical-nanobraggc) | Restore canonical nanoBragg.c | Medium | in_progress |
| [PERF-PYTORCH-004](#perf-pytorch-004-fuse-physics-kernels) | Fuse physics kernels | High | in_progress |
| [AT-TIER2-GRADCHECK](#at-tier2-gradcheck-implement-tier-2-gradient-correctness-tests) | Implement Tier 2 gradient correctness tests | High | in_progress |
| [VECTOR-TRICUBIC-001](#vector-tricubic-001-vectorize-tricubic-interpolation-and-detector-absorption) | Vectorize tricubic interpolation and detector absorption | High | in_planning |
| [CLI-DTYPE-002](#cli-dtype-002-cli-dtype-parity) | CLI dtype parity | High | in_progress |
[2025-10-07T07:26:50] tokens used: 26,496

[2025-10-07T07:26:52] thinking

**Continuing review with testing strategy**
[2025-10-07T07:26:52] exec bash -lc 'cat docs/development/testing_strategy.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:53] bash -lc 'cat docs/development/testing_strategy.md' succeeded in 717ms:
# nanoBragg PyTorch Testing Strategy

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** [Your Name/Team]

## 1. Introduction & Philosophy

This document outlines the comprehensive testing strategy for the PyTorch implementation of nanoBragg. The primary goal is to ensure that the new application is a correct, verifiable, and trustworthy scientific tool.
Our testing philosophy is a three-tiered hybrid approach, designed to build confidence layer by layer:

1. **Tier 1: Translation Correctness:** We first prove that the PyTorch code is a faithful, numerically precise reimplementation of the original C code's physical model. The C code is treated as the "ground truth" specification.
2. **Tier 2: Gradient Correctness:** We then prove that the new differentiable capabilities are mathematically sound.
3. **Tier 3: Scientific Validation:** Finally, we validate the model against objective physical principles.

All tests will be implemented using the PyTest framework.

### 1.4 PyTorch Device & Dtype Discipline

- **Device-neutral code:** Every PyTorch path MUST operate correctly on both CPU and CUDA tensors and across supported dtypes. Do not hard-code `.cpu()`/`.cuda()` calls, create CPU-only constants, or assume float64 execution when the caller may supply float32/half tensors.
[2025-10-07T07:26:53] tokens used: 29,419

[2025-10-07T07:26:57] thinking

**Planning deep codebase analysis**
[2025-10-07T07:26:58] exec bash -lc 'cat plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:26:59] bash -lc 'cat plans/active/cli-noise-pix0/plan.md' succeeded in 683ms:
## Context
- Initiative: CLI Parity for nanoBragg PyTorch vs C (supports long-term goal in prompts/supervisor.md)
- Phase Goal: Accept `-nonoise` and `-pix0_vector_mm` flags with C-equivalent semantics so the parallel comparison command in prompts/supervisor.md executes end-to-end.
- Dependencies: specs/spec-a-cli.md §§3.2–3.4, docs/architecture/detector.md §5, docs/development/c_to_pytorch_config_map.md (detector pivot + noise), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860 (flag behavior), docs/debugging/detector_geometry_checklist.md (pix0 validation), docs/development/testing_strategy.md §2 (CLI parity tests).
- Current gap snapshot (2025-10-18 refresh): Phase H4 parity landed (Attempt #25) with pix0 deltas < 2e-8 m and regression tolerances tightened; the remaining blocker before executing the supervisor command end-to-end is Phase I polarization alignment.
- Gap snapshot update (2025-10-19): Attempt #27 parity run reveals 1.24538e5× intensity scaling mismatch despite polarization fix; Phases J–L track the normalization diagnosis, fix, and closure required before long-term Goal #1 completes.
- Gap snapshot refresh (2025-10-22): Attempt #31 restored C precedence for custom detector vectors (pix0 override now gated again). Outstanding work: H5c PyTorch traces and Phase K normalization to eliminate the `F_latt`-driven intensity gap.
- Gap snapshot update (2025-10-24): Attempt #35 reran the PyTorch trace harness post-unit-fix; metrics still show |Δpix0| ≈ 1.145 mm, so a fresh diagnostics phase is required before resuming normalization work.
- Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
- Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
- Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
- Gap snapshot update (2025-11-11): Discovered the trace harness omitted `-beam_vector`, `-flux`, and `-beamsize`, forcing oversample=4 and a +Z beam; Py traces zeroed `F_cell` and inflated `steps`. Marked unresolved pending harness fix.
- Gap snapshot refresh (2025-11-12): Attempt #66 (commit 1d2e6ac) completed Phase L2b0 — the trace harness now feeds the full supervisor flag set, and the refreshed `trace_py_scaling.log` confirms `steps=10`, `fluence≈1.0e24`, and `r_e^2` parity with C. TRACE_PY still reports `I_before_scaling NOT_EXTRACTED`, `polar=0`, and `I_pixel_final=0`, so Phase L2b instrumentation must surface the real polarization/absorption tensors before we can quantify the K3e k_frac drift.
- Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
- Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
- Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
[2025-10-07T07:26:59] tokens used: 32,172

[2025-10-07T07:27:00] thinking

**Reviewing vectorization plan**
[2025-10-07T07:27:01] exec bash -lc 'cat plans/active/vectorization.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:01] bash -lc 'cat plans/active/vectorization.md' succeeded in 690ms:
## Context
- Initiative: PERF-PYTORCH-004 vectorization backlog; supports long-term goal "Parallel parity + performance" from docs/fix_plan.md.
- Phase Goal: Deliver a production-ready, fully vectorized tricubic interpolation pipeline (gather + polynomial evaluation) and follow with detector absorption vectorization without regressing physics, gradients, or device neutrality.
- Dependencies: specs/spec-a-core.md §4 (Structure factors), specs/spec-a-parallel.md §2.3 (Interpolation acceptance tests), docs/architecture/pytorch_design.md §§2.2–2.4, docs/architecture/c_code_overview.md (tricubic routines), docs/development/testing_strategy.md §§2–4, docs/development/pytorch_runtime_checklist.md, `nanoBragg.c` lines 2604–3278 (polin3/polin2/polint + detector absorption), `tests/test_at_str_002.py`, `tests/test_at_abs_001.py`, `reports/benchmarks/20250930-165726-compile-cache/` (current performance references). No dedicated tricubic/absorption microbenchmarks exist yet; Phase A will author reusable harnesses under `scripts/benchmarks/` so future loops can rerun baselines without ad-hoc snippets.
- Gap Snapshot (2025-11-17): Phase A1 baseline evidence now exists under `reports/2025-10-vectorization/phase_a/` (collect log, pytest logs, env.json). A2/A3 remain outstanding—no microbenchmark harnesses yet—so Ralph must prioritise creating the reusable scripts before any implementation edits. docs/fix_plan.md item VECTOR-TRICUBIC-001 lists these as the gating next actions.
- Supervisory note (2025-11-17): fix_plan#VECTOR-TRICUBIC-001 continues to require the Phase A push (now narrowed to A2/A3). All guidance below assumes Ralph works from a clean editable install with `KMP_DUPLICATE_LIB_OK=TRUE` exported.

### Phase A — Evidence & Baseline Capture
Goal: Document the current (non-vectorized) behavior, warnings, and performance so we can prove the impact of the refactor.
Prereqs: Editable install (`pip install -e .`), C binary available for parity (when needed), environment variable `KMP_DUPLICATE_LIB_OK=TRUE`.
Exit Criteria: Baseline pytest log, timing snippet results, and notes stored under `reports/2025-10-vectorization/phase_a/` with references in docs/fix_plan.md Attempt history.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| A1 | Re-run tricubic acceptance tests | [D] | ✅ Completed 2025-10-06 (`reports/2025-10-vectorization/phase_a/test_at_str_002.log`, `.collect.log`, `test_at_abs_001.log`, `env.json`). Re-run only if dependencies change or new warnings appear. |
| A2 | Measure current tricubic runtime | [D] | ✅ Completed 2025-10-07 (`scripts/benchmarks/tricubic_baseline.py` created; CPU & CUDA baselines captured in `phase_a/tricubic_baseline.md` and `phase_a/tricubic_baseline_results.json`). Key finding: scalar-only implementation @ ~1.4 ms/call (CPU), ~5.5 ms/call (CUDA). |
| A3 | Record detector absorption baseline | [D] | ✅ Completed 2025-10-07 (`scripts/benchmarks/absorption_baseline.py` created; CPU & CUDA baselines captured in `phase_a/absorption_baseline.md` and `phase_a/absorption_baseline_results.json`). GPU shows 5.4× speedup for 512² detector. |

### Phase B — Tricubic Vectorization Design
Goal: Lock the tensor-shape design, broadcasting plan, and gradient checks before code changes.
[2025-10-07T07:27:01] tokens used: 35,347

[2025-10-07T07:27:08] thinking

**Reviewing CLI plan status and flags**
[2025-10-07T07:27:09] exec bash -lc 'rg "CLI-FLAGS-003" -n docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:10] bash -lc 'rg "CLI-FLAGS-003" -n docs/fix_plan.md' succeeded in 721ms:
21:| [CLI-FLAGS-003](#cli-flags-003-handle-nonoise-and-pix0_vector_mm) | Handle -nonoise and -pix0_vector_mm | High | in_progress |
448:## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
463:  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
[2025-10-07T07:27:10] tokens used: 38,130

[2025-10-07T07:27:12] thinking

**Navigating to specific section**
[2025-10-07T07:27:12] exec bash -lc "sed -n '448,640p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:13] bash -lc "sed -n '448,640p' docs/fix_plan.md" succeeded in 704ms:
## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
- Priority: High
- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
- Owner/Date: ralph/2025-10-05
- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
- Reproduction (C & PyTorch):
  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
- Next Actions (2025-11-17 parity refresh):
  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
- Attempts History:
  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
    Artifacts:
[2025-10-07T07:27:13] tokens used: 38,492

[2025-10-07T07:27:16] thinking

**Reviewing -nonoise feature status**
[2025-10-07T07:27:16] exec bash -lc 'rg "nonoise" -n plans/active/cli-noise-pix0/plan.md docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:17] bash -lc 'rg "nonoise" -n plans/active/cli-noise-pix0/plan.md docs/fix_plan.md' succeeded in 684ms:
plans/active/cli-noise-pix0/plan.md:3:- Phase Goal: Accept `-nonoise` and `-pix0_vector_mm` flags with C-equivalent semantics so the parallel comparison command in prompts/supervisor.md executes end-to-end.
plans/active/cli-noise-pix0/plan.md:42:| A1 | Extract C reference for `-nonoise` | [D] | ✅ 2025-10-05: Supervisor command captured with/without `-nonoise`; logs stored under `reports/2025-10-cli-flags/phase_a/`. |
plans/active/cli-noise-pix0/plan.md:54:| B2 | Thread `-nonoise` to simulator | [D] | `parse_and_validate_args()` records `suppress_noise`, and the noise writer at `__main__.py:1170-1199` skips SMV noise output when true. Add warning copy before closing Phase C. |
plans/active/cli-noise-pix0/plan.md:68:| C1 | Add CLI regression tests | [D] | ✅ 2025-10-16 (commit b6c6a61). `tests/test_cli_flags.py` covers meters/mm aliases, mutual exclusion, override persistence (CPU/CUDA), dtype preservation, and `-nonoise` suppression. |
docs/fix_plan.md:21:| [CLI-FLAGS-003](#cli-flags-003-handle-nonoise-and-pix0_vector_mm) | Handle -nonoise and -pix0_vector_mm | High | in_progress |
docs/fix_plan.md:448:## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
docs/fix_plan.md:455:  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
docs/fix_plan.md:456:  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
docs/fix_plan.md:979:      - `reports/2025-10-cli-flags/phase_a/c_with_nonoise.log` - C run WITH -nonoise
docs/fix_plan.md:980:      - `reports/2025-10-cli-flags/phase_a/c_with_noise.log` - C run WITHOUT -nonoise
docs/fix_plan.md:984:      - `-nonoise` suppresses noise parameters section AND noise image generation (noiseimage.img not written)
docs/fix_plan.md:985:      - Float/int/PGM images still generated when -nonoise is active
docs/fix_plan.md:1006:      - ✅ -nonoise suppresses noisefile generation (suppress_noise=True)
docs/fix_plan.md:1007:      - ✅ -nonoise doesn't mutate adc default (40.0) or seed handling
docs/fix_plan.md:1008:      - ✅ -noisefile without -nonoise enables noise generation (suppress_noise=False)
docs/fix_plan.md:1009:      - ✅ -nonoise valid without -noisefile (no-op but accepted)
docs/fix_plan.md:1061:      - `-nonoise` flag successfully suppressed noise file generation in both implementations
[2025-10-07T07:27:17] tokens used: 41,693

[2025-10-07T07:27:21] thinking

**Assessing vectorization plan and next steps**
[2025-10-07T07:27:21] exec bash -lc "python - <<'PY'
import random
print('heads' if random.random() < 0.5 else 'tails')
PY" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:22] bash -lc "python - <<'PY'
import random
print('heads' if random.random() < 0.5 else 'tails')
PY" succeeded in 774ms:
heads
[2025-10-07T07:27:22] tokens used: 42,876

[2025-10-07T07:27:26] thinking

**Reviewing recent commits by Ralph**
[2025-10-07T07:27:27] exec bash -lc 'git log -n 10 --oneline' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:27] bash -lc 'git log -n 10 --oneline' succeeded in 676ms:
4e8fd57 supervisor.md
a77ea6e [SYNC i=77] actor=galph status=running
a68ee62 [SYNC i=77] actor=ralph → next=galph status=ok ralph_commit=1a5c3e4
1a5c3e4 CLI-FLAGS-003 Phase L3a: HKL coverage re-verification complete
acab43b [SYNC i=76] actor=ralph status=running
9564cd4 [SYNC i=76] actor=galph → next=ralph status=ok galph_commit=a029a41
a029a41 SUPERVISOR: CLI-FLAGS L3 prep - tests: not run
552ecb5 update callchain.md
f713ed4 [SYNC i=76] actor=galph status=running
6c31ef2 [SYNC i=76] actor=ralph → next=galph status=ok ralph_commit=0426b1e
[2025-10-07T07:27:27] tokens used: 43,188

[2025-10-07T07:27:30] thinking

**Inspecting specific commit changes**
[2025-10-07T07:27:31] exec bash -lc 'git show 1a5c3e4' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:31] bash -lc 'git show 1a5c3e4' succeeded in 739ms:
commit 1a5c3e474b193002c2f69a984819518d2da2d7d9
Author: ollie <ohoidn>
Date:   Tue Oct 7 00:25:20 2025 -0700

    CLI-FLAGS-003 Phase L3a: HKL coverage re-verification complete
    
    Evidence-only loop confirming structure-factor coverage on current workstation.
    
    Executed probe.py against scaled.hkl, confirmed target reflection (-7,-1,-14)
    present with F_cell=190.27 matching C reference within 4.27e-06 delta.
    
    **Artifacts captured:**
    - probe_20251117.log: Fresh execution log (git SHA acab43b, Torch 2.8.0+cu128)
    - analysis_20251117.md: Updated findings confirming HKL coverage
    - collect_cli_scaling_20251117.log: Pytest collection validation (successful)
    
    **Key findings:**
    - HKL grid: 49×57×62, h∈[-24,24], k∈[-28,28], l∈[-31,30]
    - Target in range: TRUE
    - Lookup infrastructure works when HKL data properly attached
[2025-10-07T07:27:31] tokens used: 43,608

[2025-10-07T07:27:34] thinking

**Reviewing recent commits**
[2025-10-07T07:27:34] exec bash -lc 'git show 0426b1e' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:27:35] bash -lc 'git show 0426b1e' succeeded in 696ms:
commit 0426b1e581642b079902a374a0db6a717df4fd8c
Author: ollie <ohoidn>
Date:   Tue Oct 7 00:11:59 2025 -0700

    VECTOR-TRICUBIC-001 Phase A2/A3: Add tricubic & absorption baseline benchmarks
    
    - Created scripts/benchmarks/tricubic_baseline.py
      - Device/dtype neutral benchmark harness (CPU & CUDA support)
      - Measures scalar interpolation performance (100 calls per size)
      - Results: CPU ~1.4ms/call, CUDA ~5.5ms/call
    
    - Created scripts/benchmarks/absorption_baseline.py
      - Device/dtype neutral benchmark harness (CPU & CUDA support)
      - Measures detector absorption with layering
      - Results: CUDA 5.4× faster than CPU for 512² detector
    
    - Captured baseline artifacts in reports/2025-10-vectorization/phase_a/:
      - tricubic_baseline.md / .json (CPU & CUDA results)
      - absorption_baseline.md / .json (CPU & CUDA results)
      - run_log.txt (combined execution log)
[2025-10-07T07:27:35] tokens used: 46,572

[2025-10-07T07:28:08] thinking

**Analyzing commit and planning updates**

I’m examining the new scripts and commit details, noticing references to a possibly external path that might cause confusion. The plan for CLI L3 tasks needs updating since L3a is done but not marked completed, so I’m preparing to update the plan and fix_plan documents to reflect progress, adjust next steps, and gather relevant docs before editing input.md as part of a planning action.
[2025-10-07T07:28:10] exec bash -lc 'cat input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:11] bash -lc 'cat input.md' succeeded in 1.05s:
Summary: Reconfirm HKL coverage for the supervisor command so the scaling trace stops zeroing F_cell.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: pytest --collect-only -q tests/test_cli_scaling.py
Artifacts: reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log, reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md, reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt
Do Now: CLI-FLAGS-003 Phase L3a — `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/structure_factor/probe.py --pixel 685 1039 --hkl scaled.hkl --fdump reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_scaled_20251006175946.bin --dtype float32 --device cpu`
If Blocked: Copy the probe stdout/stderr to `reports/2025-10-cli-flags/phase_l/structure_factor/probe_blocker_20251117.log`, record the failure context in docs/fix_plan.md Attempt history for CLI-FLAGS-003, and halt before editing any source files.
Priorities & Rationale:
- docs/fix_plan.md:448 pinpoints F_cell=0 as the gating blocker for CLI-FLAGS-003 and now requires a fresh Phase L3a rerun.
- plans/active/cli-noise-pix0/plan.md:248 spells out the Phase L3a/L3b evidence steps that must precede any simulator edits.
- specs/spec-a-cli.md:1 keeps the -hkl/-nonoise flag contract authoritative while we debug ingestion.
- docs/development/c_to_pytorch_config_map.md:1 reminds us of C↔Py HKL parity rules that the probe must satisfy.
- reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1 documents the I_before_scaling divergence we are trying to clear.
How-To Map:
- Back up existing artifacts first: `cp reports/2025-10-cli-flags/phase_l/structure_factor/probe.log reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251007.log` (same for analysis.md) so the rerun has a clean diff trail.
- Run the probe command from repo root (see Do Now) and let it regenerate probe.log/analysis.md with current data.
- Immediately copy the fresh outputs to timestamped filenames listed under Artifacts (`cp probe.log probe_20251117.log`, etc.) before any additional runs overwrite them.
- Capture the HKL metadata snapshot by dumping the relevant keys from `trace_harness.py` (e.g., rerun harness or use a short Python snippet) and store results in `harness_hkl_state_20251117.txt`.
- Validate the pytest selector without executing tests: `KMP_DUPLICATE_LIB_OK=TRUE pytest --collect-only -q tests/test_cli_scaling.py > reports/2025-10-cli-flags/phase_l/structure_factor/collect_cli_scaling_20251117.log`.
[2025-10-07T07:28:11] tokens used: 51,689

[2025-10-07T07:28:17] thinking

**Preparing to update input and plan files**
[2025-10-07T07:28:18] exec bash -lc "sed -n '1,160p' reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:19] bash -lc "sed -n '1,160p' reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md" succeeded in 738ms:
# Structure-Factor Coverage Analysis

**Date:** 2025-10-07T07:23:29.479460Z
**Task:** CLI-FLAGS-003 Phase L3a
**Goal:** Verify structure-factor source for supervisor pixel

## Target Reflection

- **Miller index:** h=-7, k=-1, l=-14
- **C reference F_cell:** 190.27
- **Source:** `reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log:143-164`

## Data Sources Tested

See `probe.log` for detailed output from:
- `scaled.hkl` (HKL text file)
- Fdump binaries (if provided via --fdump)

## Findings

[2025-10-07T07:28:19] tokens used: 53,320

[2025-10-07T07:28:28] thinking

**Evaluating analysis completion and plan updates**

I'm weighing whether to mark L3a as done despite the analysis file still being a placeholder—commit logs suggest success but no filled summary yet. I'll align the plan state with the fix_plan attempt, mark L3a as done [D] but note the need to update analysis later, possibly as part of L3b. Next, I'll prepare instructions for Ralph in a new input.md and check the current branch status before editing.
[2025-10-07T07:28:29] exec bash -lc 'git status --short --branch' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:30] bash -lc 'git status --short --branch' succeeded in 945ms:
## feature/spec-based-2...origin/feature/spec-based-2
[2025-10-07T07:28:30] tokens used: 54,267

[2025-10-07T07:28:32] thinking

**Updating plan with patch details**
[2025-10-07T07:28:33] exec bash -lc "sed -n '1,120p' plans/active/cli-noise-pix0/plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:34] bash -lc "sed -n '1,120p' plans/active/cli-noise-pix0/plan.md" succeeded in 738ms:
## Context
- Initiative: CLI Parity for nanoBragg PyTorch vs C (supports long-term goal in prompts/supervisor.md)
- Phase Goal: Accept `-nonoise` and `-pix0_vector_mm` flags with C-equivalent semantics so the parallel comparison command in prompts/supervisor.md executes end-to-end.
- Dependencies: specs/spec-a-cli.md §§3.2–3.4, docs/architecture/detector.md §5, docs/development/c_to_pytorch_config_map.md (detector pivot + noise), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860 (flag behavior), docs/debugging/detector_geometry_checklist.md (pix0 validation), docs/development/testing_strategy.md §2 (CLI parity tests).
- Current gap snapshot (2025-10-18 refresh): Phase H4 parity landed (Attempt #25) with pix0 deltas < 2e-8 m and regression tolerances tightened; the remaining blocker before executing the supervisor command end-to-end is Phase I polarization alignment.
- Gap snapshot update (2025-10-19): Attempt #27 parity run reveals 1.24538e5× intensity scaling mismatch despite polarization fix; Phases J–L track the normalization diagnosis, fix, and closure required before long-term Goal #1 completes.
- Gap snapshot refresh (2025-10-22): Attempt #31 restored C precedence for custom detector vectors (pix0 override now gated again). Outstanding work: H5c PyTorch traces and Phase K normalization to eliminate the `F_latt`-driven intensity gap.
- Gap snapshot update (2025-10-24): Attempt #35 reran the PyTorch trace harness post-unit-fix; metrics still show |Δpix0| ≈ 1.145 mm, so a fresh diagnostics phase is required before resuming normalization work.
- Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
- Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
- Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
- Gap snapshot update (2025-11-11): Discovered the trace harness omitted `-beam_vector`, `-flux`, and `-beamsize`, forcing oversample=4 and a +Z beam; Py traces zeroed `F_cell` and inflated `steps`. Marked unresolved pending harness fix.
- Gap snapshot refresh (2025-11-12): Attempt #66 (commit 1d2e6ac) completed Phase L2b0 — the trace harness now feeds the full supervisor flag set, and the refreshed `trace_py_scaling.log` confirms `steps=10`, `fluence≈1.0e24`, and `r_e^2` parity with C. TRACE_PY still reports `I_before_scaling NOT_EXTRACTED`, `polar=0`, and `I_pixel_final=0`, so Phase L2b instrumentation must surface the real polarization/absorption tensors before we can quantify the K3e k_frac drift.
- Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
- Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
- Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
[2025-10-07T07:28:34] tokens used: 55,088

[2025-10-07T07:28:35] thinking

**Confirming context completion**
[2025-10-07T07:28:36] exec bash -lc "sed -n '120,240p' plans/active/cli-noise-pix0/plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:36] bash -lc "sed -n '120,240p' plans/active/cli-noise-pix0/plan.md" succeeded in 741ms:
Prereqs: Phases F and G complete; orientation traces captured (Phase G3 artifacts in place).
Exit Criteria: Trace comparison shows `pix0_vector` components matching C within 5e-5 m, `h`, `k`, `l` fractional components within 1e-3, `F_latt_a/b/c` within 0.5% (signed), and a parity rerun demonstrates ≥0.95 correlation improvement attributable to lattice fixes (polarization still disabled).

Update 2025-10-17: Attempt #22 proved the projection-based override mapping pushes pix0 off by 2.16e-1 m; Attempt #23 (now superseded) inferred the C code ignored `-pix0_vector_mm` when custom detector vectors are present. Phase J traces captured on 2025-10-21 demonstrate the override IS honoured (C recomputes `Fbeam/Sbeam` ≈0.2179/0.2139 m), so PyTorch must re-apply overrides even in the custom-vector path.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| H1 | Refresh trace evidence post-orientation | [D] | ✅ 2025-10-17: `reports/2025-10-cli-flags/phase_h/trace_harness.py` + `trace_comparison.md` captured the no-override trace and logged Attempt #19 (beam vector divergence documented). |
| H2 | Fix incident beam propagation from CLI | [D] | ✅ 2025-10-17 Attempt #20 (ralph). Simulator now clones `detector.beam_vector` (commit 8c1583d); regression covered by `tests/test_cli_flags.py::TestCLIBeamVector::test_custom_beam_vector_propagates`. Pending: rerun `phase_h/trace_harness.py` to confirm `incident_vec` parity before Phase H3. |
| H3a | Diagnose residual lattice mismatch | [D] | ✅ 2025-10-17 Attempt #21 (ralph). `trace_py_after_H3_refresh.log`, `pix0_reproduction.md`, and `implementation_notes.md` quantify the ~1.14 mm pix0 delta and show the cascade into scattering vector and `F_latt` disparities; `attempt_log.txt` restored with evidence summary. |
| H3b1 | Capture C vs PyTorch pix0/F/S references (override on/off) | [D] | ✅ 2025-10-17 Attempt #23: Stored paired C traces (`reports/2025-10-cli-flags/phase_h/implementation/c_trace_with_override.log` / `_without_override.log`), PyTorch traces, and `pix0_mapping_analysis.md`; evidence proves C geometry is identical with or without `-pix0_vector_mm` when custom vectors are supplied. |
| H3b2 | Implement pix0 precedence (custom vectors > override) | [D] | ✅ 2025-10-06 Attempt #24 (commit d6f158c). `Detector._calculate_pix0_vector` now skips overrides when any custom detector vector is supplied; override logic only executes in the no-custom-vectors path. Validation: `tests/test_cli_flags.py::TestCLIPix0Override::test_pix0_vector_mm_beam_pivot` (CPU) and evidence log `reports/2025-10-cli-flags/phase_h/implementation/pix0_override_validation.md`. Residual pix0 Y-delta (≈3.9 mm) traced to missing post-rotation `Fbeam/Sbeam` recomputation and deferred to H4. |
| H3b3 | Repair CLI regression + artifacts for precedence | [D] | ✅ 2025-10-06 Attempt #24. Regression test now covers BOTH precedence cases (custom vectors ignored, standard path applies override) and records expected C pix0 vector in `pix0_expected.json`. CUDA parametrisation remains skipped until parity fix lands; tolerance temporarily relaxed to 5 mm pending H4 geometry alignment. |
| H4a | Port post-rotation beam-centre recomputation | [D] | ✅ 2025-10-17 Attempt #25: Ported nanoBragg.c lines 1846-1860 into `Detector._calculate_pix0_vector`; see `reports/2025-10-cli-flags/phase_h/parity_after_lattice_fix/implementation.md`. |
| H4b | Refresh traces and parity evidence | [D] | ✅ 2025-10-17 Attempt #25: Stored C/PyTorch traces in `reports/2025-10-cli-flags/phase_h/parity_after_lattice_fix/trace_c.log` and `trace_py.log`; `summary.md` records pix0 deltas ≤1.3e-8 m and `F_latt` agreement <0.2%. |
| H4c | Tighten regression test + targeted pytest | [D] | ✅ 2025-10-17 Attempt #25: Updated regression tolerance to 5e-5 m (CPU and CUDA parametrised) and captured `pytest_h4c.log`; Attempt #25 logged in docs/fix_plan.md. |

### Phase H5 — Custom Vector Pix0 Override Parity
Goal: Bring PyTorch pix0 precedence back in line with C when custom detector vectors accompany `-pix0_vector_mm`, eliminating the 1.14 mm delta driving the `F_latt` mismatch.
Prereqs: Phase H4 artifacts available; C precedence proof in `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`.
[2025-10-07T07:28:36] tokens used: 57,389

[2025-10-07T07:28:39] thinking

**Searching for Phase L3 table**
[2025-10-07T07:28:40] exec bash -lc 'rg "Phase L3" -n plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:40] bash -lc 'rg "Phase L3" -n plans/active/cli-noise-pix0/plan.md' succeeded in 712ms:
248:### Phase L3 — Normalization Fix Implementation
[2025-10-07T07:28:40] tokens used: 59,055
[2025-10-07T07:28:42] exec bash -lc "sed -n '240,320p' plans/active/cli-noise-pix0/plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:43] bash -lc "sed -n '240,320p' plans/active/cli-noise-pix0/plan.md" succeeded in 771ms:
Exit Criteria: C and PyTorch traces capturing scaling factors are stored under `reports/2025-10-cli-flags/phase_l/scaling_audit/`, a comparison summary identifies the first mismatched term with quantitative deltas, and docs/fix_plan.md Attempt log references the findings.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
| L2b | Expose and log PyTorch scaling factors | [D] | ✅ Attempts #69, #72, and #74 (commits `1cf77b5`, `0f3de26`) now drive the harness through the updated HKL API, attach `crystal.hkl_data` / `crystal.hkl_metadata`, and capture live TRACE_PY output for pixel (685,1039). `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log` shows real polarization/omega/steps, and `harness_hkl_state_fixed.txt` confirms the HKL tensor lives on the simulator device. |
| L2c | Diff traces & record first divergence | [D] | ✅ Attempt #71 refreshed `scaling_audit_summary.md` via `scripts/validation/compare_scaling_traces.py`, highlighting `I_before_scaling` as the first divergence (C=9.4365e5, Py=0) while all downstream scaling factors match within 2e-3%. This establishes that the remaining gap stems from missing structure-factor coverage rather than normalization math. |

### Phase L3 — Normalization Fix Implementation
Goal: Patch the PyTorch scaling pipeline so every factor (omega, polarization, capture_fraction, r_e², fluence, steps) matches the C implementation within 1e-6 relative error.
Prereqs: L2 comparison summary pointing to the exact divergent factor; relevant C references documented; regression tests identified.
Exit Criteria: PyTorch source changes with inline C-code docstring references are merged, targeted regression tests cover the corrected scaling semantics, and new artifacts demonstrate matching intermediate values. docs/fix_plan.md records the fix attempt and resulting metrics.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
| L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
| L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
| L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |
| L3e | Validate scaling via script | [ ] | Enhance `scripts/validation/compare_scaling_chain.py` (or create it if missing) to consume the L2 traces plus the new structure-factor data, emit JSON under `reports/2025-10-cli-flags/phase_l/scaling_validation/metrics.json`, and fail loudly if any factor exceeds the agreed thresholds. |
[2025-10-07T07:28:43] tokens used: 59,310

[2025-10-07T07:28:49] thinking

**Preparing L3a state and guidance updates**
[2025-10-07T07:28:50] exec bash -lc "sed -n '120,200p' plans/active/cli-noise-pix0/plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:28:50] bash -lc "sed -n '120,200p' plans/active/cli-noise-pix0/plan.md" succeeded in 861ms:
Prereqs: Phases F and G complete; orientation traces captured (Phase G3 artifacts in place).
Exit Criteria: Trace comparison shows `pix0_vector` components matching C within 5e-5 m, `h`, `k`, `l` fractional components within 1e-3, `F_latt_a/b/c` within 0.5% (signed), and a parity rerun demonstrates ≥0.95 correlation improvement attributable to lattice fixes (polarization still disabled).

Update 2025-10-17: Attempt #22 proved the projection-based override mapping pushes pix0 off by 2.16e-1 m; Attempt #23 (now superseded) inferred the C code ignored `-pix0_vector_mm` when custom detector vectors are present. Phase J traces captured on 2025-10-21 demonstrate the override IS honoured (C recomputes `Fbeam/Sbeam` ≈0.2179/0.2139 m), so PyTorch must re-apply overrides even in the custom-vector path.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| H1 | Refresh trace evidence post-orientation | [D] | ✅ 2025-10-17: `reports/2025-10-cli-flags/phase_h/trace_harness.py` + `trace_comparison.md` captured the no-override trace and logged Attempt #19 (beam vector divergence documented). |
| H2 | Fix incident beam propagation from CLI | [D] | ✅ 2025-10-17 Attempt #20 (ralph). Simulator now clones `detector.beam_vector` (commit 8c1583d); regression covered by `tests/test_cli_flags.py::TestCLIBeamVector::test_custom_beam_vector_propagates`. Pending: rerun `phase_h/trace_harness.py` to confirm `incident_vec` parity before Phase H3. |
| H3a | Diagnose residual lattice mismatch | [D] | ✅ 2025-10-17 Attempt #21 (ralph). `trace_py_after_H3_refresh.log`, `pix0_reproduction.md`, and `implementation_notes.md` quantify the ~1.14 mm pix0 delta and show the cascade into scattering vector and `F_latt` disparities; `attempt_log.txt` restored with evidence summary. |
| H3b1 | Capture C vs PyTorch pix0/F/S references (override on/off) | [D] | ✅ 2025-10-17 Attempt #23: Stored paired C traces (`reports/2025-10-cli-flags/phase_h/implementation/c_trace_with_override.log` / `_without_override.log`), PyTorch traces, and `pix0_mapping_analysis.md`; evidence proves C geometry is identical with or without `-pix0_vector_mm` when custom vectors are supplied. |
| H3b2 | Implement pix0 precedence (custom vectors > override) | [D] | ✅ 2025-10-06 Attempt #24 (commit d6f158c). `Detector._calculate_pix0_vector` now skips overrides when any custom detector vector is supplied; override logic only executes in the no-custom-vectors path. Validation: `tests/test_cli_flags.py::TestCLIPix0Override::test_pix0_vector_mm_beam_pivot` (CPU) and evidence log `reports/2025-10-cli-flags/phase_h/implementation/pix0_override_validation.md`. Residual pix0 Y-delta (≈3.9 mm) traced to missing post-rotation `Fbeam/Sbeam` recomputation and deferred to H4. |
| H3b3 | Repair CLI regression + artifacts for precedence | [D] | ✅ 2025-10-06 Attempt #24. Regression test now covers BOTH precedence cases (custom vectors ignored, standard path applies override) and records expected C pix0 vector in `pix0_expected.json`. CUDA parametrisation remains skipped until parity fix lands; tolerance temporarily relaxed to 5 mm pending H4 geometry alignment. |
| H4a | Port post-rotation beam-centre recomputation | [D] | ✅ 2025-10-17 Attempt #25: Ported nanoBragg.c lines 1846-1860 into `Detector._calculate_pix0_vector`; see `reports/2025-10-cli-flags/phase_h/parity_after_lattice_fix/implementation.md`. |
| H4b | Refresh traces and parity evidence | [D] | ✅ 2025-10-17 Attempt #25: Stored C/PyTorch traces in `reports/2025-10-cli-flags/phase_h/parity_after_lattice_fix/trace_c.log` and `trace_py.log`; `summary.md` records pix0 deltas ≤1.3e-8 m and `F_latt` agreement <0.2%. |
| H4c | Tighten regression test + targeted pytest | [D] | ✅ 2025-10-17 Attempt #25: Updated regression tolerance to 5e-5 m (CPU and CUDA parametrised) and captured `pytest_h4c.log`; Attempt #25 logged in docs/fix_plan.md. |

### Phase H5 — Custom Vector Pix0 Override Parity
Goal: Bring PyTorch pix0 precedence back in line with C when custom detector vectors accompany `-pix0_vector_mm`, eliminating the 1.14 mm delta driving the `F_latt` mismatch.
Prereqs: Phase H4 artifacts available; C precedence proof in `reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`.
[2025-10-07T07:28:50] tokens used: 61,305

[2025-10-07T07:28:57] thinking

**Inserting update and revising plan table**

I’m preparing to add a new bullet about the 2025-11-17 gap snapshot refresh right after the 2025-11-15 entry, keeping style consistent. Then I’ll update the Phase L3 table state to `[D]` and adjust guidance to reflect success on attempt #77, highlight next steps, and mention the need for an analysis.md summary without blocking progress.
[2025-10-07T07:29:04] tokens used: 63,863

[2025-10-07T07:29:07] thinking

**Investigating patch application failure**
[2025-10-07T07:29:09] exec bash -lc 'rg "Attempts #69/#71/#74" -n plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:29:09] bash -lc 'rg "Attempts #69/#71/#74" -n plans/active/cli-noise-pix0/plan.md' succeeded in 932ms:
18:- Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
[2025-10-07T07:29:09] tokens used: 65,289

[2025-10-07T07:29:12] thinking

**Preparing to patch code with exact line match**
[2025-10-07T07:29:17] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/plans/active/cli-noise-pix0/plan.md
@@ -18,2 +18,3 @@
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
@@ -254,3 +255,3 @@
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
[2025-10-07T07:29:17] apply_patch(auto_approved=true) exited 0 in 16ms:
Success. Updated the following files:
M plans/active/cli-noise-pix0/plan.md
[2025-10-07T07:29:17] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |

[2025-10-07T07:29:17] tokens used: 66,232
[2025-10-07T07:29:17] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:29:20] thinking

**Updating documentation and next steps**
[2025-10-07T07:29:20] exec bash -lc "sed -n '450,520p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:29:21] bash -lc "sed -n '450,520p' docs/fix_plan.md" succeeded in 1.01s:
- Priority: High
- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
- Owner/Date: ralph/2025-10-05
- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
- Reproduction (C & PyTorch):
  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
- Next Actions (2025-11-17 parity refresh):
  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
- Attempts History:
  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
    Artifacts:
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log:1-41` - Refreshed trace with live values (polar=0.91464138, omega_pixel=4.20404859e-07 sr, F_cell=0, I_before_scaling=0)
      - `reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1-54` - Phase L2c comparison report showing first divergence and all divergent factors
[2025-10-07T07:29:21] tokens used: 67,133
[2025-10-07T07:29:21] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:29:30] thinking

**Revising next actions list**
[2025-10-07T07:29:37] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -458,7 +458,7 @@
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
[2025-10-07T07:29:37] apply_patch(auto_approved=true) exited 0 in 27ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T07:29:37] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |

[2025-10-07T07:29:37] tokens used: 70,104
[2025-10-07T07:29:37] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:29:53] thinking

**Preparing detailed next steps and input update**

I’m working to finalize the updated plan and align the new Do Now instructions for Ralph, focusing on running a harness audit involving possible code changes to `trace_harness.py`. I’m weighing which Mode flag fits best—Docs or Parity—and gathering details on how to run the harness script to craft precise guidance and commands before updating the input summary.
[2025-10-07T07:29:55] exec bash -lc 'head -n 40 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:29:56] bash -lc 'head -n 40 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py' succeeded in 938ms:
#!/usr/bin/env python
"""
Phase L2b PyTorch Scaling Trace Harness

Captures the PyTorch side of the scaling chain comparison for supervisor command.
Mirrors the C trace fields from reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log.

Date: 2025-10-17
Engineer: Ralph (loop i=62, evidence-only mode)
Plan Reference: plans/active/cli-noise-pix0/plan.md Phase L2b
"""

import argparse
import json
import sys
import platform
import os
import torch
from pathlib import Path
from datetime import datetime
[2025-10-07T07:29:56] tokens used: 72,044
[2025-10-07T07:29:56] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:29:58] thinking

**Checking command usage details**
[2025-10-07T07:29:58] exec bash -lc 'rg "def main" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:29:59] bash -lc 'rg "def main" -n reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py' succeeded in 699ms:
107:def main():
[2025-10-07T07:29:59] tokens used: 72,671
[2025-10-07T07:29:59] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |

[2025-10-07T07:30:01] exec bash -lc "sed -n '107,240p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:02] bash -lc "sed -n '107,240p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" succeeded in 745ms:
def main():
    parser = create_parser()
    args = parser.parse_args()

    # Resolve dtype and device (Step 3)
    dtype = torch.float32 if args.dtype == 'float32' else torch.float64
    device = torch.device(args.device)

    # Log environment (Step 9 per input.md)
    env_info = {
        'python_version': sys.version,
        'torch_version': torch.__version__,
        'platform': platform.platform(),
        'cuda_available': torch.cuda.is_available(),
        'device': str(device),
        'dtype': args.dtype,
        'git_sha': os.popen('git rev-parse HEAD').read().strip(),
        'timestamp_iso': datetime.utcnow().isoformat() + 'Z'
    }

[2025-10-07T07:30:02] tokens used: 72,914
[2025-10-07T07:30:02] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:03] thinking

**Verifying completeness**
[2025-10-07T07:30:04] exec bash -lc "sed -n '240,400p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:05] bash -lc "sed -n '240,400p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" succeeded in 746ms:
    # automatically emits TRACE_PY lines to stdout containing real computed values
    slow, fast = args.pixel
    debug_config = {'trace_pixel': [slow, fast]}

    simulator = Simulator(
        crystal, detector,
        beam_config=beam_config,
        dtype=dtype, device=device,
        debug_config=debug_config
    )

    # Capture stdout during simulation run (Step 6 per input.md)
    # Use contextlib.redirect_stdout to capture TRACE_PY output
    import io
    from contextlib import redirect_stdout

    stdout_capture = io.StringIO()
    with redirect_stdout(stdout_capture):
        intensities = simulator.run()

[2025-10-07T07:30:05] tokens used: 75,164
[2025-10-07T07:30:05] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:11] thinking

**Reviewing trace harness attachment and next steps**
[2025-10-07T07:30:11] exec bash -lc "sed -n '40,120p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:12] bash -lc "sed -n '40,120p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" succeeded in 731ms:
    parser = argparse.ArgumentParser(description="PyTorch scaling trace harness")
    parser.add_argument('--pixel', type=int, nargs=2, required=True, metavar=('SLOW', 'FAST'),
                        help='Pixel coordinates to trace (slow, fast)')
    parser.add_argument('--out', type=str, required=True,
                        help='Output trace filename (e.g., trace_py_scaling.log)')
    parser.add_argument('--config', type=str, choices=['supervisor'], required=True,
                        help='Config preset to use (supervisor = CLI-FLAGS-003 supervisor command)')
    parser.add_argument('--device', type=str, default='cpu',
                        help='Device (cpu or cuda)')
    parser.add_argument('--dtype', type=str, default='float32', choices=['float32', 'float64'],
                        help='Tensor dtype (default float32)')
    return parser


def get_supervisor_params(dtype, device):
    """
    Return supervisor command parameter dict (not yet instantiated as config objects).

    Per input.md Step 3.
    """
[2025-10-07T07:30:12] tokens used: 76,038
[2025-10-07T07:30:12] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:34] thinking

**Preparing detailed harness audit instructions**

I'm planning a precise step-by-step guide for running and verifying the trace harness focused on aligning HKL attachments with scaled data, including commands, expected artifacts, test mappings, and references to the fix plan and specs to help the user audit and document crystal metadata properly.
[2025-10-07T07:30:35] exec bash -lc "nl -ba docs/fix_plan.md | sed -n '448,468p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:36] bash -lc "nl -ba docs/fix_plan.md | sed -n '448,468p'" succeeded in 912ms:
   448	## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
   449	- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
   450	- Priority: High
   451	- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
   452	- Owner/Date: ralph/2025-10-05
   453	- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
   454	- Reproduction (C & PyTorch):
   455	  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   456	  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
   457	- First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
   458	- Next Actions (2025-11-17 parity refresh):
   459	  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
   460	  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
   461	  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
   462	  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
   463	  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
   464	- Attempts History:
   465	  * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
   466	    Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
   467	    Artifacts:
[2025-10-07T07:30:36] tokens used: 79,236
[2025-10-07T07:30:36] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:38] thinking

**Planning line number usage**
[2025-10-07T07:30:39] exec bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '240,280p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:40] bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '240,280p'" succeeded in 697ms:
   240	Prereqs: Phase L1d artifacts in place; Ralph working under `prompts/debug.md`; C binary instrumentable via `golden_suite_generator/` per CLAUDE Rule #0.3.
   241	Exit Criteria: C and PyTorch traces capturing scaling factors are stored under `reports/2025-10-cli-flags/phase_l/scaling_audit/`, a comparison summary identifies the first mismatched term with quantitative deltas, and docs/fix_plan.md Attempt log references the findings.
   242	
   243	| ID | Task Description | State | How/Why & Guidance |
   244	| --- | --- | --- | --- |
   245	| L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
   246	| L2b | Expose and log PyTorch scaling factors | [D] | ✅ Attempts #69, #72, and #74 (commits `1cf77b5`, `0f3de26`) now drive the harness through the updated HKL API, attach `crystal.hkl_data` / `crystal.hkl_metadata`, and capture live TRACE_PY output for pixel (685,1039). `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log` shows real polarization/omega/steps, and `harness_hkl_state_fixed.txt` confirms the HKL tensor lives on the simulator device. |
   247	| L2c | Diff traces & record first divergence | [D] | ✅ Attempt #71 refreshed `scaling_audit_summary.md` via `scripts/validation/compare_scaling_traces.py`, highlighting `I_before_scaling` as the first divergence (C=9.4365e5, Py=0) while all downstream scaling factors match within 2e-3%. This establishes that the remaining gap stems from missing structure-factor coverage rather than normalization math. |
   248	
   249	### Phase L3 — Normalization Fix Implementation
   250	Goal: Patch the PyTorch scaling pipeline so every factor (omega, polarization, capture_fraction, r_e², fluence, steps) matches the C implementation within 1e-6 relative error.
   251	Prereqs: L2 comparison summary pointing to the exact divergent factor; relevant C references documented; regression tests identified.
   252	Exit Criteria: PyTorch source changes with inline C-code docstring references are merged, targeted regression tests cover the corrected scaling semantics, and new artifacts demonstrate matching intermediate values. docs/fix_plan.md records the fix attempt and resulting metrics.
   253	
   254	| ID | Task Description | State | How/Why & Guidance |
   255	| --- | --- | --- | --- |
   256	| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
   257	| L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
   258	| L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
   259	| L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |
[2025-10-07T07:30:40] tokens used: 80,433
[2025-10-07T07:30:40] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:43] thinking

**Searching for CLI option references**
[2025-10-07T07:30:43] exec bash -lc 'rg "-hkl" -n specs/spec-a-cli.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:44] bash -lc 'rg "-hkl" -n specs/spec-a-cli.md' exited 2 in 730ms:
rg: unrecognized flag -k
[2025-10-07T07:30:44] tokens used: 82,269
[2025-10-07T07:30:44] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:47] thinking

**Fixing ripgrep argument quoting**
[2025-10-07T07:30:47] exec bash -lc 'rg "-hkl" specs/spec-a-cli.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:48] bash -lc 'rg "-hkl" specs/spec-a-cli.md' exited 2 in 727ms:
rg: unrecognized flag -k
[2025-10-07T07:30:48] tokens used: 82,439
[2025-10-07T07:30:48] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:51] thinking

**Considering ripgrep pattern handling**
[2025-10-07T07:30:52] exec bash -lc "rg -- '-hkl' specs/spec-a-cli.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:52] bash -lc "rg -- '-hkl' specs/spec-a-cli.md" succeeded in 772ms:
    - -hkl <file>: Text file of “h k l F” (integers h,k,l; real F). Required unless Fdump.bin exists
    - On reading -hkl, the program writes a binary dump Fdump.bin to speed subsequent runs.
    - If -hkl is not provided, it attempts to read Fdump.bin. If neither is available and -default_F
- If -hkl is missing and Fdump.bin is missing and -default_F is zero → exit with usage.
  - Expectation: Prints usage including at least: -hkl, -mat, -cell, -pixel, -detpixels, -distance, -lambda|-energy, -floatfile, -intfile, -noisefile, -pgmfile, -scale, -adc, -mosflm/-xds/-adxv/-denzo/-dials, -roi. Exit code indicates success.
  - Setup: Invoke nanoBragg without -hkl and without an Fdump.bin and with -default_F=0.
[2025-10-07T07:30:52] tokens used: 82,577
[2025-10-07T07:30:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:56] thinking

**Adding line numbers for context**
[2025-10-07T07:30:56] exec bash -lc "nl -ba specs/spec-a-cli.md | sed -n '40,80p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:30:57] bash -lc "nl -ba specs/spec-a-cli.md | sed -n '40,80p'" succeeded in 720ms:
    40	Sets pivot to SAMPLE.
    41	    - -point_pixel: Use 1/R^2 solid-angle only (no obliquity).
    42	- Detector orientation:
    43	    - -detector_rotx <deg>, -detector_roty <deg>, -detector_rotz <deg>: Detector rotations about lab
    44	axes (radians internally).
    45	    - -twotheta <deg>: Detector rotation about twotheta_axis.
    46	    - -twotheta_axis x y z: Axis (unit vector) for -twotheta.
    47	    - -curved_det: Treat detector pixels as lying on a sphere so all pixels are equidistant from
    48	sample. Switch only; no argument used.
    49	- Detector absorption:
    50	    - -detector_abs <µm>|inf|0: Attenuation depth (µm); “inf” or 0 disable absorption (μ=0,
    51	thickness=0).
    52	    - -detector_thick <µm>: Sensor thickness (µm).
    53	    - -detector_thicksteps <int> or -thicksteps <int>: Discretization layers through the thickness.
    54	- Beam/conventions:
    55	    - -mosflm, -xds, -adxv, -denzo, -dials: Predefined detector/beam conventions (see Geometry
    56	& conventions).
    57	    - -pivot beam|sample: Override pivot; otherwise set by convention or certain flags as noted.
    58	    - -fdet_vector x y z, -sdet_vector x y z, -odet_vector x y z, -beam_vector x y z, -polar_vector
    59	x y z, -spindle_axis x y z, -twotheta_axis x y z: Custom unit vectors; sets convention to CUSTOM.
[2025-10-07T07:30:57] tokens used: 82,929
[2025-10-07T07:30:57] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:30:59] thinking

**Searching for -hkl references**
[2025-10-07T07:31:00] exec bash -lc "nl -ba specs/spec-a-cli.md | sed -n '80,140p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:31:01] bash -lc "nl -ba specs/spec-a-cli.md | sed -n '80,140p'" succeeded in 719ms:
    80	- Crystal size/shape and mosaicity:
    81	    - -Na <int>, -Nb <int>, -Nc <int>, -N <int>: Number of unit cells along a,b,c. Any values ≤1 are
    82	clamped to 1 later (zero is not preserved).
    83	    - -samplesize <mm> or -xtalsize <mm>: Set crystal full widths in x=y=z (mm); alternative:
    84	-sample_thick|_x, -sample_width|_y|-width, -sample_height|_z|-height, and -xtal_thick|_x,
    85	-xtal_width|_y|-width, -xtal_height|_z|-height for each dimension.
    86	    - -square_xtal (default), -round_xtal, -gauss_xtal, -binary_spots/-tophat_spots: Spot shapes.
    87	See “Lattice factors”.
    88	    - -fudge <value>: Shape parameter scaling (dimensionless).
    89	    - -mosaic, -mosaici, or -mosaic_spr <deg>: Isotropic mosaic spread (degrees).
    90	    - -mosaic_dom <int>: Number of mosaic domains.
    91	- Sampling:
    92	    - -phi <deg>: Starting spindle rotation angle.
    93	    - -osc <deg>: Oscillation range.
    94	    - -phistep <deg>: Step size.
    95	    - -phisteps <int>: Number of steps.
    96	    - -dmin <Å>: Minimum d-spacing cutoff. Subpaths with computed d < dmin are skipped.
    97	    - -oversample <int>: Sub-pixel sampling per axis (square grid).
    98	    - -oversample_thick: Recompute absorption per subpixel (see “Normalization” caveat).
    99	    - -oversample_polar: Recompute polarization per subpixel (see “Normalization” caveat).
[2025-10-07T07:31:01] tokens used: 84,064
[2025-10-07T07:31:01] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:31:42] thinking

**Outlining detailed input.md content**

I’m gathering all the key references, commands, and instructions needed to create a comprehensive input.md file that covers fix_plan actions, relevant docs, tests, commands for running the harness, and how to handle logs. I’m balancing the need to specify exact Do Now commands (even if not pytest-based), including file pointers, and clarifying testing strategy to align with harness fixes. Now I’ll prepare the structured markdown with bullets and sections as required.
[2025-10-07T07:32:05] exec bash -lc "cat <<'EOF' > input.md
Summary: Attach scaled.hkl data in the scaling trace harness so TRACE_PY reports F_cell=190.27 for the supervisor pixel.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: pytest --collect-only -q tests/test_cli_scaling.py
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt, reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md
Do Now: CLI-FLAGS-003 Phase L3c harness audit — KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --device cpu --dtype float32 --out trace_py_scaling.log
If Blocked: Capture stdout/stderr to reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness_failure_20251117.log, note the exception in docs/fix_plan.md Attempt history, and pause before modifying code.
Priorities & Rationale:
- docs/fix_plan.md:448-463 lists the updated CLI-FLAGS-003 next steps, starting with the L3c harness audit.
- plans/active/cli-noise-pix0/plan.md:249-261 marks L3a done and keeps L3c open pending scaled.hkl ingestion fixes.
- specs/spec-a-cli.md:60-120 defines -pix0_vector_mm, -hkl, and -nonoise semantics we must mirror when loading structure factors.
- docs/development/c_to_pytorch_config_map.md:1-40 reinforces the HKL parity rules and device expectations for CLI ingestion.
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1 confirms scaled.hkl already contains F_cell=190.27 for hkl=(-7,-1,-14).
How-To Map:
- Backup the current harness outputs: cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_pre_L3c.log (same for config_snapshot.json and harness_hkl_state_fixed.txt if present).
- Run the harness command from repo root (see Do Now); stdout should list the refreshed TRACE_PY lines and environment snapshot path.
- Copy the new trace to a timestamped file: cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log.
- Dump the HKL metadata actually attached by the harness with: PYTHONPATH=src python - <<'PY' > reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt
from nanobrag_torch.io.hkl import read_hkl_file
F, meta = read_hkl_file('scaled.hkl', default_F=0.0)
print(meta)
PY
- Update reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md with a short summary of the probe + harness results (target reflection present, F_cell=190.270004, harness now attaches metadata).
- Validate the mapped selector without running tests: KMP_DUPLICATE_LIB_OK=TRUE pytest --collect-only -q tests/test_cli_scaling.py > reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log.
- Record a quick diff note against config_snapshot.json and hkl state in docs/fix_plan.md Attempt log once evidence is captured.
Pitfalls To Avoid:
- Do not edit simulator scaling code yet—L3c limits work to harness/CLI ingestion.
- Keep KMP_DUPLICATE_LIB_OK=TRUE on every Python invocation to avoid MKL errors.
- Preserve device/dtype neutrality; no implicit .cpu()/.to() inside the harness edits.
- Maintain vectorized flows—no new per-pixel loops when touching trace_harness.py.
- Copy existing artifacts before overwriting; avoid clobbering earlier evidence.
- Leave Protected Assets from docs/index.md untouched (loop.sh, supervisor.sh, input.md).
- Do not run the full pytest suite; stay with the collect-only selector this loop.
- Avoid adding "'`.item()` calls on tensors that may need gradients later.
- Keep new logs under reports/… with timestamps; no stray files in repo root.
- Skip CLI code edits until the harness evidence proves the ingestion path.
Pointers:
- docs/fix_plan.md:448
- plans/active/cli-noise-pix0/plan.md:249
- specs/spec-a-cli.md:60
- docs/development/c_to_pytorch_config_map.md:1
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:1
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1
Next Up: 1. Audit src/nanobrag_torch/__main__.py HKL loading to match the harness; 2. Add structure-factor lookup regression in tests/test_cli_scaling.py once HKL ingestion is fixed.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:32:06] bash -lc "cat <<'EOF' > input.md
Summary: Attach scaled.hkl data in the scaling trace harness so TRACE_PY reports F_cell=190.27 for the supervisor pixel.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: pytest --collect-only -q tests/test_cli_scaling.py
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt, reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md
Do Now: CLI-FLAGS-003 Phase L3c harness audit — KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --device cpu --dtype float32 --out trace_py_scaling.log
If Blocked: Capture stdout/stderr to reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness_failure_20251117.log, note the exception in docs/fix_plan.md Attempt history, and pause before modifying code.
Priorities & Rationale:
- docs/fix_plan.md:448-463 lists the updated CLI-FLAGS-003 next steps, starting with the L3c harness audit.
- plans/active/cli-noise-pix0/plan.md:249-261 marks L3a done and keeps L3c open pending scaled.hkl ingestion fixes.
- specs/spec-a-cli.md:60-120 defines -pix0_vector_mm, -hkl, and -nonoise semantics we must mirror when loading structure factors.
- docs/development/c_to_pytorch_config_map.md:1-40 reinforces the HKL parity rules and device expectations for CLI ingestion.
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1 confirms scaled.hkl already contains F_cell=190.27 for hkl=(-7,-1,-14).
How-To Map:
- Backup the current harness outputs: cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_pre_L3c.log (same for config_snapshot.json and harness_hkl_state_fixed.txt if present).
- Run the harness command from repo root (see Do Now); stdout should list the refreshed TRACE_PY lines and environment snapshot path.
- Copy the new trace to a timestamped file: cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log.
- Dump the HKL metadata actually attached by the harness with: PYTHONPATH=src python - <<'PY' > reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt
from nanobrag_torch.io.hkl import read_hkl_file
F, meta = read_hkl_file('scaled.hkl', default_F=0.0)
print(meta)
PY
- Update reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md with a short summary of the probe + harness results (target reflection present, F_cell=190.270004, harness now attaches metadata).
- Validate the mapped selector without running tests: KMP_DUPLICATE_LIB_OK=TRUE pytest --collect-only -q tests/test_cli_scaling.py > reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log.
- Record a quick diff note against config_snapshot.json and hkl state in docs/fix_plan.md Attempt log once evidence is captured.
Pitfalls To Avoid:
- Do not edit simulator scaling code yet—L3c limits work to harness/CLI ingestion.
- Keep KMP_DUPLICATE_LIB_OK=TRUE on every Python invocation to avoid MKL errors.
- Preserve device/dtype neutrality; no implicit .cpu()/.to() inside the harness edits.
- Maintain vectorized flows—no new per-pixel loops when touching trace_harness.py.
- Copy existing artifacts before overwriting; avoid clobbering earlier evidence.
- Leave Protected Assets from docs/index.md untouched (loop.sh, supervisor.sh, input.md).
- Do not run the full pytest suite; stay with the collect-only selector this loop.
- Avoid adding "'`.item()` calls on tensors that may need gradients later.
- Keep new logs under reports/… with timestamps; no stray files in repo root.
- Skip CLI code edits until the harness evidence proves the ingestion path.
Pointers:
- docs/fix_plan.md:448
- plans/active/cli-noise-pix0/plan.md:249
- specs/spec-a-cli.md:60
- docs/development/c_to_pytorch_config_map.md:1
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:1
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1
Next Up: 1. Audit src/nanobrag_torch/__main__.py HKL loading to match the harness; 2. Add structure-factor lookup regression in tests/test_cli_scaling.py once HKL ingestion is fixed.
EOF
' succeeded in 914ms:

[2025-10-07T07:32:06] tokens used: 89,506
[2025-10-07T07:32:06] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:32:15] thinking

**Planning to expand content length**

I want to grow the file from about 47 lines to closer to 100 by adding more detailed bullets and elaborations in sections like How-To and Pitfalls, making sure we meet the user’s guideline of roughly 100–200 lines without padding unnecessarily.
[2025-10-07T07:32:35] exec bash -lc "cat <<'EOF' > input.md
Summary: Attach scaled.hkl into the scaling trace harness so TRACE_PY reports F_cell=190.27 for the supervisor pixel and the harness evidence matches the new probe run.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: pytest --collect-only -q tests/test_cli_scaling.py
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt, reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md
Do Now: CLI-FLAGS-003 Phase L3c harness audit — KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --device cpu --dtype float32 --out trace_py_scaling.log
If Blocked: Capture stdout/stderr to reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness_failure_20251117.log, note the exception (command + stack) in docs/fix_plan.md Attempt history for CLI-FLAGS-003, and stop before editing simulator or CLI code.
Priorities & Rationale:
- docs/fix_plan.md:448-463 records the refreshed next actions; L3c harness audit is explicitly the first todo after the probe success.
- plans/active/cli-noise-pix0/plan.md:249-261 promotes L3a to [D] and keeps L3c/L3d open until scaled.hkl is threaded through harness + CLI.
- specs/spec-a-cli.md:60-120 defines the -hkl / Fdump precedence and -pix0_vector semantics that must remain correct when we change ingestion.
- docs/development/c_to_pytorch_config_map.md:1-40 reiterates that HKL tensors must stay on the caller device/dtype; the harness should respect that contract.
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1 proves F_cell=190.270004 already lives in scaled.hkl, so the harness must surface it without fallback hacks.
- reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1 pinpoints "'`I_before_scaling` (F_cell=0) as the first divergence, so today’s evidence should replace that zero.
How-To Map:
- Ensure env vars are in place: export KMP_DUPLICATE_LIB_OK=TRUE and confirm NB_C_BIN still points at golden_suite_generator/nanoBragg for later comparisons.
- Snapshot existing artifacts so the rerun diff is clean: cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_pre_L3c.log; cp reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot_pre_L3c.json; cp reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_fixed.txt reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_pre_L3c.txt (if the file exists).
- Run the harness command from repo root (see Do Now). Expect the run to emit TRACE_PY lines, config snapshot, env snapshot, and the final pixel intensity summary.
- Immediately promote the refreshed outputs to timestamped names so earlier evidence is preserved:
  - cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log
  - cp reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot_20251117.json
  - cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env_20251117.json
- Dump the HKL metadata actually attached by the harness so we can compare ranges against the earlier probe: PYTHONPATH=src python - <<'"'PY' > reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt
from nanobrag_torch.io.hkl import read_hkl_file
F, meta = read_hkl_file('scaled.hkl', default_F=0.0)
for key in sorted(meta):
    print(f\"{key}: {meta[key]}\")
PY
- Inspect the new trace to ensure F_cell now matches: grep 'TRACE_PY: F_cell' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log
- Update reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md with two short bullet sections: (1) probe summary (grid ranges, F_cell), (2) harness confirmation (metadata snapshot path, F_cell match, remaining deltas if any).
- Re-run the mapped selector to confirm tests still collect: KMP_DUPLICATE_LIB_OK=TRUE pytest --collect-only -q tests/test_cli_scaling.py > reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log
- Record a short diff note under docs/fix_plan.md Attempt history describing the harness rerun, file names, and whether F_cell still needs work.
- Stage but do not commit until supervisor loop ends; keep notes about any harness edits you make so we can document them explicitly.
Pitfalls To Avoid:
- Do not touch simulator scaling code or CLI parser yet; L3c is about evidence and harness parity only.
- No new ad-hoc scripts in repo root—keep tooling under reports/… per SOP.
- Avoid "'`.item()` on tensors destined for gradients; if you need scalar logging, use `.detach().cpu().item()` in test-only code paths.
- Keep harness device/dtype neutral; never hard-code CPU tensors when running on CUDA.
- Do not delete prior evidence logs; copy-before-overwrite is mandatory for reproducibility.
- Stay on the supervisor branch; no rebases or merges mid-loop.
- Resist rerunning the full parity command until plan Phase L3 closes; nb-compare waits for L4.
- Maintain Protected Assets (`docs/index.md` list) untouched.
- Keep Mode Parity focus—avoid perf tweaks or vectorization work in this loop.
- Capture all new files with descriptive timestamps to simplify archive later.
Pointers:
- docs/fix_plan.md:448
- plans/active/cli-noise-pix0/plan.md:249
- specs/spec-a-cli.md:60
- docs/development/c_to_pytorch_config_map.md:1
- docs/development/testing_strategy.md:1
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:1
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1
- reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1
Next Up: 1. Audit src/nanobrag_torch/__main__.py HKL loading to mirror the harness attachment pattern; 2. Add a structure-factor + intensity regression in tests/test_cli_scaling.py once HKL ingestion is fixed.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:32:36] bash -lc "cat <<'EOF' > input.md
Summary: Attach scaled.hkl into the scaling trace harness so TRACE_PY reports F_cell=190.27 for the supervisor pixel and the harness evidence matches the new probe run.
Mode: Parity
Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
Branch: feature/spec-based-2
Mapped tests: pytest --collect-only -q tests/test_cli_scaling.py
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt, reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md
Do Now: CLI-FLAGS-003 Phase L3c harness audit — KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --device cpu --dtype float32 --out trace_py_scaling.log
If Blocked: Capture stdout/stderr to reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness_failure_20251117.log, note the exception (command + stack) in docs/fix_plan.md Attempt history for CLI-FLAGS-003, and stop before editing simulator or CLI code.
Priorities & Rationale:
- docs/fix_plan.md:448-463 records the refreshed next actions; L3c harness audit is explicitly the first todo after the probe success.
- plans/active/cli-noise-pix0/plan.md:249-261 promotes L3a to [D] and keeps L3c/L3d open until scaled.hkl is threaded through harness + CLI.
- specs/spec-a-cli.md:60-120 defines the -hkl / Fdump precedence and -pix0_vector semantics that must remain correct when we change ingestion.
- docs/development/c_to_pytorch_config_map.md:1-40 reiterates that HKL tensors must stay on the caller device/dtype; the harness should respect that contract.
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1 proves F_cell=190.270004 already lives in scaled.hkl, so the harness must surface it without fallback hacks.
- reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1 pinpoints "'`I_before_scaling` (F_cell=0) as the first divergence, so today’s evidence should replace that zero.
How-To Map:
- Ensure env vars are in place: export KMP_DUPLICATE_LIB_OK=TRUE and confirm NB_C_BIN still points at golden_suite_generator/nanoBragg for later comparisons.
- Snapshot existing artifacts so the rerun diff is clean: cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_pre_L3c.log; cp reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot_pre_L3c.json; cp reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_fixed.txt reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_pre_L3c.txt (if the file exists).
- Run the harness command from repo root (see Do Now). Expect the run to emit TRACE_PY lines, config snapshot, env snapshot, and the final pixel intensity summary.
- Immediately promote the refreshed outputs to timestamped names so earlier evidence is preserved:
  - cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log
  - cp reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot_20251117.json
  - cp reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env_20251117.json
- Dump the HKL metadata actually attached by the harness so we can compare ranges against the earlier probe: PYTHONPATH=src python - <<'"'PY' > reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt
from nanobrag_torch.io.hkl import read_hkl_file
F, meta = read_hkl_file('scaled.hkl', default_F=0.0)
for key in sorted(meta):
    print(f\"{key}: {meta[key]}\")
PY
- Inspect the new trace to ensure F_cell now matches: grep 'TRACE_PY: F_cell' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log
- Update reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md with two short bullet sections: (1) probe summary (grid ranges, F_cell), (2) harness confirmation (metadata snapshot path, F_cell match, remaining deltas if any).
- Re-run the mapped selector to confirm tests still collect: KMP_DUPLICATE_LIB_OK=TRUE pytest --collect-only -q tests/test_cli_scaling.py > reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log
- Record a short diff note under docs/fix_plan.md Attempt history describing the harness rerun, file names, and whether F_cell still needs work.
- Stage but do not commit until supervisor loop ends; keep notes about any harness edits you make so we can document them explicitly.
Pitfalls To Avoid:
- Do not touch simulator scaling code or CLI parser yet; L3c is about evidence and harness parity only.
- No new ad-hoc scripts in repo root—keep tooling under reports/… per SOP.
- Avoid "'`.item()` on tensors destined for gradients; if you need scalar logging, use `.detach().cpu().item()` in test-only code paths.
- Keep harness device/dtype neutral; never hard-code CPU tensors when running on CUDA.
- Do not delete prior evidence logs; copy-before-overwrite is mandatory for reproducibility.
- Stay on the supervisor branch; no rebases or merges mid-loop.
- Resist rerunning the full parity command until plan Phase L3 closes; nb-compare waits for L4.
- Maintain Protected Assets (`docs/index.md` list) untouched.
- Keep Mode Parity focus—avoid perf tweaks or vectorization work in this loop.
- Capture all new files with descriptive timestamps to simplify archive later.
Pointers:
- docs/fix_plan.md:448
- plans/active/cli-noise-pix0/plan.md:249
- specs/spec-a-cli.md:60
- docs/development/c_to_pytorch_config_map.md:1
- docs/development/testing_strategy.md:1
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:1
- reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log:1
- reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md:1
Next Up: 1. Audit src/nanobrag_torch/__main__.py HKL loading to mirror the harness attachment pattern; 2. Add a structure-factor + intensity regression in tests/test_cli_scaling.py once HKL ingestion is fixed.
EOF
' succeeded in 920ms:

[2025-10-07T07:32:36] tokens used: 93,613
[2025-10-07T07:32:36] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:32:45] thinking

**Preparing final update for galph_memory.md**

I’m getting ready to update `galph_memory.md` with a summary of the plan and key modifications before concluding this step. I’ll check git status and review changed files to ensure the updates reflect recent work on the plan, fix_plan, and input. I also need to clearly mention the selected focus issue and action type in the final message, plus provide a detailed narrative of the analysis phase. Once all is gathered, I’ll append these insights to `galph_memory.md` before staging and committing everything.
[2025-10-07T07:32:49] exec bash -lc "cat <<'EOF' >> galph_memory.md
## 2025-11-17 (galph loop — L3 harness alignment)
- "'`git pull --rebase` clean; reviewed docs/index.md, specs/spec-a.md shards, arch.md, config map, debugging SOP, testing strategy, fix_plan, and active plans before selecting focus.
- Focused on CLI-FLAGS-003 after Attempt #77; updated `plans/active/cli-noise-pix0/plan.md` (added 2025-11-17 gap snapshot, marked L3a [D], clarified evidence expectations) and refreshed `docs/fix_plan.md` next-actions list to center L3c harness/CLI audits and L3e validation script.
- Authored new `input.md` (Parity mode) directing Ralph to rerun the scaling trace harness with scaled.hkl attached, capture HKL metadata in `harness_hkl_state_20251117.txt`, promote new trace/env snapshots, and update `analysis.md` before touching simulator code.
- Key follow-up for Ralph: execute the harness audit command, preserve timestamped artifacts, summarise findings in analysis.md, and log outcomes in fix_plan prior to proceeding to the CLI ingestion audit (L3c) next loop.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:32:50] bash -lc "cat <<'EOF' >> galph_memory.md
## 2025-11-17 (galph loop — L3 harness alignment)
- "'`git pull --rebase` clean; reviewed docs/index.md, specs/spec-a.md shards, arch.md, config map, debugging SOP, testing strategy, fix_plan, and active plans before selecting focus.
- Focused on CLI-FLAGS-003 after Attempt #77; updated `plans/active/cli-noise-pix0/plan.md` (added 2025-11-17 gap snapshot, marked L3a [D], clarified evidence expectations) and refreshed `docs/fix_plan.md` next-actions list to center L3c harness/CLI audits and L3e validation script.
- Authored new `input.md` (Parity mode) directing Ralph to rerun the scaling trace harness with scaled.hkl attached, capture HKL metadata in `harness_hkl_state_20251117.txt`, promote new trace/env snapshots, and update `analysis.md` before touching simulator code.
- Key follow-up for Ralph: execute the harness audit command, preserve timestamped artifacts, summarise findings in analysis.md, and log outcomes in fix_plan prior to proceeding to the CLI ingestion audit (L3c) next loop.
EOF
' succeeded in 882ms:

[2025-10-07T07:32:50] tokens used: 96,173
[2025-10-07T07:32:50] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:32:54] thinking

**Verifying modifications with git status**
[2025-10-07T07:32:54] exec bash -lc 'git status --short --branch' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:32:55] bash -lc 'git status --short --branch' succeeded in 738ms:
## feature/spec-based-2...origin/feature/spec-based-2
 M docs/fix_plan.md
 M galph_memory.md
 M input.md
 M plans/active/cli-noise-pix0/plan.md
[2025-10-07T07:32:55] tokens used: 96,789
[2025-10-07T07:32:55] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:32:58] thinking

**Reviewing code diffs**
[2025-10-07T07:32:58] exec bash -lc 'git diff plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:32:59] bash -lc 'git diff plans/active/cli-noise-pix0/plan.md' succeeded in 716ms:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe..cca3247 100644
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@ Exit Criteria: PyTorch source changes with inline C-code docstring references ar
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
[2025-10-07T07:32:59] tokens used: 97,039
[2025-10-07T07:32:59] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:02] thinking

**Applying fix_plan diff**
[2025-10-07T07:33:03] exec bash -lc 'git diff docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:33:04] bash -lc 'git diff docs/fix_plan.md' succeeded in 700ms:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a..d1435cc 100644
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
[2025-10-07T07:33:04] tokens used: 98,592
[2025-10-07T07:33:04] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:06] thinking

**Reviewing galph_memory.md changes**
[2025-10-07T07:33:07] exec bash -lc 'git diff galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:33:08] bash -lc 'git diff galph_memory.md' succeeded in 693ms:
diff --git a/galph_memory.md b/galph_memory.md
index b788352..faa5349 100644
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1504,3 +1504,8 @@ Follow-ups for Ralph:
 - Noted inconsistency: `structure_factor/probe.log` shows scaled.hkl spans [-24,24]×[-28,28]×[-31,30] with F_cell=190.27, but `scaling_audit/config_snapshot.json` still reports h/k/l ranges (1,12,3) and TRACE_PY keeps F_cell=0.
 - Updated docs/fix_plan.md: CLI-FLAGS-003 Next Actions now start with a fresh Phase L3a probe rerun plus harness/CLI audits; VECTOR-TRICUBIC-001 Next Actions point to Phase B design tasks (B1–B3).
 - No new evidence generated; left instructions for Ralph to recapture HKL coverage on the current machine, reconcile harness metadata, then proceed with Phase L3 scaling fixes once F_cell matches C.
+## 2025-11-17 (galph loop — L3 harness alignment)
+- `git pull --rebase` clean; reviewed docs/index.md, specs/spec-a.md shards, arch.md, config map, debugging SOP, testing strategy, fix_plan, and active plans before selecting focus.
+- Focused on CLI-FLAGS-003 after Attempt #77; updated `plans/active/cli-noise-pix0/plan.md` (added 2025-11-17 gap snapshot, marked L3a [D], clarified evidence expectations) and refreshed `docs/fix_plan.md` next-actions list to center L3c harness/CLI audits and L3e validation script.
+- Authored new `input.md` (Parity mode) directing Ralph to rerun the scaling trace harness with scaled.hkl attached, capture HKL metadata in `harness_hkl_state_20251117.txt`, promote new trace/env snapshots, and update `analysis.md` before touching simulator code.
+- Key follow-up for Ralph: execute the harness audit command, preserve timestamped artifacts, summarise findings in analysis.md, and log outcomes in fix_plan prior to proceeding to the CLI ingestion audit (L3c) next loop.
[2025-10-07T07:33:08] tokens used: 99,930
[2025-10-07T07:33:08] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:11] thinking

**Verifying changes in input.md**
[2025-10-07T07:33:12] exec bash -lc 'git diff input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:33:12] bash -lc 'git diff input.md' succeeded in 702ms:
diff --git a/input.md b/input.md
index 455bd32..7bf369c 100644
--- a/input.md
+++ b/input.md
@@ -1,35 +1,55 @@
-Summary: Reconfirm HKL coverage for the supervisor command so the scaling trace stops zeroing F_cell.
+Summary: Attach scaled.hkl into the scaling trace harness so TRACE_PY reports F_cell=190.27 for the supervisor pixel and the harness evidence matches the new probe run.
 Mode: Parity
 Focus: CLI-FLAGS-003 – Handle -nonoise and -pix0_vector_mm
 Branch: feature/spec-based-2
 Mapped tests: pytest --collect-only -q tests/test_cli_scaling.py
-Artifacts: reports/2025-10-cli-flags/phase_l/structure_factor/probe_20251117.log, reports/2025-10-cli-flags/phase_l/structure_factor/analysis_20251117.md, reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt
-Do Now: CLI-FLAGS-003 Phase L3a — `KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/structure_factor/probe.py --pixel 685 1039 --hkl scaled.hkl --fdump reports/2025-10-cli-flags/phase_l/hkl_parity/Fdump_scaled_20251006175946.bin --dtype float32 --device cpu`
-If Blocked: Copy the probe stdout/stderr to `reports/2025-10-cli-flags/phase_l/structure_factor/probe_blocker_20251117.log`, record the failure context in docs/fix_plan.md Attempt history for CLI-FLAGS-003, and halt before editing any source files.
+Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt, reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log, reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md
+Do Now: CLI-FLAGS-003 Phase L3c harness audit — KMP_DUPLICATE_LIB_OK=TRUE PYTHONPATH=src python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --device cpu --dtype float32 --out trace_py_scaling.log
+If Blocked: Capture stdout/stderr to reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness_failure_20251117.log, note the exception (command + stack) in docs/fix_plan.md Attempt history for CLI-FLAGS-003, and stop before editing simulator or CLI code.
 Priorities & Rationale:
-- docs/fix_plan.md:448 pinpoints F_cell=0 as the gating blocker for CLI-FLAGS-003 and now requires a fresh Phase L3a rerun.
-- plans/active/cli-noise-pix0/plan.md:248 spells out the Phase L3a/L3b evidence steps that must precede any simulator edits.
[2025-10-07T07:33:12] tokens used: 100,952
[2025-10-07T07:33:12] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:16] thinking

**Preparing to stage changes**
[2025-10-07T07:33:19] exec bash -lc 'git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md input.md galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:33:20] bash -lc 'git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md input.md galph_memory.md' succeeded in 743ms:

[2025-10-07T07:33:20] tokens used: 103,993
[2025-10-07T07:33:20] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:25] thinking

**Preparing commit message**
[2025-10-07T07:33:25] exec bash -lc 'git commit -m "SUPERVISOR: CLI-FLAGS L3 harness prep - tests: not run"' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:33:26] bash -lc 'git commit -m "SUPERVISOR: CLI-FLAGS L3 harness prep - tests: not run"' succeeded in 693ms:
[feature/spec-based-2 ca8945d] SUPERVISOR: CLI-FLAGS L3 harness prep - tests: not run
 4 files changed, 58 insertions(+), 32 deletions(-)
[2025-10-07T07:33:26] tokens used: 104,221
[2025-10-07T07:33:26] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:28] thinking

**Preparing to push changes**
[2025-10-07T07:33:29] exec bash -lc 'git push' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:33:31] bash -lc 'git push' succeeded in 2.03s:
To github.com:hoidn/nanoBragg.git
   4e8fd57..ca8945d  feature/spec-based-2 -> feature/spec-based-2
[2025-10-07T07:33:31] tokens used: 104,485
[2025-10-07T07:33:31] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:43] thinking

**Preparing comprehensive final summary**

I’m putting together a detailed final message that covers the focus issue CLI-FLAGS-003, planning action type with Docs mode, and outcomes from doc reviews and commit analyses. I’ll include references to all consulted docs with line pointers, mention the coin flip result, commit inspection findings, updated plans, and next steps for Ralph, all structured clearly for the user’s clarity.
[2025-10-07T07:33:44] exec bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '12,30p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T07:33:45] bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '12,30p'" succeeded in 853ms:
    12	- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
    13	- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
    14	- Gap snapshot update (2025-11-11): Discovered the trace harness omitted `-beam_vector`, `-flux`, and `-beamsize`, forcing oversample=4 and a +Z beam; Py traces zeroed `F_cell` and inflated `steps`. Marked unresolved pending harness fix.
    15	- Gap snapshot refresh (2025-11-12): Attempt #66 (commit 1d2e6ac) completed Phase L2b0 — the trace harness now feeds the full supervisor flag set, and the refreshed `trace_py_scaling.log` confirms `steps=10`, `fluence≈1.0e24`, and `r_e^2` parity with C. TRACE_PY still reports `I_before_scaling NOT_EXTRACTED`, `polar=0`, and `I_pixel_final=0`, so Phase L2b instrumentation must surface the real polarization/absorption tensors before we can quantify the K3e k_frac drift.
    16	- Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
    17	- Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
    18	- Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
    19	- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
    20	- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
    21	- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
    22	- Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
    23	- Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
    24	- Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
    25	- Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
    26	- Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
    27	- Gap confirmation (2025-11-08 trace diff): Phase K3f base-lattice traces show PyTorch keeps the placeholder unit cell volume (`V≈1 Å^3`) when MOSFLM vectors are supplied, so `Crystal.compute_cell_tensors` fails to rescale the real vectors. C multiplies `b*×c*` by `V_cell≈2.4682×10^4 Å^3` before converting to meters, yielding |a|≈26.75 Å; PyTorch leaves |a|≈5.8×10^3 Å, which in turn inflates h/k/l by ≈6 units. Fix requires mirroring the MOSFLM derivation pipeline in `Crystal.compute_cell_tensors` (see `specs/spec-a-core.md` Geometry + `golden_suite_generator/nanoBragg.c` 3135-3210).
    28	- Gap snapshot update (2025-11-08 evening): Commit 46ba36b implements the MOSFLM rescale path and new regression test `TestMOSFLMCellVectors::test_mosflm_cell_vectors` passes (see `reports/2025-10-cli-flags/phase_k/base_lattice/post_fix/cell_tensors_py.txt`). Base-lattice summary now records the fix; remaining blocker is Phase K3g3 to rerun scaling evidence and refresh nb-compare/pytest artifacts.
    29	- Evidence status: Phase E artifacts (`reports/2025-10-cli-flags/phase_e/`) hold C/PyTorch traces, diffs, and beam-vector checks. Phase H3b1 also stashes WITH/without override traces under `reports/2025-10-cli-flags/phase_h/implementation/` for reference.
    30	- Documentation anchors for this focus:
[2025-10-07T07:33:45] tokens used: 105,397
[2025-10-07T07:33:45] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |


[2025-10-07T07:33:56] thinking

**Organizing final response structure**
[2025-10-07T07:33:56] codex

**Outcome**
- Recorded the new 2025‑11‑17 gap snapshot and marked Phase L3a complete so the plan now points Ralph at the harness/CLI wiring work (`plans/active/cli-noise-pix0/plan.md:19`, `plans/active/cli-noise-pix0/plan.md:256`).
- Narrowed CLI-FLAGS-003 next actions to L3c/L3e/L4 sequencing, adding the harness audit, CLI ingestion check, scaling validation script, and parity rerun steps (`docs/fix_plan.md:458`).
- Published a refreshed supervisor handoff (Parity mode) that walks Ralph through the harness rerun, metadata snapshot, probe summary update, and collect-only validation (`input.md:1`).
- Logged the loop decisions and follow-up expectations in `galph_memory.md:1504`.

**Analysis**
- Pre-work review covered the required docs: `docs/index.md`, `arch.md`, `specs/spec-a.md` (plus `specs/spec-a-cli.md`), `docs/development/c_to_pytorch_config_map.md`, `docs/debugging/debugging.md`, and `docs/development/testing_strategy.md`.
- Coin flip landed **heads**; the audit of Ralph’s recent work (commits 1a5c3e4 and 0426b1e) showed solid evidence-only progress and reusable benchmark tooling with no regressions detected.

**Next Steps for Ralph**
- Follow the new `input.md` directive: rerun `trace_harness.py` with the supervisor config, promote the outputs to timestamped artifacts, dump HKL metadata, refresh `analysis.md`, capture the collect-only log, and document results in the CLI-FLAGS attempt log before advancing to the CLI ingestion audit.
[2025-10-07T07:34:01] tokens used: 108,276
[2025-10-07T07:34:01] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index 1bf838a3836807a4dfbbcffbbbed5cd5339796aa..d1435cc6cfc90f9514dd7f90637e5e6a71952a4f
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -456,11 +456,11 @@
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase L2c comparison shows all scaling factors (ω, polarization, r_e², fluence, steps) match C within 0.2%, but `I_before_scaling` diverges because PyTorch reports `F_cell=0` at hkl≈(−7,−1,−14) while C's trace records `F_cell=190.27`. **Phase L3b (Attempt #76) proved the data exists (scaled.hkl contains F=190.27 for this reflection); root cause is configuration/loading, NOT missing coverage.**
 - Next Actions (2025-11-17 parity refresh):
-  1. Phase L3a coverage re-verification — Re-run `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` on the current workstation against repo-root `scaled.hkl` and archived Fdump snapshots; capture fresh `probe.log`/`analysis.md` showing the h/k/l ranges so we can reconcile the probe evidence with `config_snapshot.json` (which still reports h/k/l=(1,12,3)).
-  2. Phase L3c harness audit — Review `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to identify why HKL data isn't loaded despite scaled.hkl containing 168k reflections. Compare against probe.py successful pattern (lines 206-236). Fix attachment to mirror: `F_grid, metadata = read_hkl_file(...); crystal.hkl_data = F_grid; crystal.hkl_metadata = metadata`.
-  3. Phase L3c CLI verification — Audit `src/nanobrag_torch/__main__.py` to ensure `-hkl` flag loading follows same pattern. NO simulator.py changes needed (scaling math is correct per L2c; lookup logic works per L3b probe).
-  4. Phase L3d regression — Extend `tests/test_cli_scaling.py` with structure-factor lookup assertion for hkl=(-7,-1,-14) using scaled.hkl, validating retrieval of F=190.27 and end-to-end intensity matching C reference (≤1e-6 relative tolerance, CPU/CUDA parametrised).
-  5. Phase L4 parity rerun — Execute supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
+  1. Phase L3c harness audit — Mirror the probe attachment pattern inside `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` (assign both `crystal.hkl_data` and `crystal.hkl_metadata`) and capture a fresh `harness_hkl_state_*.txt`; while doing so, summarize the new probe findings in `analysis.md` so the evidence bundle is self-contained.
+  2. Phase L3c CLI verification — Inspect `src/nanobrag_torch/__main__.py` HKL-loading path to ensure it applies the same tensor/metadata attachment and device handling as the harness; log results under `reports/.../structure_factor/cli_hkl_audit.md` before touching simulator internals.
+  3. Phase L3d regression — Extend `tests/test_cli_scaling.py` with a structure-factor lookup assertion for hkl=(-7,-1,-14) plus a supervisor-scaling expectation check (CPU/CUDA parametrised once the harness proves out).
+  4. Phase L3e scaling validation — Enhance or author a `scripts/validation/compare_scaling_chain.py` helper to diff TRACE_C/TRACE_PY after the HKL fix, storing metrics under `reports/2025-10-cli-flags/phase_l/scaling_validation/` to document closure before parity.
+  5. Phase L4 parity rerun — Execute the supervisor command end-to-end, capture nb-compare metrics, confirm correlation ≥0.9995 + sum_ratio 0.99-1.01, archive results, and close CLI-FLAGS-003.
 - Attempts History:
   * [2025-11-13] Attempt #71 (ralph loop) — Result: **SUCCESS** (Phase L2b validation + Phase L2c complete). **Harness already functional from Attempt #69; executed comparison script to identify first divergence.**
     Metrics: 40 TRACE_PY lines captured; test suite passes 4/4 variants (cpu/cuda × float32/float64). Comparison identifies `I_before_scaling` as first divergent factor (C=943654.809, PyTorch=0, -100% delta).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index b1945fe8253b742f4947ebf5420e2494bfe65031..cca3247962e1f8add39b3635f3af4623d1c5973b
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -16,6 +16,7 @@
 - Gap snapshot update (2025-11-13): Latest harness rerun aborted because `trace_harness.py` still expected the legacy `read_hkl_file` return tuple (see `trace_py_fullrun.log`). **Resolved in Attempt #74, which updated the harness to the `(F_grid, metadata)` signature.**
 - Gap snapshot refresh (2025-11-14): Attempt #73 showed `Crystal.hkl_data` unset (leading to `F_cell=0`), keeping L2b open; **Attempt #74 fixed this by wiring the HKL tensor/metadata before Simulator construction.**
 - Gap snapshot refresh (2025-11-15): Attempts #69/#71/#74 finished Phase L2 — TRACE_PY now reports real scaling factors, `scaling_audit_summary.md` flags `I_before_scaling` as the lone divergence, and C shows `F_cell=190.27` for hkl=(-7,-1,-14) while PyTorch remains at 0. Next phase focuses on understanding how C derives that amplitude (likely via generated `Fdump.bin`) before touching normalization code. 
+- Gap snapshot refresh (2025-11-17): Attempt #77 reran Phase L3a probe on the current workstation, logged `probe_20251117.log` (F_cell=190.270004 for hkl=(-7,-1,-14)), and confirmed scaled.hkl already covers the supervisor reflection. Remaining blockers shift to wiring that data through the harness/CLI path (Phase L3c onward).
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -252,7 +253,7 @@
 
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
-| L3a | Verify structure-factor coverage for supervisor pixel | [ ] | Use the refreshed harness to reproduce the rounded Miller index from C (`trace_c_scaling.log` lines 143-164 → hkl=(-7,-1,-14)). Author a small probe (e.g., `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py`) that loads `scaled.hkl` via `read_hkl_file` **and** the latest `Fdump.bin` via `read_fdump`, then calls `Crystal.get_structure_factor` for that hkl. Record both outputs and compare against the C reference value `F_cell=190.27`. |
+| L3a | Verify structure-factor coverage for supervisor pixel | [D] | ✅ 2025-11-17 Attempt #77: `reports/2025-10-cli-flags/phase_l/structure_factor/probe.py` recorded `probe_20251117.log` showing scaled.hkl returns `F_cell=190.270004` for hkl=(-7,-1,-14). Keep the log + timestamped copies alongside a brief summary in `analysis.md`, then proceed to L3c to thread the same attachment pattern through the harness/CLI path. |
 | L3b | Reconcile HKL ingestion with C | [ ] | If the probe shows a mismatch (PyTorch 0 vs C 190.27), inspect the C reader (`nanoBragg.c:2333-2490`) to determine whether additional reflections are synthesized (e.g., via symmetry, padding, or sinc tables) despite the minimal HKL file. Document hypotheses and decide whether the harness must load the generated `Fdump.bin`, expand the HKL grid, or adjust interpolation flags before touching production code. Store notes in `reports/2025-10-cli-flags/phase_l/structure_factor/analysis.md`. |
 | L3c | Implement scaling fix in simulator | [ ] | After L3a/L3b confirm the correct structure-factor source, modify `src/nanobrag_torch/simulator.py` (≈lines 930-1085) so the scaling chain mirrors the C ordering. Include mandatory C-code quotes per Rule #11 and keep the implementation vectorized/device-neutral. |
 | L3d | Add targeted regression tests | [ ] | Extend `tests/test_cli_scaling.py` with (1) a structure-factor lookup assertion for hkl=(-7,-1,-14) using the agreed data source, and (2) a supervisor-command regression that checks I_before_scaling, ω, polarization, and final intensity against recorded C values within ≤1e-6 relative tolerance. Parametrise over CPU/CUDA when available. |

Reading prompt from stdin...
To github.com:hoidn/nanoBragg.git
   ca8945d..b1c3ed7  HEAD -> feature/spec-based-2
Waiting for Ralph to complete i=77...
Already up to date.
[sync] state: actor=ralph status=waiting-ralph iter=77
Updating b1c3ed7..fdbdff2
Fast-forward
 sync/state.json | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
From github.com:hoidn/nanoBragg
   b1c3ed7..fdbdff2  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=ralph status=running-ralph iter=77
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating fdbdff2..ed91218
Fast-forward
 docs/fix_plan.md                                   | 16 +++++++
 .../scaling_audit/collect_cli_scaling_20251117.log |  4 ++
 .../phase_l/scaling_audit/config_snapshot.json     | 12 ++---
 .../scaling_audit/config_snapshot_20251117.json    | 52 ++++++++++++++++++++++
 .../scaling_audit/harness_hkl_state_20251117.txt   |  9 ++++
 .../phase_l/scaling_audit/trace_py_env.json        |  4 +-
 .../scaling_audit/trace_py_env_20251117.json       | 10 +++++
 .../phase_l/scaling_audit/trace_py_scaling.log     |  8 ++--
 .../scaling_audit/trace_py_scaling_20251117.log    | 40 +++++++++++++++++
 .../phase_l/structure_factor/analysis_20251117.md  | 38 +++++++++++++++-
 10 files changed, 180 insertions(+), 13 deletions(-)
 create mode 100644 reports/2025-10-cli-flags/phase_l/scaling_audit/collect_cli_scaling_20251117.log
 create mode 100644 reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot_20251117.json
 create mode 100644 reports/2025-10-cli-flags/phase_l/scaling_audit/harness_hkl_state_20251117.txt
 create mode 100644 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env_20251117.json
 create mode 100644 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling_20251117.log
From github.com:hoidn/nanoBragg
   fdbdff2..ed91218  feature/spec-based-2 -> origin/feature/spec-based-2
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating ed91218..3d986fc
Fast-forward
 sync/state.json | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)
From github.com:hoidn/nanoBragg
   ed91218..3d986fc  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=galph status=complete iter=78
Ralph completed iteration 77; proceeding to 78
