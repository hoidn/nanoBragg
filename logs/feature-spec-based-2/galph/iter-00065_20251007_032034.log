Already up to date.
Waiting for expected_actor=galph...
Already up to date.
[sync] state: actor=galph status=complete iter=65
To github.com:hoidn/nanoBragg.git
   54a94d2..c3c6eef  HEAD -> feature/spec-based-2
$ codex exec -m gpt-5-codex -c model_reasoning_effort=high --dangerously-bypass-approvals-and-sandbox
[2025-10-07T03:20:37] OpenAI Codex v0.38.0 (research preview)
--------
workdir: /Users/ollie/Documents/nanoBragg3
model: gpt-5-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
--------
[2025-10-07T03:20:37] User instructions:
<role>
planning, review and analysis. do not make code changes.
</role>
<current long-term goals>
in order of decreasing priority:
<1>
run a successful parallel test of pytorch nanobragg against nanobragg c with this command:

nanoBragg  -mat A.mat -floatfile img.bin -hkl scaled.hkl  -nonoise  -nointerpolate -oversample 1  -exposure 1  -flux 1e18 -beamsize 1.0  -spindle_axis -1 0 0 -Xbeam 217.742295 -Ybeam 213.907080  -distance 231.274660 -lambda 0.976800 -pixel 0.172 -detpixels_x 2463 -detpixels_y 2527 -odet_vector -0.000088 0.004914 -0.999988 -sdet_vector -0.005998 -0.999970 -0.004913 -fdet_vector 0.999982 -0.005998 -0.000118 -pix0_vector_mm -216.336293 215.205512 -230.200866  -beam_vector 0.00051387949 0.0 -0.99999986  -Na 36  -Nb 47 -Nc 29 -osc 0.1 -phi 0 -phisteps 10 -detector_rotx 0 -detector_roty 0 -detector_rotz 0 -twotheta 0

this will require first adding support for the following cli params to pytorch nanobragg:
-nonoise
-pix0_vector_mm

</1>
<2>
review and firm up if needed plans/active/vectorization.md. then implement it (or rather, delegate its implementation to ralph), starting with the tricubic interpolation vectorization 
</2>
</current long-term goals>
</past long term goals>
(just for archival / reference purposes):
- error-correcting the engineer agent 
- profiling pytorch nanobragg to find performance bottlenecks. analyzing to understand the CRITICAL slowness of the pytorch implementation compared to reference C, which persists after our recent improvement attempts
- finding performance issues in the PyTorch reimplementation of nanobragg (originally a C code) and speeding it up. It should be efficiently vectorized and faster than C nanobragg, but currently it's slower 
- ensuring that the pytorch implementation uses fp32 (not fp64) by default 
- understanding why pytorch is slower than C in the warm condition at 4096x4096 resolution and fixing the underlying performance issue, if one is found. THIS IS A CRITICAL PERFORMANCE ISSUE.
- once all the above are taken care of: building a user-facing showcase of autodiff-based parameter refinement, with plots / visuals

</past long-term goals>
<task>
You are galph, a planner / supervisor agent. you are overseeing the work of an agent (ralph) that is running prompts/main.md in a loop, using docs/fix_plan.md as its instruction set and long term memory. 

You will get invoked repeatedly. Use galph_memory.md to communicate with your future self. You'll plans under plans/, when needed, to help steer multi-turn efforts by the coder agent (ralph). Those plans will be cross referenced from docs/fix_plan.md so that ralph can find / read them. 

At the start of every invocation:
- Run `timeout 30 git pull --rebase` with a hard 30-second timeout  to sync with origin before reviewing context. If the command times out, immediately abort any partial rebase (`git rebase --abort`) and fall back to a normal merge via `git pull --no-rebase`. Whatever path you take, resolve resulting conflicts (docs/fix_plan.md is a frequent hotspot) and document key decisions in galph_memory.md. If the pull reports conflicts:
  * Run `git status --short` to list conflicted files.
  * Fix each file (remove conflict markers, keep the intended content) and stage it with `git add`.
  * When the index is clean, resume with `timeout 30 git rebase --continue --no-edit`. IMPORTANT: NEVER RUN THIS COMMAND WITHOUT TIMEOUT
- After resolving any conflicts, read the latest galph_memory.md entry (and any linked plan files) so you do not lose past context.
- Prune or summarize older entries when they become stale or redundant.

Before concluding each invocation:
- Append a concise update to galph_memory.md capturing key findings, decisions, and open questions (reference file paths or plan names).
- Note any follow-up actions you expect Ralph to take.
- If no substantial update is needed, explicitly log that fact so future runs know context was reviewed.
</task>

<instructions>
<0>
READ the following files:
- Index of project documentation: `./docs/index.md`
- $SPECS: `./specs/spec-a.md`
- $ARCH: `./arch.md` (ADR-backed implementation architecture; reconcile design with spec, surface conflicts)
- docs/development/c_to_pytorch_config_map.md — C↔Py config parity and implicit rules
-- docs/debugging/debugging.md — Parallel trace-driven debugging SOP
- $PLAN: `./fix_plan.md` (living, prioritized to‑do; keep it up to date)
- $TESTS: `./docs/development/testing_strategy.md` (testing philosophy, tiers, seeds/tolerances, commands)
- Set `AUTHORITATIVE_CMDS_DOC=./docs/development/testing_strategy.md` (or project‑specific path) and consult it for authoritative reference commands and test discovery.
<0>
<1>
do a deep analysis of the codebase in light of the <current long term goals>. What are some current issues / gaps and possible approaches to resolving them? Review docs/fix_plan.md and plans/active/, as previous iterations of you may have already done some legwork.
 


Debugging Items — Hypothesis + Triage (initial pass):
- Only when the selected item is a debugging or parity analysis task, formulate 1–3 plausible hypotheses for the observed gap/issue.
- Triage each with checks appropriate to scope using existing artifacts or quick, documented reproductions; this triage may constitute the primary work for the iteration. Record outcomes succinctly in the evidence.
- For the top hypothesis, state confidence and the single confirming step to run next. Deliver 1–3 ranked hypotheses with supporting evidence, artifact paths, and the next confirming step.
</1>
<2>
flip a coin using python. if it comes up <heads>:
review ralph's work over the last N (~10 but can be more or less - you decide) iterations. Check the commit history. Has the work been productive? Have there been regressions? Do we need to provide any feedback / course-correction?
</heads>
if it comes up <tails>: proceed to step <3>
</2>
<3>
Given your findings in <1> and <2>, think about whether there's any need for a multi-turn planning effort -- i.e. ralph can't see the forest for the trees and may struggle with major refactorings and multi-turn implementation efforts unless they are coordinated by you. Is there a need for such planning *right now*? If such a plan is already spoken for (plans/active/ or wherever else past galph might have saved to), does that plan require updates or is it up to date / high quality and simply pending? IFF you determine the need for a new plan or modification of an existing one:
<yes case>
- we will be calling the plan topic the <focus issue> of this turn.
- based on which long term <goal> and sub-goal is that effort / plan? 
- Which existing docs/fix_plan.md items does it (i.e. the <focus issue>) relate to? 
- Documentation review for <focus issue>: Using `docs/index.md` as the map, identify and read the specific documents relevant to the chosen <focus issue> (e.g., component contracts, architecture ADRs, parity/testing strategy). List the file paths you will rely on (with a one‑line rationale each) before drafting or updating the plan.
- think deeply. draft / redraft the plan and save it to a .md under plans/active/. Structure the write-up as a phased implementation document (see `plans/archive/general-detector-geometry/implementation.md` for tone/shape): begin with context + phase overviews, then outline each phase’s intent, prerequisites, and exit criteria. When a phase benefits from explicit tracking, embed a checklist table using the `ID | Task Description | State | How/Why & Guidance` format (with `[ ]`, `[P]`, `[D]` markers) inside that phase section.
  • Include reproduction commands, owners (if known), and decision rules in the guidance column.
  • Favor narrative flow first; layer checklists only where they clarify verification steps or deliverables.
  • Mini-template to crib when drafting:
    ```md
    ## Context
    - Initiative: <initiative>
    - Phase Goal: <outcome>
    - Dependencies: <docs/tests>

    ### Phase A — <short title>
    Goal: <what this phase proves or delivers>
    Prereqs: <artifacts or measurements required before starting>
    Exit Criteria: <verifiable completion signal>

    | ID | Task Description | State | How/Why & Guidance |
    | --- | --- | --- | --- |
    | A1 | <Key diagnostic or implementation step> | [ ] | Run `<command>`; capture results under `reports/<date>/...`. |
    | A2 | <Follow-up validation> | [ ] | Compare against `<artifact>`; stop if deviation > threshold. |
    ```
- When refreshing an existing plan, retrofit it to this phased format before adding or editing tasks.
- review docs/fix_plan.md. edit if needed. cross reference the new plans .md so that ralph can find it.
</yes case>
</no case>
- Since you decided there's no need for planning, you will instead focus on review / housekeeping. 
- Once the above is done (or deemed unneeded): review and evaluate ralph's work. Scrutinize the commit history. Look at the diffs. 
- Are the docs/fix_plan.md contents and priorities sane? things to consider:
  - if docs/fix_plan.md is longer than 1000 lines it should be housecleaned. If it's disorganized and / or internally inconsistent, consider how this could be addressed. 
- after considering the above, you have enough information to choose a <focus issue> for this turn. do so. Consider the nature of the <focus issue>:
    - Documentation review for <focus issue>: From `docs/index.md`, enumerate and read the documents that are most relevant to the chosen <focus issue>`; note the key file paths you will rely on for this turn.
    - Do we need a new docs/fix_plan item to put ralph back on course, fix one of his mistakes, or instruct him to do something that he overlooked? If so, draft it and add it to docs/fix_plans
    - does the <focus issue> involve identified issues in docs/fix_plan.md? If so, fix them. If you decide to shorten docs/fix_plan.md, the least relevant portions should be moved to archive/fix_plan_archive.md (with summary + archive cross reference if appropriate)
</no case>
</3>
<3.5>
Render and write ./input.md (supervisor→engineer steering memo). Overwrite the entire file every invocation. Keep length ~100–200 lines. Include these sections in order:
Header:
- Summary: <one‑sentence goal for this loop>
 - Mode: <TDD | Parity | Perf | Docs | none>
- Focus: <plan item ID/title from docs/fix_plan.md>
- Branch: <expected branch>
- Mapped tests: <validated list | none — evidence‑only>
- Artifacts: <key paths to produce under reports/>
- Do Now: 1 line naming the exact docs/fix_plan.md item (ID and title) to execute this loop, plus the exact pytest command/env to reproduce it (only when an authoritative mapping exists). If no such test exists, the Do Now MUST be to author the minimal targeted test first and then run it. If you intend to delegate the choice, write “Do Now: delegate” and provide decision guidance below.
  - Note: If operating in supervisor evidence mode (internal-only), do not run the full suite. Allowed: pytest --collect-only to validate selectors; in TDD mode only, run one targeted selector to confirm a new failing test. All other test execution is deferred to Ralph. Do not include any Phase label in input.md.
<Do Now guidelines>
Supervisor Evidence Gate (internal-only): No full pytest runs. Allowed: non-mutating probes and CLI validation tools (e.g., scripts/validation/*, nb-compare, pixel trace); pytest --collect-only to validate selectors; and in TDD mode only, a single targeted pytest run to confirm the newly authored test fails. Do not change production code. This gate applies only to galph. Ralph is exempt. Do not include any Phase label in input.md.
Verification scripts: You may run nb-compare and scripts/validation/* to collect metrics and artifacts (no code changes). Record outputs under reports/.
Mapped tests under supervisor evidence mode: Include exact selectors in input.md; validate with --collect-only; do not run them (except the TDD failure confirmation). Do not include any Phase label in input.md.
Mode flags: Mode flags are combinable and refine execution; do not include any Phase label in input.md.
TDD mode (supervisor-scoped): When operating in supervisor evidence mode, author/update a single minimal failing test that encodes the acceptance criterion. Confirm it fails via a targeted pytest selector; record the selector and expected failure text in input.md. Do not change production code, and do not include any Phase label in input.md.
Command Sourcing (tests): Only include an exact pytest command in Do Now when sourced from an authoritative mapping (e.g., docs/development/testing_strategy.md) or an existing, known test file/identifier. If no authoritative mapping exists, set the Do Now task to author the minimal targeted test first; do not guess at a pytest selector here.
</Do Now guidelines>
- If Blocked: fallback action (what to run/capture, how to record it in Attempts History).
- Priorities & Rationale: 3–6 bullets with file pointers (specs/arch/tests/plan) justifying the Do Now.
- How-To Map: exact commands (pytest, nb-compare), env vars, ROI/thresholds, and where to store artifacts; prefer using `scripts/validation/*` when available.
- Pitfalls To Avoid: 5–10 crisp do/don’t reminders (device/dtype neutrality, vectorization, Protected Assets, two‑message loop policy, no ad‑hoc scripts).
- Pointers: file paths with line anchors to the most relevant spec/arch/testing_strategy/fix_plan entries for this loop.
- Next Up (optional): 1–2 candidates Ralph may choose next loop if he finishes early (still one thing per loop).

 Rules:
 - Ralph must not edit input.md. You (galph) are the single writer.
 - If input.md’s Do Now conflicts with an existing in_progress selection in fix_plan.md, Ralph is allowed to switch; he must record the change in Attempts History.
- Do not instruct Ralph to run the full test suite more than once per turn. Require targeted tests first; run the full suite once at the end only if code changed. For prompt/docs‑only loops, require `pytest --collect-only -q` instead.
- Prefer referencing reusable scripts under `scripts/validation/` from Do Now when available.
- Commit input.md each run as part of step <4> (commit only if the content changed).
 </3.5>

Callchain Tracing (Evidence Task)
- When to use: before code edits when the pipeline/factor order relevant to the current issue is unclear; when onboarding a new surface; or upon parity failures where the locus is unknown.
- Directive: run `prompts/callchain.md` with a question‑driven invocation and produce standardized artifacts under `reports/<initiative_id>/`.
- Example invocation variables (fill at run time):
  analysis_question: "<what are we trying to understand or fix?>"
  initiative_id: "<short‑slug>"
  scope_hints: ["CLI flags", "normalization", "scaling"]
  roi_hint: "<minimal input/ROI>"
  namespace_filter: "<project primary package>"
  time_budget_minutes: 30
- Expected outputs:
  - `reports/<initiative_id>/callchain/static.md` (entry→sink with file:line anchors)
  - `reports/<initiative_id>/callgraph/dynamic.txt` (optional, module‑filtered)
  - `reports/<initiative_id>/trace/tap_points.md` (proposed numeric taps with owners)
  - `reports/<initiative_id>/summary.md` (question‑oriented narrative + next steps)
  - `reports/<initiative_id>/env/trace_env.json`
- Guardrails: evidence‑only (no prod edits), module/device/dtype neutrality, small ROI, respect Protected Assets, stable key names in traces.
<4>
Before finishing the loop, enforce git hygiene:
- Run `git status --short` to inspect new or modified files that appeared during this invocation.
- Review diffs for each change; revert only if you created it this loop and it was accidental.
- Stage intentional updates with `git add -A` and commit via `git commit -m "SUPERVISOR: <scope> - <tests or rationale>"`, noting any tests run (use `tests: not run` when you skip them).
- After committing, run `git push` to share the updates. If the push is rejected, `timeout 30 git pull --rebase`, resolve conflicts (capture decisions—especially for docs/fix_plan.md—in galph_memory.md), then push again.
- If you deliberately leave the tree dirty, document the rationale in galph_memory.md so the next invocation knows why.
</4>
</instructions>
<notes>
- ignore 'routing violations'. these are out of scope.
- ignore AT parallel 012-related items for now
</notes>
Now carefully and exhaustively follow your <instructions>.


[2025-10-07T03:20:42] thinking

**Preparing to update repository**
[2025-10-07T03:20:42] exec bash -lc 'timeout 30 git pull --rebase' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:20:44] bash -lc 'timeout 30 git pull --rebase' succeeded in 2.16s:
Already up to date.
[2025-10-07T03:20:44] tokens used: 13,490

[2025-10-07T03:20:48] thinking

**Preparing to read latest memory entry**
[2025-10-07T03:20:49] exec bash -lc 'cat galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:20:49] bash -lc 'cat galph_memory.md' succeeded in 752ms:
## 2025-09-30 (loops 1-4 summary)
- Established parity backlog: AT-020/021/022/024 failing; created first supervisor plan for AT-021 and enforced prompts/debug.md routing.
- Diagnosed AT-024 random misset cap mismatch (π/2 vs 90 radians) and opened dedicated plan; insisted Ralph stop verification-only loops.
- Flagged torch.compile warm-up overhead (per-instance compilation, inline tensor factories) and opened AT-012 rotation parity plan (Phases A–E).
- Documented performance hotspots (`_compute_physics_for_position` guard tensors, `Crystal.compute_cell_tensors` clamps) and chronic dirty worktree artifacts blocking `git pull --rebase`.

## 2025-09-30 (loops 5-10 summary)
- Authored repo hygiene checklist (REPO-HYGIENE-002) to undo accidental `golden_suite_generator/nanoBragg.c` churn and purge stray reports.
- Created PERF-PYTORCH-004 plan to hoist constants, introduce compile caching, and remove Dynamo graph breaks; reiterated need for prompts/debug.md while parity fails.
- Repeatedly documented routing violations from Ralph and emphasized restoring Core Rule #13 metric duality after WIP commit 058986f swapped `V_actual` for formula volume.
- Noted that `_compute_physics_for_position` still recompiles each Simulator and that benchmarks in `reports/benchmarks/20250930-004916/` show ≥200× slowdown vs C for small detectors.

## 2025-09-30 (ralph debugging loop — AT-012 float32 regression)
- **Context**: AT-PARALLEL-012 test_simple_cubic_correlation failing (43/50 peaks matched, need ≥95%) after DTYPE-DEFAULT-001 (commit 8c2ceb4) switched simulator to native float32.
- **Investigation**:
  * Reproduced: 43/50 peaks matched (86%), corr≈1.0 (perfect physics) ✅
  * Parity matrix test: PASSES perfectly ✅ (live C↔Py comparison with current float32)
  * **Root cause identified**: PyTorch float32 produces 7× more unique values (4901 vs C's 669) in hot pixels, fragmenting intensity plateaus. Peak detection algorithm (scipy.ndimage.maximum_filter) is sensitive to plateau structure.
  * Parallel comparison: C float32 beam center (20×20 ROI) has 91 unique values; PyTorch has 331 (3.6× fragmentation). Max absolute diff tiny (0.0017).
  * **First Divergence**: Numerical accumulation patterns differ (likely FMA, compiler opts, vectorized accumulation order, or torch.compile kernel fusion) → perfect correlation but incompatible plateau structure.
[2025-10-07T03:20:49] tokens used: 13,711

[2025-10-07T03:20:55] thinking

**Preparing to read required docs files**
[2025-10-07T03:20:55] exec bash -lc 'cat docs/index.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:20:56] bash -lc 'cat docs/index.md' succeeded in 699ms:
# Project Documentation Index

Welcome to the central index for all `nanoBragg-PyTorch` project documentation. This guide provides a map to the key technical specifications, development processes, and historical records that define the project.

---

## 🏛️ Permanent Documentation

These are the core, living documents that guide the project.

### Core Project Guides
### PyTorch Guardrails
* **[PyTorch Runtime Checklist](./development/pytorch_runtime_checklist.md)** – quick do/don’t list for vectorization and device placement. Always review before editing simulator code.
* **[PyTorch Design](./architecture/pytorch_design.md#vectorization-strategy)** – detailed explanation of the required broadcast shapes and batched flows.
* **[Testing Strategy §1.4](./development/testing_strategy.md#14-pytorch-device--dtype-discipline)** – required CPU/GPU smoke tests and compile checks.

* **[CLAUDE.md](../CLAUDE.md)** - The primary instruction set for the AI agent.
* **[README.md](../README.md)** - The main project entry point.
* **[README_PYTORCH.md](../README_PYTORCH.md)** - Comprehensive user guide for the PyTorch implementation, including CLI usage, parallel comparison tools, and visualization.
* **[PROJECT_STATUS.md](../PROJECT_STATUS.md)** - Tracks the current active development initiative.
[2025-10-07T03:20:56] tokens used: 16,752

[2025-10-07T03:20:58] thinking

**Searching for spec-a.md**
[2025-10-07T03:20:59] exec bash -lc 'cat specs/spec-a.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:20:59] bash -lc 'cat specs/spec-a.md' succeeded in 663ms:
# nanoBragg Spec A — Sharded Specification

Spec A is now organized across multiple shards. Read them together for the full normative contract.

- [spec-a-core.md](spec-a-core.md): Core physics, geometry, sampling, data, and associated acceptance tests.
- [spec-a-cli.md](spec-a-cli.md): CLI surface area, precedence rules, and CLI-focused acceptance tests.
- [spec-a-parallel.md](spec-a-parallel.md): Conformance profiles, parallel validation suites, and developer tooling requirements.

Each shard cross-references the others; updates must keep them consistent.
[2025-10-07T03:20:59] tokens used: 18,429

[2025-10-07T03:21:01] thinking

**Searching for architecture documentation**
[2025-10-07T03:21:02] exec bash -lc 'cat arch.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:03] bash -lc 'cat arch.md' succeeded in 694ms:
# nanoBragg PyTorch — Implementation Architecture

Document version: 1.0 (aligned with specs/spec-a.md)
Target spec: specs/spec-a.md (single-file normative spec)
Implementation target: Python (>=3.11) + PyTorch (float32 default). Type blocks may use TypeScript-style for clarity; implement with Python dataclasses and torch tensors.

## Running The CLI

- Install (recommended):
  - `pip install -e .` from repo root (provides `nanoBragg` console script).
- Run (installed):
  - `nanoBragg -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Run (without install):
  - `PYTHONPATH=src python -m nanobrag_torch -hkl P1.hkl -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -floatfile out.bin`
- Minimal quick test (no HKL):
  - `nanoBragg -default_F 1 -cell 100 100 100 90 90 90 -lambda 1.0 -distance 100 -detpixels 128 -pgmfile test.pgm`

Notes
- Entry point: `src/nanobrag_torch/__main__.py` (module runner `python -m nanobrag_torch`).
- Required inputs: provide `-hkl` (or existing `Fdump.bin`) or set `-default_F > 0`, and either `-mat` (MOSFLM A*) or `-cell`.
[2025-10-07T03:21:03] tokens used: 18,782

[2025-10-07T03:21:04] thinking

**Checking for config map documentation**
[2025-10-07T03:21:05] exec bash -lc 'cat docs/development/c_to_pytorch_config_map.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:05] bash -lc 'cat docs/development/c_to_pytorch_config_map.md' succeeded in 689ms:
# C-CLI to PyTorch Configuration Map

## Critical Importance Notice

**This document is the authoritative source of truth for configuration parity between nanoBragg.c and the PyTorch implementation.**

Before writing any test or implementation that involves C-code validation, you **MUST** consult this document. Failure to ensure 1:1 configuration parity is the most common source of bugs, particularly with:
- Implicit pivot mode logic
- Convention-dependent beam center calculations
- Rotation axis defaults
- Unit conversions

## Quick Reference Table

### Crystal Parameters

| C-CLI Flag | PyTorch Config Field | C Variable | Units/Convention | Critical Notes |
|------------|---------------------|------------|------------------|----------------|
| `-cell a b c al be ga` | `CrystalConfig.cell_a/b/c/alpha/beta/gamma` | `a[0], b[0], c[0], alpha, beta, gamma` | Å and degrees → radians | Must convert degrees to radians internally |
| `-N <val>` | `CrystalConfig.N_cells` | `Na, Nb, Nc` | Number of unit cells | Sets all three axes to same value |
[2025-10-07T03:21:05] tokens used: 21,511

[2025-10-07T03:21:07] thinking

**Searching for debugging documentation**
[2025-10-07T03:21:07] exec bash -lc 'cat docs/debugging/debugging.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:08] bash -lc 'cat docs/debugging/debugging.md' succeeded in 673ms:
# nanoBragg PyTorch Debugging Guidelines

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** PyTorch Development Team

## Overview

This document outlines the debugging methodology and tools for the nanoBragg PyTorch implementation. The debugging approach is built around a parallel trace comparison between the C code and the PyTorch implementation to ensure correctness and facilitate rapid issue resolution.

## Core Debugging Philosophy

### Parallel Trace Comparison Rule

**CRITICAL:** All debugging of physics discrepancies MUST begin with a parallel trace comparison. This involves:
1. Generating a detailed, step-by-step log of intermediate variables from the original C code for a specific pixel (the "Golden Trace").
2. Generating an identical log from the PyTorch implementation using `scripts/debug_pixel_trace.py`.
3. Comparing these two logs line-by-line to find the first point of divergence.

This methodology is mandatory to avoid guesswork and ensure bugs are found deterministically.
[2025-10-07T03:21:08] tokens used: 23,481

[2025-10-07T03:21:09] thinking

**Identifying missing documentation**
[2025-10-07T03:21:10] exec bash -lc 'cat docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:11] bash -lc 'cat docs/fix_plan.md' succeeded in 659ms:
# Fix Plan Ledger

**Last Updated:** 2025-10-17 (galph loop)
**Active Focus:**
- ROUTING: Close the reopened guard plan by capturing a fresh regression audit referencing commit `c49e3be` and re-confirming the guarded `loop.sh` flow (`plans/active/routing-loop-guard/plan.md` Phases A–C) before automation resumes.
- ROUTING-SUPERVISOR: Launch Phase A of `plans/active/supervisor-loop-guard/plan.md`, then drive Phase B guard work (B2–B4) and new task B5 to add `supervisor.sh` to docs/index.md so Protected-Asset policy covers the script before automation runs again.
- AT-012: Plan archived (`plans/archive/at-parallel-012-plateau-regression/plan.md`); monitor for regressions using `reports/2025-10-AT012-regression/phase_c_validation/` artifacts and re-open only if peak matches drop below spec.
- GRADIENT: Execute `plans/active/gradcheck-tier2-completion/plan.md` Phase A (A1–A3 baseline audit + env alignment) before adding misset/beam gradchecks from Phases B/C; once pass logs exist, close `[AT-TIER2-GRADCHECK]` with Phase D documentation updates.
- DTYPE: ✅ Complete. Plan archived to `plans/archive/dtype-default-fp32/`. All phases (A-D) finished; float32 defaults documented in arch.md, pytorch_runtime_checklist.md, CLAUDE.md, prompts/debug.md.
- PERF: Land plan task B7 (benchmark env toggle fix), rerun Phase B6 ten-process reproducibility with the new compile metadata, capture the weighted-source parity memo feeding C5, then execute Phase C diagnostics (C1/C2 plus C8/C9 pixel-grid & rotated-vector cost probes, and new C10 mosaic RNG timing) ahead of Phase D caching work (D5/D6/D7) and detector-scalar hoisting (D8).

## Index
| ID | Title | Priority | Status |
| --- | --- | --- | --- |
| [PROTECTED-ASSETS-001](#protected-assets-001-docsindexmd-safeguard) | Protect docs/index.md assets | Medium | in_progress |
| [REPO-HYGIENE-002](#repo-hygiene-002-restore-canonical-nanobraggc) | Restore canonical nanoBragg.c | Medium | in_progress |
| [PERF-PYTORCH-004](#perf-pytorch-004-fuse-physics-kernels) | Fuse physics kernels | High | in_progress |
| [AT-TIER2-GRADCHECK](#at-tier2-gradcheck-implement-tier-2-gradient-correctness-tests) | Implement Tier 2 gradient correctness tests | High | in_progress |
| [VECTOR-TRICUBIC-001](#vector-tricubic-001-vectorize-tricubic-interpolation-and-detector-absorption) | Vectorize tricubic interpolation and detector absorption | High | in_planning |
| [CLI-DTYPE-002](#cli-dtype-002-cli-dtype-parity) | CLI dtype parity | High | in_progress |
[2025-10-07T03:21:11] tokens used: 25,881

[2025-10-07T03:21:14] thinking

**Searching for testing strategy document**
[2025-10-07T03:21:14] exec bash -lc 'cat docs/development/testing_strategy.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:15] bash -lc 'cat docs/development/testing_strategy.md' succeeded in 709ms:
# nanoBragg PyTorch Testing Strategy

**Version:** 1.1  
**Date:** 2024-07-25  
**Owner:** [Your Name/Team]

## 1. Introduction & Philosophy

This document outlines the comprehensive testing strategy for the PyTorch implementation of nanoBragg. The primary goal is to ensure that the new application is a correct, verifiable, and trustworthy scientific tool.
Our testing philosophy is a three-tiered hybrid approach, designed to build confidence layer by layer:

1. **Tier 1: Translation Correctness:** We first prove that the PyTorch code is a faithful, numerically precise reimplementation of the original C code's physical model. The C code is treated as the "ground truth" specification.
2. **Tier 2: Gradient Correctness:** We then prove that the new differentiable capabilities are mathematically sound.
3. **Tier 3: Scientific Validation:** Finally, we validate the model against objective physical principles.

All tests will be implemented using the PyTest framework.

### 1.4 PyTorch Device & Dtype Discipline

- **Device-neutral code:** Every PyTorch path MUST operate correctly on both CPU and CUDA tensors and across supported dtypes. Do not hard-code `.cpu()`/`.cuda()` calls, create CPU-only constants, or assume float64 execution when the caller may supply float32/half tensors.
[2025-10-07T03:21:15] tokens used: 28,750

[2025-10-07T03:21:18] thinking

**Preparing to review active plans**
[2025-10-07T03:21:18] exec bash -lc 'ls plans/active' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:19] bash -lc 'ls plans/active' succeeded in 697ms:
cli-noise-pix0
gradcheck-tier2-completion
perf-pytorch-compile-refactor
routing-loop-guard
supervisor-loop-guard
unification.md
vectorization.md
[2025-10-07T03:21:19] tokens used: 31,303

[2025-10-07T03:21:22] thinking

**Reviewing CLI plan documents**
[2025-10-07T03:21:23] exec bash -lc 'cat plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:23] bash -lc 'cat plans/active/cli-noise-pix0/plan.md' succeeded in 717ms:
## Context
- Initiative: CLI Parity for nanoBragg PyTorch vs C (supports long-term goal in prompts/supervisor.md)
- Phase Goal: Accept `-nonoise` and `-pix0_vector_mm` flags with C-equivalent semantics so the parallel comparison command in prompts/supervisor.md executes end-to-end.
- Dependencies: specs/spec-a-cli.md §§3.2–3.4, docs/architecture/detector.md §5, docs/development/c_to_pytorch_config_map.md (detector pivot + noise), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860 (flag behavior), docs/debugging/detector_geometry_checklist.md (pix0 validation), docs/development/testing_strategy.md §2 (CLI parity tests).
- Current gap snapshot (2025-10-18 refresh): Phase H4 parity landed (Attempt #25) with pix0 deltas < 2e-8 m and regression tolerances tightened; the remaining blocker before executing the supervisor command end-to-end is Phase I polarization alignment.
- Gap snapshot update (2025-10-19): Attempt #27 parity run reveals 1.24538e5× intensity scaling mismatch despite polarization fix; Phases J–L track the normalization diagnosis, fix, and closure required before long-term Goal #1 completes.
- Gap snapshot refresh (2025-10-22): Attempt #31 restored C precedence for custom detector vectors (pix0 override now gated again). Outstanding work: H5c PyTorch traces and Phase K normalization to eliminate the `F_latt`-driven intensity gap.
- Gap snapshot update (2025-10-24): Attempt #35 reran the PyTorch trace harness post-unit-fix; metrics still show |Δpix0| ≈ 1.145 mm, so a fresh diagnostics phase is required before resuming normalization work.
- Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
- Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
- Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
- Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
- Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
- Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
- Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
- Diagnostic note (2025-11-08 galph audit): `scripts/trace_per_phi.py` currently subtracts `pix0_vector` from `detector.get_pixel_coords()`, yielding plane-relative vectors instead of sample-to-pixel paths. Fix this double subtraction in the Phase K3f PyTorch harness before recording new parity traces.
[2025-10-07T03:21:23] tokens used: 31,602

[2025-10-07T03:21:25] thinking

**Checking vectorization plan**
[2025-10-07T03:21:25] exec bash -lc 'cat plans/active/vectorization.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:26] bash -lc 'cat plans/active/vectorization.md' succeeded in 728ms:
## Context
- Initiative: PERF-PYTORCH-004 vectorization backlog; supports long-term goal "Parallel parity + performance" from docs/fix_plan.md
- Phase Goal: Deliver a production-ready, fully vectorized tricubic interpolation pipeline (gather + polynomial evaluation) and follow with detector absorption vectorization without regressing physics, gradients, or device neutrality.
- Dependencies: specs/spec-a-core.md §4 (Structure factors), specs/spec-a-parallel.md §2.3 (Interpolation acceptance tests), docs/architecture/pytorch_design.md §§2.2–2.4, docs/architecture/c_code_overview.md (tricubic routines), docs/development/testing_strategy.md §§2–4, docs/development/pytorch_runtime_checklist.md, `nanoBragg.c` lines 2604–3278 (polin3/polin2/polint + detector absorption), `tests/test_at_str_002.py`, `tests/test_at_abs_001.py`, `reports/benchmarks/20250930-165726-compile-cache/` (current performance references). No dedicated tricubic/absorption microbenchmarks exist yet; Phase A will author reusable harnesses under `scripts/benchmarks/` so future loops can rerun baselines without ad-hoc snippets.
- Gap Snapshot (2025-11-09): Phase A remains untouched; no artifacts exist under `reports/2025-10-vectorization/phase_a/`, so Ralph must prioritise A1–A3 baseline capture before any implementation edits. docs/fix_plan.md item VECTOR-TRICUBIC-001 lists these as the gating next actions.

### Phase A — Evidence & Baseline Capture
Goal: Document the current (non-vectorized) behavior, warnings, and performance so we can prove the impact of the refactor.
Prereqs: Editable install (`pip install -e .`), C binary available for parity (when needed), environment variable `KMP_DUPLICATE_LIB_OK=TRUE`.
Exit Criteria: Baseline pytest log, timing snippet results, and notes stored under `reports/2025-10-vectorization/phase_a/` with references in docs/fix_plan.md Attempt history.

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
| A1 | Re-run tricubic acceptance tests | [ ] | `env KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_at_str_002.py -v` ⇒ save log to `reports/2025-10-vectorization/phase_a/test_at_str_002.log`; confirm nearest-neighbour fallback warning message appears for batched inputs. |
| A2 | Measure current tricubic runtime | [ ] | Author `scripts/benchmarks/tricubic_baseline.py` (device/dtype neutral, honours `KMP_DUPLICATE_LIB_OK=TRUE`) and run `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/tricubic_baseline.py --sizes 256 512 --repeats 5`. Capture CPU timings (and CUDA when available) in `phase_a/tricubic_baseline.md` plus raw JSON/CSV under `phase_a/tricubic_baseline_results.json`. |
| A3 | Record detector absorption baseline | [ ] | Create `scripts/benchmarks/absorption_baseline.py` mirroring A2's interface; execute `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/absorption_baseline.py --sizes 256 512 --repeats 5` alongside `env KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_at_abs_001.py -v`. Store logs in `phase_a/absorption_baseline.md` and metrics in `phase_a/absorption_baseline_results.json`. |

### Phase B — Tricubic Vectorization Design
Goal: Lock the tensor-shape design, broadcasting plan, and gradient checks before code changes.
Prereqs: Phase A artifacts uploaded and referenced in docs/fix_plan.md.
[2025-10-07T03:21:26] tokens used: 34,385

[2025-10-07T03:21:32] thinking

**Preparing for detailed code analysis**
[2025-10-07T03:21:32] exec bash -lc 'rg "nonoise" -n' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:33] bash -lc 'rg "nonoise" -n' succeeded in 873ms:
src/nanobrag_torch/__main__.py:339:    parser.add_argument('-nonoise', action='store_true',
src/nanobrag_torch/__main__.py:711:    config['suppress_noise'] = args.nonoise
src/nanobrag_torch/__main__.py:1185:        # CLI-FLAGS-003: Honor -nonoise flag
context:248:            if(strstr(argv[i], "-nonoise") )
context:2276:            if(strstr(argv[i], "-nonoise") )
context:2609:        printf("\t-nonoise         \tdisable generating the noisefile\n");
context:6274:            if(strstr(argv[i], "-nonoise") )
context:8812:***-nonoise***
noisify.c:247:            if(strstr(argv[i], "-nonoise") )
docs/fix_plan.md:21:| [CLI-FLAGS-003](#cli-flags-003-handle-nonoise-and-pix0_vector_mm) | Handle -nonoise and -pix0_vector_mm | High | in_progress |
docs/fix_plan.md:448:## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
docs/fix_plan.md:455:  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
docs/fix_plan.md:456:  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
docs/fix_plan.md:841:      - `reports/2025-10-cli-flags/phase_a/c_with_nonoise.log` - C run WITH -nonoise
docs/fix_plan.md:842:      - `reports/2025-10-cli-flags/phase_a/c_with_noise.log` - C run WITHOUT -nonoise
docs/fix_plan.md:846:      - `-nonoise` suppresses noise parameters section AND noise image generation (noiseimage.img not written)
docs/fix_plan.md:847:      - Float/int/PGM images still generated when -nonoise is active
docs/fix_plan.md:868:      - ✅ -nonoise suppresses noisefile generation (suppress_noise=True)
docs/fix_plan.md:869:      - ✅ -nonoise doesn't mutate adc default (40.0) or seed handling
docs/fix_plan.md:870:      - ✅ -noisefile without -nonoise enables noise generation (suppress_noise=False)
[2025-10-07T03:21:33] tokens used: 37,158

[2025-10-07T03:21:35] thinking

**Investigating -nonoise implementation**
[2025-10-07T03:21:36] exec bash -lc 'rg "pix0_vector_mm" -n' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:37] bash -lc 'rg "pix0_vector_mm" -n' succeeded in 718ms:
src/nanobrag_torch/__main__.py:190:    parser.add_argument('-pix0_vector_mm', nargs=3, type=float,
src/nanobrag_torch/__main__.py:462:            args.pix0_vector, args.pix0_vector_mm]):
src/nanobrag_torch/__main__.py:557:    if args.pix0_vector and args.pix0_vector_mm:
src/nanobrag_torch/__main__.py:558:        raise ValueError("Cannot specify both -pix0_vector and -pix0_vector_mm simultaneously")
src/nanobrag_torch/__main__.py:564:    elif args.pix0_vector_mm:
src/nanobrag_torch/__main__.py:566:        config['pix0_override_m'] = tuple(x * 0.001 for x in args.pix0_vector_mm)
src/nanobrag_torch/models/detector.py:524:            # C code IGNORES -pix0_vector_mm when custom detector vectors are present.
SPEC_PROMPT.md:65:  - Unsupported example flags: Explicitly list any flags present in the examples but not recognized by this code (e.g., `-pix0_vector_mm` or `-matrix` if absent), and explain the closest supported equivalent if any.
docs/fix_plan.md:21:| [CLI-FLAGS-003](#cli-flags-003-handle-nonoise-and-pix0_vector_mm) | Handle -nonoise and -pix0_vector_mm | High | in_progress |
docs/fix_plan.md:448:## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
docs/fix_plan.md:509:    Metrics: All 26 CLI flags tests passed (test_cli_flags.py). Target test: test_pix0_vector_mm_beam_pivot 4/4 passed (cpu + cuda, dtype0 + dtype1). Expected pix0 delta reduced from 1.136 mm to well within 5e-5 m threshold.
docs/fix_plan.md:547:      - **Pivot:** SAMPLE (custom vectors present, so -pix0_vector_mm override is ignored per C precedence)
docs/fix_plan.md:763:      - `reports/2025-10-cli-flags/phase_h5/c_traces/with_override.log` - C run WITH -pix0_vector_mm flag
docs/fix_plan.md:764:      - `reports/2025-10-cli-flags/phase_h5/c_traces/without_override.log` - C run WITHOUT -pix0_vector_mm flag
docs/fix_plan.md:768:      - **Critical finding:** When custom detector vectors (`-odet_vector`, `-sdet_vector`, `-fdet_vector`) are supplied, C code computes pix0 from Xbeam/Ybeam and IGNORES `-pix0_vector_mm` override
docs/fix_plan.md:780:      - `reports/2025-10-cli-flags/phase_h5/c_traces/2025-10-22/with_override.log` - 267 lines, C run WITH -pix0_vector_mm flag
docs/fix_plan.md:781:      - `reports/2025-10-cli-flags/phase_h5/c_traces/2025-10-22/without_override.log` - 267 lines, C run WITHOUT -pix0_vector_mm flag
docs/fix_plan.md:785:      - **Re-confirms 2025-10-06 evidence:** C ignores `-pix0_vector_mm` when custom detector vectors are supplied
docs/fix_plan.md:848:      - C code treats `-pix0_vector` and `-pix0_vector_mm` identically (strstr match, no unit conversion)
docs/fix_plan.md:860:      - ✅ pix0_vector_mm millimeters alias parsing with conversion to meters (mm * 0.001)
[2025-10-07T03:21:37] tokens used: 38,945

[2025-10-07T03:21:38] thinking

**Inspecting detector.py behavior**
[2025-10-07T03:21:39] exec bash -lc "sed -n '480,560p' src/nanobrag_torch/models/detector.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:40] bash -lc "sed -n '480,560p' src/nanobrag_torch/models/detector.py" succeeded in 686ms:
            self.r_factor = ratio.to(device=self.device, dtype=self.dtype)
        else:
            self.r_factor = torch.tensor(ratio, device=self.device, dtype=self.dtype)

        if self.config.detector_pivot == DetectorPivot.BEAM:
            # BEAM pivot mode: detector rotates around the direct beam spot
            # Use exact C-code formula: pix0_vector = -Fbeam*fdet_vec - Sbeam*sdet_vec + distance*beam_vec

            # Calculate Fbeam and Sbeam from beam centers
            # For MOSFLM: Fbeam = Ybeam + 0.5*pixel_size, Sbeam = Xbeam + 0.5*pixel_size (C-code line 382)
            # CLI-FLAGS-003 Phase H5e unit correction (2025-10-24):
            # CRITICAL: self.beam_center_f and self.beam_center_s are stored as config values
            # from DetectorConfig in MILLIMETERS (see config.py:186-188, __main__.py:921-922, 930-931).
            # Must convert mm→m before use in geometry calculations.
            #
            # C-code reference (nanoBragg.c:1184-1239, specifically lines 1220-1221 for MOSFLM):
            # Fbeam_m = (Ybeam_mm + 0.5*pixel_mm) / 1000
            # Sbeam_m = (Xbeam_mm + 0.5*pixel_mm) / 1000
            #
            # Since __main__.py already applies the axis swap at lines 921-922, we only need to:
[2025-10-07T03:21:40] tokens used: 42,545

[2025-10-07T03:21:41] thinking

**Continuing process**
[2025-10-07T03:21:42] exec bash -lc "sed -n '560,680p' src/nanobrag_torch/models/detector.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:42] bash -lc "sed -n '560,680p' src/nanobrag_torch/models/detector.py" succeeded in 676ms:
                # Override the Fbeam/Sbeam values with derived ones
                Fbeam = Fbeam_override
                Sbeam = Sbeam_override

                # Update beam_center_f/s tensors to maintain header consistency
                # Convert from meters back to pixels: beam_center = offset_m / pixel_size_m
                # Note: For MOSFLM the +0.5 offset was already applied above, so reverse it
                if self.config.detector_convention == DetectorConvention.MOSFLM:
                    # MOSFLM: Fbeam = (beam_center_f + 0.5) * pixel_size
                    # So: beam_center_f = Fbeam / pixel_size - 0.5
                    self.beam_center_f = (Fbeam / self.pixel_size - 0.5).to(device=self.device, dtype=self.dtype)
                    self.beam_center_s = (Sbeam / self.pixel_size - 0.5).to(device=self.device, dtype=self.dtype)
                else:
                    # Other conventions: beam_center = offset / pixel_size
                    self.beam_center_f = (Fbeam / self.pixel_size).to(device=self.device, dtype=self.dtype)
                    self.beam_center_s = (Sbeam / self.pixel_size).to(device=self.device, dtype=self.dtype)

            # Use exact C-code formula WITH distance correction (AT-GEO-003)
            # BEAM pivot formula (C-code reference: nanoBragg.c lines 1833-1835):
            # pix0_vector[1] = -Fbeam*fdet_vector[1]-Sbeam*sdet_vector[1]+distance*beam_vector[1];
[2025-10-07T03:21:42] tokens used: 44,064

[2025-10-07T03:21:45] thinking

**Inspecting noise suppression logic**
[2025-10-07T03:21:45] exec bash -lc "sed -n '660,760p' src/nanobrag_torch/__main__.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:46] bash -lc "sed -n '660,760p' src/nanobrag_torch/__main__.py" succeeded in 698ms:
    # Fluence calculation
    if args.flux and args.exposure and args.beamsize:
        config['flux'] = args.flux
        config['exposure'] = args.exposure
        config['beamsize_mm'] = args.beamsize
    elif args.fluence:
        config['fluence'] = args.fluence

    # Polarization
    if args.nopolar:
        config['nopolar'] = True
    elif args.polar is not None:
        config['polarization_factor'] = args.polar

    # Divergence and dispersion parameters for source generation
    # Convert from mrad to radians for divergence parameters
    if args.divergence is not None:
        config['hdivrange'] = mrad_to_radians(args.divergence)
        config['vdivrange'] = mrad_to_radians(args.divergence)
    if args.hdivrange is not None:
[2025-10-07T03:21:46] tokens used: 46,250

[2025-10-07T03:21:48] thinking

**Reviewing noise handling in write section**
[2025-10-07T03:21:49] exec bash -lc "sed -n '1160,1220p' src/nanobrag_torch/__main__.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:49] bash -lc "sed -n '1160,1220p' src/nanobrag_torch/__main__.py" succeeded in 687ms:
                filepath=config['intfile'],
                image_data=scaled_int,
                pixel_size_mm=detector_config.pixel_size_mm,
                distance_mm=detector_config.distance_mm,
                wavelength_angstrom=beam_config.wavelength_A,
                beam_center_x_mm=detector_config.beam_center_s,
                beam_center_y_mm=detector_config.beam_center_f,
                close_distance_mm=detector_config.close_distance_mm,
                phi_deg=config.get('phi_deg', 0.0),
                osc_start_deg=config.get('phi_deg', 0.0),
                osc_range_deg=config.get('osc_deg', 0.0),
                twotheta_deg=config.get('twotheta_deg', 0.0),
                convention=detector_config.detector_convention.name,
                scale=1.0,  # Already scaled
                adc_offset=0.0  # Already applied ADC
            )
            print(f"Wrote SMV image to {config['intfile']}")

        if config.get('pgmfile'):
            # Write PGM per AT-CLI-006
[2025-10-07T03:21:49] tokens used: 47,770

[2025-10-07T03:21:57] thinking

**Reviewing noise suppression and CLI flags implementation**

I’m analyzing how noise suppression works in the pipeline and whether the CLI flags like -nonoise and -pix0_vector_mm are fully supported and consistent with long-term goals. I also plan to inspect the fix_plan document sections around CLI flags to understand current tasks and prepare hypotheses about scaling mismatches before moving on.
[2025-10-07T03:21:58] exec bash -lc "sed -n '430,620p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:21:59] bash -lc "sed -n '430,620p' docs/fix_plan.md" succeeded in 714ms:
            "-cell", "100", "100", "100", "90", "90", "90",
            "-default_F", "0",
            "-dtype", "float64"
        ])
        config = parse_and_validate_args(args)
        grid, _ = config['hkl_data']
        print(grid.dtype)  # BUG: prints torch.float32 today
    ```
    `PY`
- First Divergence (if known): `parse_and_validate_args` (src/nanobrag_torch/__main__.py:435) calls `read_hkl_file`/`try_load_hkl_or_fdump` without forwarding the parsed dtype, so HKL/Fdump data is always loaded as float32. When the CLI is run with `-dtype float64`, the simulator converts those float32 tensors to float64 later, permanently losing precision relative to the requested double-precision path and violating the DTYPE-DEFAULT-001 guarantee.
- Immediate Next Actions (2025-10-15):
  * Update `parse_and_validate_args` (and any helper that constructs `CrystalConfig`) to thread the parsed dtype/device into HKL/Fdump loaders so CLI `-dtype` honours float64 from the start.
  * Add a regression test (e.g., `tests/test_cli_dtype.py`) that invokes the parser with `-dtype float64` and asserts `config['hkl_data'][0].dtype == torch.float64`, covering both HKL and Fdump paths.
  * Re-run AT-IO-003 and the CLI smoke (help + minimal render) to ensure no regressions, capturing logs under `reports/DTYPE-CLI/<date>/`.
- Attempts History: None — new item.
- Risks/Assumptions: Ensure `write_fdump` continues to emit float64 on disk (spec requirement) while the in-memory tensor honours caller dtype; watch for latent callers that relied on the old float64 default during plan DTYPE-DEFAULT-001 migration.
- Exit Criteria: CLI runs with `-dtype float64` produce double-precision HKL/Fdump tensors end-to-end, regression test passes, and existing dtype-sensitive tests (AT-IO-003, CLI smoke, gradchecks) remain green.

## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
[2025-10-07T03:21:59] tokens used: 49,094

[2025-10-07T03:22:02] thinking

**Verifying instrumentation status**
[2025-10-07T03:22:03] exec bash -lc 'rg "TRACE_PY" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:03] bash -lc 'rg "TRACE_PY" -n src/nanobrag_torch/simulator.py' succeeded in 669ms:
1246:                print(f"TRACE_PY: pix0_vector_meters {pix0[0].item():.15g} {pix0[1].item():.15g} {pix0[2].item():.15g}")
1251:                print(f"TRACE_PY: fdet_vec {fdet[0].item():.15g} {fdet[1].item():.15g} {fdet[2].item():.15g}")
1252:                print(f"TRACE_PY: sdet_vec {sdet[0].item():.15g} {sdet[1].item():.15g} {sdet[2].item():.15g}")
1257:                    print(f"TRACE_PY: pixel_pos_meters {coords[0].item():.15g} {coords[1].item():.15g} {coords[2].item():.15g}")
1261:                    print(f"TRACE_PY: R_distance_meters {airpath_m:.15g}")
1271:                    print(f"TRACE_PY: omega_pixel_sr {omega_pixel_val:.15g}")
1272:                    print(f"TRACE_PY: close_distance_meters {close_distance_m:.15g}")
1273:                    print(f"TRACE_PY: obliquity_factor {close_distance_m/airpath_m:.15g}")
1281:                    print(f"TRACE_PY: diffracted_vec {diffracted_vec[0].item():.15g} {diffracted_vec[1].item():.15g} {diffracted_vec[2].item():.15g}")
1282:                    print(f"TRACE_PY: incident_vec {incident_vec[0].item():.15g} {incident_vec[1].item():.15g} {incident_vec[2].item():.15g}")
1286:                    print(f"TRACE_PY: lambda_meters {lambda_m:.15g}")
1287:                    print(f"TRACE_PY: lambda_angstroms {self.wavelength.item():.15g}")
1292:                    print(f"TRACE_PY: scattering_vec_A_inv {scattering_vec_m_inv[0].item():.15g} {scattering_vec_m_inv[1].item():.15g} {scattering_vec_m_inv[2].item():.15g}")
1301:                    print(f"TRACE_PY: rot_a_angstroms {a_vec[0].item():.15g} {a_vec[1].item():.15g} {a_vec[2].item():.15g}")
1302:                    print(f"TRACE_PY: rot_b_angstroms {b_vec[0].item():.15g} {b_vec[1].item():.15g} {b_vec[2].item():.15g}")
1303:                    print(f"TRACE_PY: rot_c_angstroms {c_vec[0].item():.15g} {c_vec[1].item():.15g} {c_vec[2].item():.15g}")
1309:                    print(f"TRACE_PY: rot_a_star_A_inv {a_star_vec[0].item():.15g} {a_star_vec[1].item():.15g} {a_star_vec[2].item():.15g}")
1310:                    print(f"TRACE_PY: rot_b_star_A_inv {b_star_vec[0].item():.15g} {b_star_vec[1].item():.15g} {b_star_vec[2].item():.15g}")
1311:                    print(f"TRACE_PY: rot_c_star_A_inv {c_star_vec[0].item():.15g} {c_star_vec[1].item():.15g} {c_star_vec[2].item():.15g}")
1330:                    print(f"TRACE_PY: hkl_frac {h:.15g} {k:.15g} {l:.15g}")
[2025-10-07T03:22:03] tokens used: 52,219

[2025-10-07T03:22:07] thinking

**Inspecting potential placeholder values**
[2025-10-07T03:22:07] exec bash -lc "sed -n '1330,1430p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:08] bash -lc "sed -n '1330,1430p' src/nanobrag_torch/simulator.py" succeeded in 774ms:
                    print(f"TRACE_PY: hkl_frac {h:.15g} {k:.15g} {l:.15g}")
                    print(f"TRACE_PY: hkl_rounded {h0} {k0} {l0}")

                    # Calculate F_latt components (SQUARE shape assumed)
                    # Per specs/spec-a-core.md §4.3 and nanoBragg.c:3071-3079:
                    # Use fractional h,k,l directly (NOT h-h0) for SQUARE lattice factor
                    Na = self.crystal.N_cells_a
                    Nb = self.crystal.N_cells_b
                    Nc = self.crystal.N_cells_c

                    from nanobrag_torch.utils import sincg
                    F_latt_a = sincg(torch.pi * torch.tensor(h), Na).item()
                    F_latt_b = sincg(torch.pi * torch.tensor(k), Nb).item()
                    F_latt_c = sincg(torch.pi * torch.tensor(l), Nc).item()
                    F_latt = F_latt_a * F_latt_b * F_latt_c

                    print(f"TRACE_PY: F_latt_a {F_latt_a:.15g}")
                    print(f"TRACE_PY: F_latt_b {F_latt_b:.15g}")
                    print(f"TRACE_PY: F_latt_c {F_latt_c:.15g}")
                    print(f"TRACE_PY: F_latt {F_latt:.15g}")
[2025-10-07T03:22:08] tokens used: 53,889

[2025-10-07T03:22:12] thinking

**Inspecting variable usage and data types**
[2025-10-07T03:22:12] exec bash -lc 'rg "steps =" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:13] bash -lc 'rg "steps =" -n src/nanobrag_torch/simulator.py' succeeded in 728ms:
459:                self.crystal.config.phi_steps = crystal_config.phi_steps
839:        # PERF-PYTORCH-004 P3.0c: Per AT-SRC-001 "steps = sources; intensity contributions SHALL sum with per-source λ and weight, then divide by steps"
842:        phi_steps = self.crystal.config.phi_steps
846:        steps = source_norm * phi_steps * mosaic_domains * oversample * oversample  # Include sources and oversample^2
1514:        thicksteps = self.detector.config.detector_thicksteps
[2025-10-07T03:22:13] tokens used: 55,673

[2025-10-07T03:22:16] thinking

**Inspecting steps definition area**
[2025-10-07T03:22:16] exec bash -lc "sed -n '780,900p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:17] bash -lc "sed -n '780,900p' src/nanobrag_torch/simulator.py" succeeded in 694ms:
            print(f"auto-selected {oversample}-fold oversampling")

        # For now, we'll implement the base case without oversampling for this test
        # The full subpixel implementation will come later
        # This matches the current implementation which doesn't yet have subpixel sampling

        # PERF-PYTORCH-004 P3.4: Use cached ROI mask instead of rebuilding every run
        roi_mask = self._cached_roi_mask

        # PERF-PYTORCH-004 P3.4: Use cached pixel coordinates instead of fetching/converting every run
        pixel_coords_meters = self._cached_pixel_coords_meters

        # Get rotated lattice vectors for all phi steps and mosaic domains
        # Shape: (N_phi, N_mos, 3)
        # PERF-PYTORCH-006: Convert crystal vectors to correct device/dtype
        if override_a_star is None:
            (rot_a, rot_b, rot_c), (rot_a_star, rot_b_star, rot_c_star) = (
                self.crystal.get_rotated_real_vectors(self.crystal.config)
            )
            # Convert to correct dtype/device
[2025-10-07T03:22:17] tokens used: 56,016

[2025-10-07T03:22:26] thinking

**Verifying steps calculation and instrumentation**
[2025-10-07T03:22:27] exec bash -lc "sed -n '400,520p' tests/test_cli_flags.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:28] bash -lc "sed -n '400,520p' tests/test_cli_flags.py" succeeded in 884ms:
        # Test configuration: MOSFLM convention with BEAM pivot
        distance_mm = 100.0  # 100mm = 0.1m
        pixel_size_mm = 0.1
        spixels = fpixels = 512

        # pix0_override value
        pix0_override = torch.tensor([0.1, -0.05, 0.05], device=device, dtype=dtype)

        cfg = DetectorConfig(
            distance_mm=distance_mm,
            pixel_size_mm=pixel_size_mm,
            spixels=spixels,
            fpixels=fpixels,
            beam_center_f=51.2,  # mm
            beam_center_s=51.2,  # mm
            detector_convention=DetectorConvention.MOSFLM,
            detector_pivot=DetectorPivot.BEAM,
            pix0_override_m=pix0_override  # Should be APPLIED (no custom vectors)
        )

[2025-10-07T03:22:28] tokens used: 58,171

[2025-10-07T03:22:31] thinking

**Analyzing scaling and normalization mismatches**
[2025-10-07T03:22:32] exec bash -lc "sed -n '640,780p' src/nanobrag_torch/models/crystal.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:33] bash -lc "sed -n '640,780p' src/nanobrag_torch/models/crystal.py" succeeded in 675ms:
        ):
            # Apply misset rotation to initial reciprocal vectors only
            # C-code reference (nanoBragg.c lines 2025-2034):
            # rotate(a_star,a_star,misset[1],misset[2],misset[3]);
            # rotate(b_star,b_star,misset[1],misset[2],misset[3]);
            # rotate(c_star,c_star,misset[1],misset[2],misset[3]);
            from ..utils.geometry import angles_to_rotation_matrix, rotate_umat
            # Preserve gradient flow: convert to tensor only if not already a tensor
            misset_rad = []
            for angle in self.config.misset_deg:
                if isinstance(angle, torch.Tensor):
                    # Already a tensor - just convert dtype/device while preserving gradients
                    angle_tensor = angle.to(dtype=self.dtype, device=self.device)
                else:
                    # Scalar - create new tensor
                    angle_tensor = torch.tensor(angle, dtype=self.dtype, device=self.device)
                misset_rad.append(torch.deg2rad(angle_tensor))
            R_misset = angles_to_rotation_matrix(misset_rad[0], misset_rad[1], misset_rad[2])
            a_star = rotate_umat(a_star, R_misset)
            b_star = rotate_umat(b_star, R_misset)
[2025-10-07T03:22:33] tokens used: 60,090

[2025-10-07T03:22:38] thinking

**Examining scaling and status details**
[2025-10-07T03:22:38] exec bash -lc 'rg "def sincg" -n' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:39] bash -lc 'rg "def sincg" -n' succeeded in 779ms:
src/nanobrag_torch/utils/physics.py:33:def sincg(u: torch.Tensor, N: torch.Tensor) -> torch.Tensor:
golden_suite_generator/scripts/trace_at012_pixel_368_262.py:94:    def sincg(x, N):
scripts/trace_at012_pixel_513_446.py:189:    def sincg(x, N):
scripts/trace_pytorch_pixel.py:201:        def sincg_trace(arg, N):
[2025-10-07T03:22:39] tokens used: 62,663

[2025-10-07T03:22:41] thinking

**Checking utils/physics.sincg definition**
[2025-10-07T03:22:42] exec bash -lc "sed -n '1,160p' src/nanobrag_torch/utils/physics.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:42] bash -lc "sed -n '1,160p' src/nanobrag_torch/utils/physics.py" succeeded in 704ms:
"""
Vectorized physics utilities for nanoBragg PyTorch implementation.

This module contains PyTorch implementations of physical models and
calculations from the original C code.
"""

import torch


# Use different compile modes for CPU vs GPU to avoid CUDA graph issues
# On GPU, "max-autotune" avoids the nested compilation CUDA graph problem
# On CPU, "reduce-overhead" is faster
def _get_compile_decorator():
    """Get appropriate compile decorator based on available hardware."""
    import os
    disable_compile = os.environ.get("NANOBRAGG_DISABLE_COMPILE", "0") == "1"

    if disable_compile:
        # Return identity decorator when compilation is disabled (for gradient testing)
[2025-10-07T03:22:42] tokens used: 63,030

[2025-10-07T03:22:47] thinking

**Analyzing vectorization plan status**
[2025-10-07T03:22:47] exec bash -lc 'ls reports/2025-10-vectorization' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:48] bash -lc 'ls reports/2025-10-vectorization' exited 1 in 741ms:
ls: reports/2025-10-vectorization: No such file or directory
[2025-10-07T03:22:48] tokens used: 65,069

[2025-10-07T03:22:56] thinking

**Reviewing relevant plan documents**
[2025-10-07T03:22:57] exec bash -lc 'cat plans/active/gradcheck-tier2-completion/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:22:57] bash -lc 'cat plans/active/gradcheck-tier2-completion/plan.md' succeeded in 714ms:
# Plan: AT-TIER2-GRADCHECK Completion

**Status:** Active (new 2025-10-15)
**Priority:** High — spec compliance blocker (testing_strategy §4.1)
**Related fix_plan item:** `[AT-TIER2-GRADCHECK]` — docs/fix_plan.md
**Created:** 2025-10-15 by galph

## Context
- Initiative: Ensure differentiability guarantees (long-term goal: autodiff-based refinement showcase).
- Phase Goal: Close the remaining Tier-2 gradcheck gaps called out in testing_strategy.md §4.1: `misset_rot_x`, beam `lambda_A`, and `fluence`.
- Current State: Commit 0e3054c implemented gradchecks for unit-cell lengths/angles and detector distance/beam_center_f. Commit d45a0f3 introduced `NANOBRAGG_DISABLE_COMPILE` to work around torch.compile bugs. However, no coverage exists yet for misset rotations or beam parameters, and the harness still emits contradictory compile-mode toggles (`NB_DISABLE_COMPILE` vs `NANOBRAGG_DISABLE_COMPILE`). The spec remains unmet.
- Dependencies: `tests/test_gradients.py`, `tests/test_suite.py::TestTier2GradientCorrectness`, `docs/development/testing_strategy.md §4.1`, `arch.md §15`, perf plan task B7 (env-var alignment for compile toggles).

### Phase A — Baseline Audit & Environment Alignment
Goal: Capture authoritative evidence of current coverage gaps and ensure the compile-disable switch is consistent across harnesses before adding new tests.
Prereqs: None.
Exit Criteria: Audit note summarising missing parameters, baseline gradcheck log attached, and env-var naming decision recorded (either standardise on `NANOBRAGG_DISABLE_COMPILE` or support both).

| ID | Task Description | State | How/Why & Guidance |
| --- | --- | --- | --- |
[2025-10-07T03:22:57] tokens used: 65,451

[2025-10-07T03:23:02] thinking

**Analyzing scaling mismatch issues**
[2025-10-07T03:23:02] exec bash -lc 'rg "get_structure_factor" -n src/nanobrag_torch/models/crystal.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:03] bash -lc 'rg "get_structure_factor" -n src/nanobrag_torch/models/crystal.py' succeeded in 696ms:
184:    def get_structure_factor(
[2025-10-07T03:23:03] tokens used: 67,370
[2025-10-07T03:23:06] exec bash -lc "sed -n '180,260p' src/nanobrag_torch/models/crystal.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:07] bash -lc "sed -n '180,260p' src/nanobrag_torch/models/crystal.py" succeeded in 684ms:

        # Note: Auto-enable interpolation is already handled in __init__
        # This should be overridden by explicit CLI flags when they're implemented

    def get_structure_factor(
        self, h: torch.Tensor, k: torch.Tensor, l: torch.Tensor  # noqa: E741
    ) -> torch.Tensor:
        """
        Look up or interpolate the structure factor for given h,k,l indices.

        Implements AT-STR-001 (nearest-neighbor lookup) and AT-STR-002 (tricubic interpolation).

        This method handles both nearest-neighbor lookup and differentiable tricubic
        interpolation, as determined by self.interpolate flag, to match the C-code.

        C-Code Implementation Reference (from nanoBragg.c, lines 3101-3139):
        The C code performs nearest-neighbor lookup or tricubic interpolation based
        on the 'interpolate' flag. Out-of-range accesses return default_F.
        """
        # If no HKL data loaded, return default_F for all reflections
[2025-10-07T03:23:07] tokens used: 67,614

[2025-10-07T03:23:11] thinking

**Investigating physical intensity calculation**
[2025-10-07T03:23:11] exec bash -lc 'rg "physical_intensity" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:12] bash -lc 'rg "physical_intensity" -n src/nanobrag_torch/simulator.py' succeeded in 689ms:
1083:        physical_intensity = (
1092:            physical_intensity = physical_intensity + water_background
1098:        if physical_intensity.shape != roi_mask.shape:
1100:            actual_spixels, actual_fpixels = physical_intensity.shape
1101:            roi_mask = torch.ones_like(physical_intensity)
1117:                if self.detector.config.mask_array.shape == physical_intensity.shape:
1123:        # Ensure roi_mask is on the same device as physical_intensity
1124:        roi_mask = roi_mask.to(physical_intensity.device)
1125:        physical_intensity = physical_intensity * roi_mask
1161:                physical_intensity,
1175:        return physical_intensity.to(dtype=self.dtype)
1178:                           physical_intensity,
1191:            physical_intensity: Final intensity values
1210:            if 0 <= target_slow < physical_intensity.shape[0] and 0 <= target_fast < physical_intensity.shape[1]:
1212:                print(f"  Final intensity: {physical_intensity[target_slow, target_fast].item():.6e}")
1227:            print(f"  Image shape: {physical_intensity.shape[0]} x {physical_intensity.shape[1]} pixels")
1228:            print(f"  Max intensity: {physical_intensity.max().item():.6e}")
1229:            print(f"  Min intensity: {physical_intensity.min().item():.6e}")
1230:            print(f"  Mean intensity: {physical_intensity.mean().item():.6e}")
1233:            max_val = physical_intensity.max()
[2025-10-07T03:23:12] tokens used: 69,757

[2025-10-07T03:23:16] thinking

**Checking specific line**
[2025-10-07T03:23:17] exec bash -lc "sed -n '1020,1105p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:18] bash -lc "sed -n '1020,1105p' src/nanobrag_torch/simulator.py" succeeded in 675ms:
                pixel_coords_angstroms * pixel_coords_angstroms, dim=-1, keepdim=True
            )
            # Use clamp_min to avoid creating fresh tensors in compiled graph (PERF-PYTORCH-004 P1.1)
            pixel_squared_sum = pixel_squared_sum.clamp_min(1e-12)
            pixel_magnitudes = torch.sqrt(pixel_squared_sum)
            airpath = pixel_magnitudes.squeeze(-1)  # Remove last dimension for broadcasting
            airpath_m = airpath * 1e-10  # Å to meters
            # Convert detector properties to tensors with correct device/dtype (AT-PERF-DEVICE-001)
            # Use as_tensor to avoid warnings when value might already be a tensor
            close_distance_m = torch.as_tensor(self.detector.close_distance, device=airpath_m.device, dtype=airpath_m.dtype)
            pixel_size_m = torch.as_tensor(self.detector.pixel_size, device=airpath_m.device, dtype=airpath_m.dtype)

            # Calculate solid angle (omega) based on point_pixel mode
            if self.detector.config.point_pixel:
                # Point pixel mode: ω = 1 / R^2
                omega_pixel = 1.0 / (airpath_m * airpath_m)
            else:
                # Standard mode with obliquity correction
                # ω = (pixel_size^2 / R^2) · (close_distance/R)
                omega_pixel = (
[2025-10-07T03:23:18] tokens used: 71,453

[2025-10-07T03:23:20] thinking

**Verifying polarization factor usage**
[2025-10-07T03:23:21] exec bash -lc 'rg "polarization" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:22] bash -lc 'rg "polarization" -n src/nanobrag_torch/simulator.py' succeeded in 726ms:
16:from .utils.physics import sincg, sinc3, polarization_factor
43:    apply_polarization: bool = True,
45:    polarization_axis: Optional[torch.Tensor] = None,
90:        apply_polarization: Whether to apply Kahn polarization correction (default True)
92:        polarization_axis: Polarization axis unit vector (3,) or broadcastable shape
324:    # PERF-PYTORCH-004 P3.0b: Apply polarization PER-SOURCE before weighted sum
325:    # IMPORTANT: Apply polarization even when kahn_factor==0.0 (unpolarized case)
327:    if apply_polarization:
328:        # Calculate polarization factor for each source
334:            # Multi-source case: apply per-source polarization
350:            # Flatten for polarization calculation
356:            # Calculate polarization
357:            polar_flat = polarization_factor(
361:                polarization_axis
370:            # Apply polarization per source
374:            # Flatten for polarization calculation
382:            # Calculate polarization
383:            polar_flat = polarization_factor(
387:                polarization_axis
396:            # Apply polarization
[2025-10-07T03:23:22] tokens used: 72,684

[2025-10-07T03:23:24] thinking

**Checking function for polarization logic**
[2025-10-07T03:23:25] exec bash -lc "sed -n '300,420p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:26] bash -lc "sed -n '300,420p' src/nanobrag_torch/simulator.py" succeeded in 710ms:
        F_latt = torch.where(inside_cutoff,
                            torch.full_like(rad_star_sqr, Na * Nb * Nc),
                            torch.zeros_like(rad_star_sqr))
    else:
        raise ValueError(f"Unsupported crystal shape: {shape}")

    # Calculate intensity
    F_total = F_cell * F_latt
    intensity = F_total * F_total  # |F|^2

    # Apply dmin culling
    if dmin_mask is not None:
        if is_multi_source:
            # Multi-source: add two unsqueeze for phi and mosaic
            keep_mask = ~dmin_mask.unsqueeze(-1).unsqueeze(-1)
        else:
            keep_mask = ~dmin_mask.unsqueeze(-1).unsqueeze(-1)
        intensity = intensity * keep_mask.to(intensity.dtype)

    # Sum over phi and mosaic dimensions
[2025-10-07T03:23:26] tokens used: 74,142

[2025-10-07T03:23:30] thinking

**Searching for division by steps**
[2025-10-07T03:23:31] exec bash -lc 'rg "/ steps" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:31] bash -lc 'rg "/ steps" -n src/nanobrag_torch/simulator.py' succeeded in 747ms:
936:            subpixel_physics_intensity_all = subpixel_physics_intensity_all / steps
1013:            normalized_intensity = intensity / steps
1359:                    # Calculate I_before_scaling (this is before r_e^2 * fluence / steps)
[2025-10-07T03:23:31] tokens used: 75,728

[2025-10-07T03:23:36] thinking

**Inspecting lattice force calculations**
[2025-10-07T03:23:37] exec bash -lc 'rg "F_latt" src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:37] bash -lc 'rg "F_latt" src/nanobrag_torch/simulator.py' succeeded in 721ms:
    #     F_latt = sin(M_PI*Na*h)*sin(M_PI*Nb*k)*sin(M_PI*Nc*l)/sin(M_PI*h)/sin(M_PI*k)/sin(M_PI*l);
    # F_latt = 1.0;
    #     double F_latt_a = 1.0, F_latt_b = 1.0, F_latt_c = 1.0;
    #         F_latt_a = sincg(M_PI*h,Na);
    #         F_latt *= F_latt_a;
    #         F_latt_b = sincg(M_PI*k,Nb);
    #         F_latt *= F_latt_b;
    #         F_latt_c = sincg(M_PI*l,Nc);
    #         F_latt *= F_latt_c;
    # "SQUARE (grating): F_latt = sincg(π·h, Na) · sincg(π·k, Nb) · sincg(π·l, Nc)"
        F_latt_a = sincg(torch.pi * h, Na)
        F_latt_b = sincg(torch.pi * k, Nb)
        F_latt_c = sincg(torch.pi * l, Nc)
        F_latt = F_latt_a * F_latt_b * F_latt_c
        F_latt = Na * Nb * Nc * 0.723601254558268 * sinc3(
        F_latt = Na * Nb * Nc * torch.exp(-(rad_star_sqr / 0.63) * fudge)
        F_latt = torch.where(inside_cutoff,
    F_total = F_cell * F_latt
                                    /* ... F_cell and F_latt calculation ... */
                                    I += F_cell*F_cell*F_latt*F_latt;
[2025-10-07T03:23:37] tokens used: 76,990

[2025-10-07T03:23:41] thinking

**Searching for actual definition**
[2025-10-07T03:23:42] exec bash -lc "sed -n '230,360p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:43] bash -lc "sed -n '230,360p' src/nanobrag_torch/simulator.py" succeeded in 695ms:
    #     }
    #     if(Nc>1){
    #         F_latt_c = sincg(M_PI*l,Nc);
    #         F_latt *= F_latt_c;
    #     }
    # }
    # ```
    #
    # Per specs/spec-a-core.md §4.3:
    # "SQUARE (grating): F_latt = sincg(π·h, Na) · sincg(π·k, Nb) · sincg(π·l, Nc)"
    #
    # CRITICAL: Use fractional h,k,l directly (NOT h-h0), unlike ROUND/GAUSS/TOPHAT shapes.
    Na = N_cells_a
    Nb = N_cells_b
    Nc = N_cells_c
    shape = crystal_shape
    fudge = crystal_fudge

    if shape == CrystalShape.SQUARE:
        # Note: sincg internally handles Na/Nb/Nc > 1 guards per C implementation
[2025-10-07T03:23:43] tokens used: 77,821

[2025-10-07T03:23:45] thinking

**Investigating physics computations**
[2025-10-07T03:23:46] exec bash -lc 'rg "h =" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:46] bash -lc 'rg "h =" -n src/nanobrag_torch/simulator.py' succeeded in 659ms:
150:                wavelength = wavelength.view(n_sources, 1, 1)
153:                wavelength = wavelength.view(n_sources, 1, 1, 1)
194:    h = dot_product(scattering_broadcast, rot_a_broadcast)
505:        self.wavelength = torch.tensor(self.beam_config.wavelength_A, device=self.device, dtype=self.dtype)
639:            wavelength = self.wavelength
1025:            airpath = pixel_magnitudes.squeeze(-1)  # Remove last dimension for broadcasting
1314:                    # Following nanoBragg.c convention: h = dot(a, scattering)
1322:                    h = dot_product(scattering_vec.unsqueeze(0), a_vec.unsqueeze(0)).item()
[2025-10-07T03:23:46] tokens used: 80,961

[2025-10-07T03:23:49] thinking

**Investigating vector mismatch cause**
[2025-10-07T03:23:49] exec bash -lc "sed -n '150,240p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:23:50] bash -lc "sed -n '150,240p' src/nanobrag_torch/simulator.py" succeeded in 722ms:
                wavelength = wavelength.view(n_sources, 1, 1)
            else:
                # (S, F, 3) case: (n_sources,) -> (n_sources, 1, 1, 1)
                wavelength = wavelength.view(n_sources, 1, 1, 1)

    # Scattering vector using crystallographic convention
    scattering_vector = (diffracted_beam_unit - incident_beam_unit) / wavelength

    # Apply dmin culling if specified
    dmin_mask = None
    if dmin > 0:
        stol = 0.5 * torch.norm(scattering_vector, dim=-1)
        stol_threshold = 0.5 / dmin
        dmin_mask = (stol > 0) & (stol > stol_threshold)

    # Calculate Miller indices
    # scattering_vector shape: (S, F, 3) single source, or
    #                          (n_sources, S, F, 3) or (n_sources, batch, 3) multi-source
    # Need to add phi and mosaic dimensions for broadcasting
    if is_multi_source:
[2025-10-07T03:23:50] tokens used: 81,277

[2025-10-07T03:23:58] thinking

**Examining vector rotation methods**
[2025-10-07T03:23:59] exec bash -lc 'rg "def get_rotated_real_vectors" -n src/nanobrag_torch/models/crystal.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:00] bash -lc 'rg "def get_rotated_real_vectors" -n src/nanobrag_torch/models/crystal.py' succeeded in 752ms:
877:    def get_rotated_real_vectors(self, config: "CrystalConfig") -> Tuple[
[2025-10-07T03:24:00] tokens used: 83,188
[2025-10-07T03:24:04] exec bash -lc "sed -n '840,980p' src/nanobrag_torch/models/crystal.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:05] bash -lc "sed -n '840,980p' src/nanobrag_torch/models/crystal.py" succeeded in 714ms:
        return self._compute_cell_tensors_cached()["a"]

    @property
    def b(self) -> torch.Tensor:
        """Real-space lattice vector b (Angstroms)."""
        return self._compute_cell_tensors_cached()["b"]

    @property
    def c(self) -> torch.Tensor:
        """Real-space lattice vector c (Angstroms)."""
        return self._compute_cell_tensors_cached()["c"]

    @property
    def a_star(self) -> torch.Tensor:
        """Reciprocal-space lattice vector a* (Angstroms^-1)."""
        return self._compute_cell_tensors_cached()["a_star"]

    @property
    def b_star(self) -> torch.Tensor:
        """Reciprocal-space lattice vector b* (Angstroms^-1)."""
[2025-10-07T03:24:05] tokens used: 83,462

[2025-10-07T03:24:07] thinking

**Checking deeper in pipeline**
[2025-10-07T03:24:08] exec bash -lc "sed -n '980,1160p' src/nanobrag_torch/models/crystal.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:09] bash -lc "sed -n '980,1160p' src/nanobrag_torch/models/crystal.py" succeeded in 678ms:
        phi_angles = config.phi_start_deg + step_size * step_indices
        phi_rad = torch.deg2rad(phi_angles)

        # Convert spindle axis to tensor
        spindle_axis = torch.tensor(
            config.spindle_axis, device=self.device, dtype=self.dtype
        )

        # Apply spindle rotation to both real and reciprocal vectors
        # Shape: (N_phi, 3)
        a_phi = rotate_axis(self.a.unsqueeze(0), spindle_axis.unsqueeze(0), phi_rad)
        b_phi = rotate_axis(self.b.unsqueeze(0), spindle_axis.unsqueeze(0), phi_rad)
        c_phi = rotate_axis(self.c.unsqueeze(0), spindle_axis.unsqueeze(0), phi_rad)

        a_star_phi = rotate_axis(
            self.a_star.unsqueeze(0), spindle_axis.unsqueeze(0), phi_rad
        )
        b_star_phi = rotate_axis(
            self.b_star.unsqueeze(0), spindle_axis.unsqueeze(0), phi_rad
        )
[2025-10-07T03:24:09] tokens used: 85,268

[2025-10-07T03:24:15] thinking

**Formulating hypotheses for scaling mismatch**
[2025-10-07T03:24:16] exec bash -lc 'ls reports/2025-10-cli-flags/phase_l' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:17] bash -lc 'ls reports/2025-10-cli-flags/phase_l' succeeded in 711ms:
hkl_parity
scaling_audit
[2025-10-07T03:24:17] tokens used: 87,803

[2025-10-07T03:24:19] thinking

**Checking scaling_audit**
[2025-10-07T03:24:20] exec bash -lc 'ls reports/2025-10-cli-flags/phase_l/scaling_audit' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:20] bash -lc 'ls reports/2025-10-cli-flags/phase_l/scaling_audit' succeeded in 703ms:
L2b_blocker.md
c_trace_scaling.log
config_snapshot.json
config_snapshot_final.json
instrumentation_notes.md
notes.md
pytest_collect.log
pytest_collect_final.log
run_pytorch_trace.sh
scaling_audit_summary.md
trace_harness.py
trace_py_cli_full.log
trace_py_env.json
trace_py_fullrun.log
trace_py_scaling.log
trace_py_stderr.txt
trace_py_stderr_cli.txt
trace_py_stdout.txt
trace_py_stdout_cli.txt
[2025-10-07T03:24:20] tokens used: 88,051

[2025-10-07T03:24:22] thinking

**Checking trace_py_scaling.log**
[2025-10-07T03:24:23] exec bash -lc 'head -n 60 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:24] bash -lc 'head -n 60 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log' succeeded in 689ms:
TRACE_PY: pix0_vector_meters 0.231274664402008 0.217322006821632 -0.211817994713783
TRACE_PY: fdet_vec 0 0 1
TRACE_PY: sdet_vec 0 -1 0
TRACE_PY: pixel_pos_meters 0.231274664402008 0.099416010081768 -0.0330239981412888
TRACE_PY: R_distance_meters 0.253893882036209
TRACE_PY: omega_pixel_sr 4.18050007056081e-07
TRACE_PY: close_distance_meters 0.231274664402008
TRACE_PY: obliquity_factor 0.910910725593567
TRACE_PY: diffracted_vec 0.910910785198212 0.391565203666687 -0.130070075392723
TRACE_PY: incident_vec 0 0 1
TRACE_PY: lambda_meters 9.7680002450943e-11
TRACE_PY: lambda_angstroms 0.97680002450943
TRACE_PY: scattering_vec_A_inv 9325458432 4008652544 -11569103872
TRACE_PY: rot_a_angstroms -14.3562698364258 -21.8717937469482 -5.58202123641968
TRACE_PY: rot_b_angstroms -11.4986982345581 0.717319786548615 -29.1132164001465
TRACE_PY: rot_c_angstroms 21.0699501037598 -24.3892974853516 -9.75265312194824
TRACE_PY: rot_a_star_A_inv -0.0290510933846235 -0.0293958801776171 0.010749876499176
TRACE_PY: rot_b_star_A_inv -0.00312639237381518 0.0104376431554556 -0.0328566692769527
TRACE_PY: rot_c_star_A_inv 0.0259604379534721 -0.0143330143764615 -0.0106066130101681
TRACE_PY: hkl_frac -15.6976222991943 23.24587059021 21.1548194885254
[2025-10-07T03:24:24] tokens used: 88,383

[2025-10-07T03:24:32] thinking

**Analyzing unit mismatch in scattering vector calculations**
[2025-10-07T03:24:32] exec bash -lc 'head -n 60 reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:33] bash -lc 'head -n 60 reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 720ms:
WARNING: auto-generating odet_vector
nanoBragg nanocrystal diffraction simulator - James Holton and Ken Frankel 5-17-17
custom convention selected.
pivoting detector around sample
TRACE_C:detector_convention=CUSTOM
TRACE_C:angles_rad=rotx:0 roty:0 rotz:0 twotheta:0
TRACE_C:beam_center_m=Xclose:0.000211818 Yclose:0.000217322 pixel_mm:0.172
TRACE_C:initial_fdet=0.999982004873912 -0.00599800002923425 -0.000118000000575132
TRACE_C:initial_sdet=-0.0059979996566955 -0.999969942765222 -0.0049129997187971
TRACE_C:initial_odet=-8.85282813644494e-05 0.00491361907271126 -0.999987924182263
TRACE_C:Fclose_m=0.217742295
TRACE_C:Sclose_m=0.21390708
TRACE_C:close_distance_m=0.231271826111781
TRACE_C:ratio=0.999987746654913
TRACE_C:distance_m=0.23127466
TRACE_C:term_fast_before_rot=-0.217738376699947 0.00130601829177553 2.56935909352306e-05
TRACE_C:term_slow_before_rot=0.00128301459240474 0.213900650544676 0.00105092542388871
TRACE_C:term_close_before_rot=-2.04740972936938e-05 0.00113638165576361 -0.231269033315361
TRACE_C:pix0_before_rotation=-0.216475836204836 0.216343050492215 -0.230192414300537
TRACE_C:pix0_after_rotz=-0.216475836204836 0.216343050492215 -0.230192414300537
[2025-10-07T03:24:33] tokens used: 89,754

[2025-10-07T03:24:35] thinking

**Investigating C trace details**
[2025-10-07T03:24:37] exec bash -lc 'rg "h =" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:38] bash -lc 'rg "h =" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' exited 1 in 744ms:

[2025-10-07T03:24:38] tokens used: 91,481

[2025-10-07T03:24:40] thinking

**Searching for keyword "hkl"**
[2025-10-07T03:24:42] exec bash -lc 'rg "h_frac" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:43] bash -lc 'rg "h_frac" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' exited 1 in 732ms:

[2025-10-07T03:24:43] tokens used: 91,657

[2025-10-07T03:24:46] thinking

**Searching for SCATTERING lines**
[2025-10-07T03:24:46] exec bash -lc 'rg "scattering" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:47] bash -lc 'rg "scattering" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 716ms:
264:TRACE_C: scattering_vec_A_inv -1556230686.07047 3933478972.39855 913944567.685604
[2025-10-07T03:24:47] tokens used: 91,942

[2025-10-07T03:24:50] thinking

**Analyzing scattering vector magnitudes**
[2025-10-07T03:24:51] exec bash -lc 'rg "hkl_frac" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:24:51] bash -lc 'rg "hkl_frac" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 704ms:
TRACE_C: hkl_frac -6.8795206024501 -0.607255839576692 -13.766298525469
[2025-10-07T03:24:51] tokens used: 92,287

[2025-10-07T03:24:54] thinking

**Comparing k_frac values and investigating F_latt**
[2025-10-07T03:25:09] exec bash -lc 'rg "F_latt" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:10] bash -lc 'rg "F_latt" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 921ms:
TRACE_C: F_latt_a -2.36012735995387
TRACE_C: F_latt_b 1.05079664302326
TRACE_C: F_latt_c 0.960961003611676
TRACE_C: F_latt -2.38319665299058
TRACE_C_PHI phi_tic=0 phi_deg=0 k_frac=-0.607255839576692 F_latt_b=1.05079664302326 F_latt=-2.38319665299058
TRACE_C_PHI phi_tic=1 phi_deg=0.01 k_frac=-0.591177727127411 F_latt_b=-0.650897255741202 F_latt=1.09860741704307
TRACE_C_PHI phi_tic=2 phi_deg=0.02 k_frac=-0.593187745641004 F_latt_b=-0.385020463934971 F_latt=0.679724660042308
TRACE_C_PHI phi_tic=3 phi_deg=0.03 k_frac=-0.595197691574922 F_latt_b=-0.0844254727339029 F_latt=0.155482072719562
TRACE_C_PHI phi_tic=4 phi_deg=0.04 k_frac=-0.597207564867936 F_latt_b=0.224723979612609 F_latt=-0.430675460925897
TRACE_C_PHI phi_tic=5 phi_deg=0.05 k_frac=-0.599217365458823 F_latt_b=0.515419354922621 F_latt=-1.02559631818979
TRACE_C_PHI phi_tic=6 phi_deg=0.06 k_frac=-0.601227093286361 F_latt_b=0.762161832840823 F_latt=-1.57135079979647
TRACE_C_PHI phi_tic=7 phi_deg=0.07 k_frac=-0.60323674828933 F_latt_b=0.943193755044476 F_latt=-2.01093178282
TRACE_C_PHI phi_tic=8 phi_deg=0.08 k_frac=-0.605246330406512 F_latt_b=1.04241217237842 F_latt=-2.29416161853174
TRACE_C_PHI phi_tic=9 phi_deg=0.09 k_frac=-0.607255839576692 F_latt_b=1.05079664302326 F_latt=-2.38319665299058
[2025-10-07T03:25:10] tokens used: 92,515

[2025-10-07T03:25:16] thinking

**Investigating scattering vector discrepancy**
[2025-10-07T03:25:16] exec bash -lc 'rg "incident" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:17] bash -lc 'rg "incident" -n reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 703ms:
33:XDS incident beam: 0.00063187 0.00490992 0.999988
133:  incident fluence: 1e+24 photons/m^2
261:TRACE_C: incident_vec 0.000513879494092498 -0 -0.999999867963924
[2025-10-07T03:25:17] tokens used: 93,497

[2025-10-07T03:25:21] thinking

**Investigating incident beam vector source**
[2025-10-07T03:25:22] exec bash -lc 'rg "incident_beam_unit" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:23] bash -lc 'rg "incident_beam_unit" -n src/nanobrag_torch/simulator.py' succeeded in 726ms:
138:        incident_beam_unit = incident_expanded.expand(n_sources, *diffracted_beam_unit.shape)
139:        diffracted_beam_unit = diffracted_expanded.expand_as(incident_beam_unit)
142:        incident_beam_unit = incident_beam_direction.expand_as(diffracted_beam_unit)
156:    scattering_vector = (diffracted_beam_unit - incident_beam_unit) / wavelength
[2025-10-07T03:25:23] tokens used: 93,864

[2025-10-07T03:25:25] thinking

**Tracing incident_beam_direction definition**
[2025-10-07T03:25:26] exec bash -lc 'rg "incident_beam_direction" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:27] bash -lc 'rg "incident_beam_direction" -n src/nanobrag_torch/simulator.py' succeeded in 705ms:
29:    incident_beam_direction: torch.Tensor,
77:        incident_beam_direction: Incident beam direction.
100:    is_multi_source = incident_beam_direction.dim() == 2
101:    n_sources = incident_beam_direction.shape[0] if is_multi_source else 1
116:    # PERF-PYTORCH-004 Phase 1: No .to() call needed - incident_beam_direction is already on correct device
123:        # incident_beam_direction: (n_sources, 3)
132:            incident_expanded = incident_beam_direction.view(n_sources, 1, 3)
135:            incident_expanded = incident_beam_direction.view(n_sources, 1, 1, 3)
142:        incident_beam_unit = incident_beam_direction.expand_as(diffracted_beam_unit)
336:            # incident_beam_direction: (n_sources, 3)
337:            # NOTE: incident_beam_direction is cloned in _compute_physics_for_position wrapper
344:                incident_expanded = incident_beam_direction.unsqueeze(1).expand(-1, diffracted_beam_unit.shape[0], -1)
348:                incident_expanded = incident_beam_direction.unsqueeze(1).unsqueeze(1).expand(-1, diffracted_beam_unit.shape[0], diffracted_beam_unit.shape[1], -1)
377:                incident_flat = incident_beam_direction.unsqueeze(0).unsqueeze(0).expand(diffracted_beam_unit.shape[0], diffracted_beam_unit.shape[1], -1).reshape(-1, 3).contiguous()
379:                incident_flat = incident_beam_direction.unsqueeze(0).expand(diffracted_beam_unit.shape[0], -1).reshape(-1, 3).contiguous()
498:            self.incident_beam_direction = self.detector.beam_vector.clone()
501:            self.incident_beam_direction = torch.tensor(
589:        # because it needs to clone incident_beam_direction before passing to the
614:    def _compute_physics_for_position(self, pixel_coords_angstroms, rot_a, rot_b, rot_c, rot_a_star, rot_b_star, rot_c_star, incident_beam_direction=None, wavelength=None, source_weights=None):
628:            incident_beam_direction: Optional incident beam direction (defaults to self.incident_beam_direction)
[2025-10-07T03:25:27] tokens used: 94,202

[2025-10-07T03:25:31] thinking

**Verifying beam_vector usage in Detector**
[2025-10-07T03:25:31] exec bash -lc 'rg "beam_vector" -n src/nanobrag_torch/models/detector.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:32] bash -lc 'rg "beam_vector" -n src/nanobrag_torch/models/detector.py' succeeded in 721ms:
351:        ratio = dot_product(beam_vector,vector);
375:            pix0_vector[1] = -Fbeam*fdet_vector[1]-Sbeam*sdet_vector[1]+distance*beam_vector[1];
376:            pix0_vector[2] = -Fbeam*fdet_vector[2]-Sbeam*sdet_vector[2]+distance*beam_vector[2];
377:            pix0_vector[3] = -Fbeam*fdet_vector[3]-Sbeam*sdet_vector[3]+distance*beam_vector[3];
433:        # Get beam vector using self.beam_vector property
434:        # This honors CUSTOM convention overrides from CLI (e.g., -beam_vector)
435:        beam_vector = self.beam_vector
446:            ratio = torch.dot(beam_vector, odet_rotated)
517:            # Reuse beam_vector from r-factor calculation above
518:            # This ensures CUSTOM convention overrides (e.g., -beam_vector) are honored
519:            # beam_vector is already set via self.beam_vector property
544:                beam_vector_local = beam_vector.to(device=self.device, dtype=self.dtype)
549:                # Compute beam term: distance_corrected * beam_vector
550:                beam_term = self.distance_corrected * beam_vector_local
579:            # pix0_vector[1] = -Fbeam*fdet_vector[1]-Sbeam*sdet_vector[1]+distance*beam_vector[1];
580:            # pix0_vector[2] = -Fbeam*fdet_vector[2]-Sbeam*sdet_vector[2]+distance*beam_vector[2];
581:            # pix0_vector[3] = -Fbeam*fdet_vector[3]-Sbeam*sdet_vector[3]+distance*beam_vector[3];
586:                + self.distance_corrected * beam_vector
699:        # newvector[1] = close_distance/ratio*beam_vector[1]-pix0_vector[1];
700:        # newvector[2] = close_distance/ratio*beam_vector[2]-pix0_vector[2];
[2025-10-07T03:25:32] tokens used: 95,134

[2025-10-07T03:25:34] thinking

**Checking beam_vector property usage**
[2025-10-07T03:25:35] exec bash -lc "sed -n '920,1000p' src/nanobrag_torch/models/detector.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:36] bash -lc "sed -n '920,1000p' src/nanobrag_torch/models/detector.py" succeeded in 683ms:
        Returns:
            torch.Tensor: Solid angle factor for each pixel, shape (spixels, fpixels)
        """
        # Get pixel coordinates if not provided
        if pixel_coords is None:
            pixel_coords = self.get_pixel_coords()

        # Calculate distance from sample to each pixel
        R = torch.norm(pixel_coords, dim=-1)  # Shape: (spixels, fpixels)

        if self.config.point_pixel:
            # Point pixel mode: use 1/R^2 solid angle only (no obliquity)
            omega = 1.0 / (R * R)
        else:
            # Default mode: include pixel size and obliquity factor
            # Ω = (pixel_size^2 / R^2) · (close_distance/R)
            # where close_distance is the minimum distance along detector normal

            # The obliquity factor is close_distance/R
            # For the standard case, close_distance = distance * cos(angle)
[2025-10-07T03:25:36] tokens used: 96,282

[2025-10-07T03:25:41] thinking

**Searching for beam vector CLI option**
[2025-10-07T03:25:42] exec bash -lc 'rg "beam_vector" src/nanobrag_torch/__main__.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:43] bash -lc 'rg "beam_vector" src/nanobrag_torch/__main__.py' succeeded in 728ms:
    parser.add_argument('-beam_vector', nargs=3, type=float,
            args.beam_vector, args.polar_vector, args.spindle_axis,
    if args.beam_vector:
        config['custom_beam_vector'] = tuple(args.beam_vector)
            custom_beam_vector=config.get('custom_beam_vector'),
[2025-10-07T03:25:43] tokens used: 97,270

[2025-10-07T03:25:49] thinking

**Examining beam vector assignment discrepancy**
[2025-10-07T03:25:49] exec bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:50] bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot.json' succeeded in 761ms:
{
  "crystal": {
    "N_cells": [
      36,
      47,
      29
    ],
    "phi_start_deg": 0.0,
    "osc_range_deg": 0.1,
    "phi_steps": 10,
    "mosaic_spread_deg": 0.0,
    "mosaic_domains": 1,
    "spindle_axis": [
      -1.0,
      0.0,
      0.0
    ]
  },
  "detector": {
    "distance_mm": 231.27466,
[2025-10-07T03:25:50] tokens used: 97,757

[2025-10-07T03:25:56] thinking

**Investigating missing custom beam vector**
[2025-10-07T03:25:56] exec bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot_final.json' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:25:57] bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/config_snapshot_final.json' succeeded in 721ms:
{
  "crystal": {
    "N_cells": [36, 47, 29],
    "phi_start_deg": 0.0,
    "osc_range_deg": 0.1,
    "phi_steps": 10,
    "mosaic_spread_deg": 0.0,
    "mosaic_domains": 1,
    "spindle_axis": [-1.0, 0.0, 0.0]
  },
  "detector": {
    "distance_mm": 231.27466,
    "pixel_size_mm": 0.172,
    "spixels": 2527,
    "fpixels": 2463,
    "Xclose_mm": 211.818,
    "Yclose_mm": 217.322,
    "oversample": 4
  },
  "beam": {
[2025-10-07T03:25:57] tokens used: 98,494

[2025-10-07T03:26:02] thinking

**Investigating custom beam vector handling**
[2025-10-07T03:26:02] exec bash -lc 'rg "custom_beam_vector" -n src/nanobrag_torch/config.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:03] bash -lc 'rg "custom_beam_vector" -n src/nanobrag_torch/config.py' succeeded in 685ms:
220:    custom_beam_vector: Optional[Tuple[float, float, float]] = None  # Incident beam direction (unit vector)
262:            or self.custom_beam_vector is not None
[2025-10-07T03:26:03] tokens used: 99,080

[2025-10-07T03:26:05] thinking

**Checking DetectorConfig dataclass**
[2025-10-07T03:26:06] exec bash -lc "sed -n '200,300p' src/nanobrag_torch/config.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:06] bash -lc "sed -n '200,300p' src/nanobrag_torch/config.py" succeeded in 706ms:
    detector_convention: DetectorConvention = DetectorConvention.MOSFLM
    detector_pivot: Optional[DetectorPivot] = None  # Will be auto-selected per AT-GEO-002

    # Sampling
    oversample: int = -1  # -1 means auto-select based on crystal size and detector geometry
    oversample_omega: bool = False  # If True, apply solid angle per subpixel (not last-value)
    oversample_polar: bool = False  # If True, apply polarization per subpixel (not last-value)
    oversample_thick: bool = False  # If True, apply absorption per subpixel (not last-value)

    # Detector geometry mode
    curved_detector: bool = False  # If True, use spherical mapping for pixel positions
    point_pixel: bool = False  # If True, use 1/R^2 solid angle only (no obliquity)

    # Detector origin override (CLI-FLAGS-003)
    pix0_override_m: Optional[Union[Tuple[float, float, float], torch.Tensor]] = None  # Override pix0 vector (meters)

    # Custom basis vectors for CUSTOM convention (unit vectors)
    custom_fdet_vector: Optional[Tuple[float, float, float]] = None  # Fast axis direction
    custom_sdet_vector: Optional[Tuple[float, float, float]] = None  # Slow axis direction
    custom_odet_vector: Optional[Tuple[float, float, float]] = None  # Normal direction
[2025-10-07T03:26:06] tokens used: 99,302

[2025-10-07T03:26:09] thinking

**Checking beam vector assignment**
[2025-10-07T03:26:10] exec bash -lc "sed -n '460,520p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:10] bash -lc "sed -n '460,520p' src/nanobrag_torch/simulator.py" succeeded in 701ms:
            if hasattr(crystal_config, 'mosaic_spread_deg'):
                self.crystal.config.mosaic_spread_deg = crystal_config.mosaic_spread_deg
            if hasattr(crystal_config, 'mosaic_domains'):
                self.crystal.config.mosaic_domains = crystal_config.mosaic_domains
            if hasattr(crystal_config, 'mosaic_seed'):
                self.crystal.config.mosaic_seed = crystal_config.mosaic_seed
            if hasattr(crystal_config, 'spindle_axis'):
                self.crystal.config.spindle_axis = crystal_config.spindle_axis
        # Use the provided beam_config, or Crystal's beam_config, or default
        if beam_config is not None:
            self.beam_config = beam_config
        elif hasattr(crystal, 'beam_config') and crystal.beam_config is not None:
            self.beam_config = crystal.beam_config
        else:
            self.beam_config = BeamConfig()
        # Normalize device to ensure consistency
        if device is not None:
            # Create a dummy tensor on the device to get the actual device with index
            temp = torch.zeros(1, device=device)
            self.device = temp.device
[2025-10-07T03:26:10] tokens used: 101,241

[2025-10-07T03:26:15] thinking

**Investigating DetectorConfig usage**
[2025-10-07T03:26:16] exec bash -lc 'rg "DetectorConfig(" -n src/nanobrag_torch/__main__.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:17] bash -lc 'rg "DetectorConfig(" -n src/nanobrag_torch/__main__.py' exited 2 in 717ms:
rg: regex parse error:
    (?:DetectorConfig()
    ^
error: unclosed group
[2025-10-07T03:26:17] tokens used: 103,237

[2025-10-07T03:26:19] thinking

**Searching for DetectorConfig references**
[2025-10-07T03:26:21] exec bash -lc 'rg "DetectorConfig" src/nanobrag_torch/__main__.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:22] bash -lc 'rg "DetectorConfig" src/nanobrag_torch/__main__.py' succeeded in 670ms:
    DetectorConfig, CrystalConfig, BeamConfig, NoiseConfig,
        detector_config: DetectorConfig object
        detector_config = DetectorConfig(
                # NOTE: DetectorConfig defaults are already set to detector center
[2025-10-07T03:26:22] tokens used: 104,361

[2025-10-07T03:26:25] thinking

**Checking surrounding context**
[2025-10-07T03:26:26] exec bash -lc "sed -n '880,1040p' src/nanobrag_torch/__main__.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:27] bash -lc "sed -n '880,1040p' src/nanobrag_torch/__main__.py" succeeded in 674ms:
            # Custom vectors for CUSTOM convention
            custom_fdet_vector=config.get('custom_fdet_vector'),
            custom_sdet_vector=config.get('custom_sdet_vector'),
            custom_odet_vector=config.get('custom_odet_vector'),
            custom_beam_vector=config.get('custom_beam_vector'),
            # Detector origin override (CLI-FLAGS-003)
            pix0_override_m=config.get('pix0_override_m')
        )

        # Set beam center if provided (values are in mm)
        # CRITICAL: C-code Xbeam/Ybeam semantics are convention AND pivot-mode dependent! (AT-PARALLEL-004 root cause)
        #
        # C-code behavior (nanoBragg.c lines 631-648, 1206-1275):
        #   - `-Xbeam`/`-Ybeam` set detector_pivot = BEAM (line 632, 637)
        #   - `-Xclose`/`-Yclose` set detector_pivot = SAMPLE (line 642, 647)
        #   - Convention selection OVERRIDES pivot: XDS/DIALS force SAMPLE pivot (lines 1250, 1265)
        #   - For SAMPLE pivot: Xbeam/Ybeam are IGNORED; C uses detector center (Fclose=detsize/2)
        #   - For BEAM pivot: Xbeam/Ybeam are mapped to Fbeam/Sbeam with convention-specific axis swaps
        #
        # Result: `-xds -Xbeam X -Ybeam Y` is contradictory; C resolves by using SAMPLE pivot
[2025-10-07T03:26:27] tokens used: 104,612

[2025-10-07T03:26:31] thinking

**Verifying custom_beam_vector handling**
[2025-10-07T03:26:32] exec bash -lc 'rg "custom_beam_vector" -n src/nanobrag_torch/__main__.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:33] bash -lc 'rg "custom_beam_vector" -n src/nanobrag_torch/__main__.py' succeeded in 738ms:
551:        config['custom_beam_vector'] = tuple(args.beam_vector)
884:            custom_beam_vector=config.get('custom_beam_vector'),
[2025-10-07T03:26:33] tokens used: 107,054

[2025-10-07T03:26:35] thinking

**Checking argument assignment near line 551**
[2025-10-07T03:26:36] exec bash -lc "sed -n '520,580p' src/nanobrag_torch/__main__.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:37] bash -lc "sed -n '520,580p' src/nanobrag_torch/__main__.py" succeeded in 692ms:
        config['fpixels'] = 1024
        config['spixels'] = 1024

    config['distance_mm'] = args.distance if args.distance else 100.0
    config['close_distance_mm'] = args.close_distance

    # Beam centers
    if args.Xbeam is not None:
        config['beam_center_x_mm'] = args.Xbeam
    if args.Ybeam is not None:
        config['beam_center_y_mm'] = args.Ybeam

    # Detector rotations
    config['detector_rotx_deg'] = args.detector_rotx
    config['detector_roty_deg'] = args.detector_roty
    config['detector_rotz_deg'] = args.detector_rotz
    config['twotheta_deg'] = args.twotheta if args.twotheta is not None else 0.0
    if args.twotheta_axis:
        config['twotheta_axis'] = args.twotheta_axis

[2025-10-07T03:26:37] tokens used: 107,350

[2025-10-07T03:26:42] thinking

**Investigating beam vector propagation**
[2025-10-07T03:26:43] exec bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:44] bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json' succeeded in 748ms:
{
  "python_version": "3.13",
  "pytorch_version": "2.x",
  "device": "cpu",
  "dtype": "torch.float32",
  "command": "python -m nanobrag_torch -mat A.mat -hkl scaled.hkl -lambda 0.9768 -distance 231.27466 -pixel 0.172 -detpixels_s 2527 -detpixels_f 2463 -Xclose 211.818 -Yclose 217.322 -phi 0.0 -osc 0.1 -phisteps 10 -Na 36 -Nb 47 -Nc 29 -mosaic 0.0 -mosaic_dom 1 -flux 1e18 -exposure 1.0 -beamsize 1.0 -spindle_axis -1 0 0 -trace_pixel 685 1039",
  "timestamp": "2025-10-17",
  "git_sha": "3f376d6"
}
[2025-10-07T03:26:44] tokens used: 108,238

[2025-10-07T03:26:49] thinking

**Examining missing beam_vector argument**
[2025-10-07T03:26:49] exec bash -lc 'rg "beam_vector" tests/test_cli_flags.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:50] bash -lc 'rg "beam_vector" tests/test_cli_flags.py' succeeded in 764ms:
    def test_pix0_does_not_alter_beam_vector(self):
        """Verify -pix0_vector doesn't mutate beam_vector."""
        # custom_beam_vector should remain unset
        assert config.get('custom_beam_vector') is None
    """Test -beam_vector CLI flag propagation through Detector→Simulator."""
    def test_custom_beam_vector_propagates(self):
        to simulator so CLI `-beam_vector` overrides reach the physics kernels.
            custom_beam_vector=custom_beam_raw
        beam_vec = detector.beam_vector
            f"Detector beam_vector {beam_vec} != expected {expected_norm}"
        # CRITICAL: Simulator.incident_beam_direction must match detector.beam_vector
            f"Simulator incident_beam_direction {incident} != detector beam_vector {expected_norm}"
                custom_beam_vector=(0.00051387949, 0.0, -0.99999986),
            custom_beam_vector=(0.00051387949, 0.0, -0.99999986)
[2025-10-07T03:26:50] tokens used: 108,877

[2025-10-07T03:26:52] thinking

**Viewing specific test function**
[2025-10-07T03:26:53] exec bash -lc "sed -n '600,720p' tests/test_cli_flags.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:54] bash -lc "sed -n '600,720p' tests/test_cli_flags.py" succeeded in 683ms:
            int nopolar = 0;
            # Per-pixel: polarization = 0.0; triggers dynamic Kahn factor computation

        CRITICAL UPDATE (CLI-FLAGS-003 Phase K3b):
        C code initializes polar=1.0 but resets polarization=0.0 per pixel (nanoBragg.c:3732),
        which triggers dynamic Kahn factor computation (≈0.9126) based on pixel geometry.
        PyTorch must default polarization_factor=0.0 to match this behavior.

        Evidence: reports/2025-10-cli-flags/phase_k/f_latt_fix/scaling_chain.md
            Shows C computing polar≈0.9126 dynamically, not using the initial 1.0 value
            Previous Phase E analysis was incomplete - it missed the per-pixel reset
        """
        # Parse minimal command with no explicit polarization flags
        config = run_parse([
            '-cell', '100', '100', '100', '90', '90', '90',
            '-default_F', '100',
            '-lambda', '6.2',
            '-distance', '100',
            '-detpixels', '128'
        ])
[2025-10-07T03:26:54] tokens used: 109,332

[2025-10-07T03:26:56] thinking

**Searching for beam vector test**
[2025-10-07T03:26:57] exec bash -lc 'rg "custom_beam_vector_propagates" -n tests/test_cli_flags.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:26:58] bash -lc 'rg "custom_beam_vector_propagates" -n tests/test_cli_flags.py' succeeded in 731ms:
310:    def test_custom_beam_vector_propagates(self):
[2025-10-07T03:26:58] tokens used: 110,904
[2025-10-07T03:27:01] exec bash -lc "sed -n '280,360p' tests/test_cli_flags.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:27:01] bash -lc "sed -n '280,360p' tests/test_cli_flags.py" succeeded in 658ms:
        ])

        # Should switch to CUSTOM when custom vectors are provided
        assert config['convention'] == 'CUSTOM'

    def test_roi_unaffected_by_new_flags(self):
        """Verify ROI defaults remain unchanged."""
        config = run_parse([
            '-cell', '100', '100', '100', '90', '90', '90',
            '-pixel', '0.1',
            '-pix0_vector', '0', '0', '0',
            '-nonoise'
        ])

        assert config.get('roi') is None

    def test_convention_preserved_without_pix0(self):
        """Verify MOSFLM convention remains when pix0 not specified."""
        config = run_parse([
            '-cell', '100', '100', '100', '90', '90', '90',
[2025-10-07T03:27:01] tokens used: 111,178
[2025-10-07T03:27:06] exec bash -lc "sed -n '360,440p' tests/test_cli_flags.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:27:06] bash -lc "sed -n '360,440p' tests/test_cli_flags.py" succeeded in 679ms:

        # Construct simulator and verify it picks up detector's beam vector
        simulator = Simulator(
            crystal=crystal,
            detector=detector,
            crystal_config=crystal_config,
            beam_config=beam_config
        )

        # CRITICAL: Simulator.incident_beam_direction must match detector.beam_vector
        incident = simulator.incident_beam_direction
        assert torch.allclose(incident, expected_norm, atol=1e-6), \
            f"Simulator incident_beam_direction {incident} != detector beam_vector {expected_norm}"


class TestCLIPix0Override:
    """
    Regression tests for CLI-FLAGS-003 Phase H3b.

    Test the BEAM-pivot pix0 transformation when -pix0_vector_mm override is provided.
[2025-10-07T03:27:06] tokens used: 112,205

[2025-10-07T03:27:13] thinking

**Analyzing beam vector issues in pyro trace**
[2025-10-07T03:27:14] exec bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:27:14] bash -lc 'cat reports/2025-10-cli-flags/phase_l/scaling_audit/scaling_audit_summary.md' succeeded in 725ms:
# Phase L2b Scaling Audit Summary

**Date:** 2025-10-17
**Engineer:** Ralph (Loop i=64, evidence-only mode)
**Plan Reference:** `plans/active/cli-noise-pix0/plan.md` Phase L2b
**Task:** Compare C vs PyTorch scaling chain for pixel (685, 1039)

---

## Executive Summary

✅ **Phase L2b COMPLETE:** PyTorch scaling trace successfully captured via CLI `-trace_pixel` flag.

**Key Finding:** Three major scaling factor divergences identified:
1. **Steps:** PyTorch=160 vs C=10 (16× factor from auto-oversample 4²)
2. **Fluence:** PyTorch=1.26e+29 vs C=1e+24 (~100,000× error)
3. **Polarization:** PyTorch=1.0 vs C=0.9146 (9.3% error)

**First Divergence:** Fluence calculation (5 orders of magnitude)

[2025-10-07T03:27:14] tokens used: 113,531

[2025-10-07T03:27:21] thinking

**Verifying beam vector inclusion in commands**

I’m focusing on confirming whether the long-term pyro command properly includes the custom beam vector since the earlier plan's command lacked it. To be sure, I'll parse the command using the script’s parse function without running heavy computations—just a lightweight Python parse—to check if the `custom_beam_vector` is correctly recognized. This should help me verify how the instrumentation handles the beam vector parameter.
[2025-10-07T03:27:24] exec bash -lc "python - <<'PY'
from nanobrag_torch.__main__ import parse_and_validate_args
args = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-oversample','1','-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
config = parse_and_validate_args(args)
print('custom_beam_vector', config.get('custom_beam_vector'))
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:27:25] bash -lc "python - <<'PY'
from nanobrag_torch.__main__ import parse_and_validate_args
args = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-oversample','1','-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
config = parse_and_validate_args(args)
print('custom_beam_vector', config.get('custom_beam_vector'))
PY
" exited 134 in 1.03s:
OMP: Error #15: Initializing libomp.dylib, but found libomp.dylib already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/
bash: line 15: 40191 Abort trap: 6              python - <<'PY'
from nanobrag_torch.__main__ import parse_and_validate_args
args = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-oversample','1','-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
config = parse_and_validate_args(args)
print('custom_beam_vector', config.get('custom_beam_vector'))
PY

[2025-10-07T03:27:25] tokens used: 116,099

[2025-10-07T03:27:32] thinking

**Setting environment variable to fix OMP error**
[2025-10-07T03:27:40] exec bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.__main__ import parse_and_validate_args
args = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-oversample','1','-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
config = parse_and_validate_args(args)
print('custom_beam_vector', config.get('custom_beam_vector'))
print('oversample', config.get('oversample'))
print('fluence', config.get('fluence'))
print('flux', config.get('flux'))
print('exposure', config.get('exposure'))
print('beamsize_mm', config.get('beamsize_mm'))
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:27:43] bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.__main__ import parse_and_validate_args
args = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-oversample','1','-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
config = parse_and_validate_args(args)
print('custom_beam_vector', config.get('custom_beam_vector'))
print('oversample', config.get('oversample'))
print('fluence', config.get('fluence'))
print('flux', config.get('flux'))
print('exposure', config.get('exposure'))
print('beamsize_mm', config.get('beamsize_mm'))
PY
" exited 1 in 3.03s:
Traceback (most recent call last):
  File "<stdin>", line 12, in <module>
  File "/Users/ollie/Documents/nanoBragg3/src/nanobrag_torch/__main__.py", line 399, in parse_and_validate_args
    has_hkl = args.hkl is not None or Path('Fdump.bin').exists()
              ^^^^^^^^
AttributeError: 'list' object has no attribute 'hkl'
[2025-10-07T03:27:43] tokens used: 117,718

[2025-10-07T03:27:46] thinking

**Checking argument parsing implementation**
[2025-10-07T03:27:47] exec bash -lc 'rg "ArgumentParser" -n src/nanobrag_torch/__main__.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:27:47] bash -lc 'rg "ArgumentParser" -n src/nanobrag_torch/__main__.py' succeeded in 716ms:
60:def create_parser() -> argparse.ArgumentParser:
63:    parser = argparse.ArgumentParser(
[2025-10-07T03:27:47] tokens used: 118,537

[2025-10-07T03:27:52] thinking

**Preparing to parse list**
[2025-10-07T03:27:56] exec bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.__main__ import create_parser, parse_and_validate_args
args_list = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-oversample','1','-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
parser = create_parser()
args = parser.parse_args(args_list)
config = parse_and_validate_args(args)
print('custom_beam_vector', config.get('custom_beam_vector'))
print('oversample', config.get('oversample'))
print('flux', config.get('flux'))
print('fluence', config.get('fluence'))
print('beamsize_mm', config.get('beamsize_mm'))
print('suppress_noise', config.get('suppress_noise'))
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:27:59] bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.__main__ import create_parser, parse_and_validate_args
args_list = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-oversample','1','-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
parser = create_parser()
args = parser.parse_args(args_list)
config = parse_and_validate_args(args)
print('custom_beam_vector', config.get('custom_beam_vector'))
print('oversample', config.get('oversample'))
print('flux', config.get('flux'))
print('fluence', config.get('fluence'))
print('beamsize_mm', config.get('beamsize_mm'))
print('suppress_noise', config.get('suppress_noise'))
PY
" succeeded in 2.98s:
custom_beam_vector (0.00051387949, 0.0, -0.99999986)
oversample 1
flux 1e+18
fluence None
beamsize_mm 1.0
suppress_noise True
[2025-10-07T03:27:59] tokens used: 119,215

[2025-10-07T03:28:06] thinking

**Investigating oversample configuration usage**
[2025-10-07T03:28:08] exec bash -lc 'rg "self.detector.config.oversample" -n src/nanobrag_torch/simulator.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:28:09] bash -lc 'rg "self.detector.config.oversample" -n src/nanobrag_torch/simulator.py' succeeded in 683ms:
747:            oversample = self.detector.config.oversample
749:            oversample_omega = self.detector.config.oversample_omega
751:            oversample_polar = self.detector.config.oversample_polar
753:            oversample_thick = self.detector.config.oversample_thick
1378:                    print(f"TRACE_PY: oversample_thick {1 if self.detector.config.oversample_thick else 0}")
1379:                    print(f"TRACE_PY: oversample_polar {1 if self.detector.config.oversample_polar else 0}")
1380:                    print(f"TRACE_PY: oversample_omega {1 if self.detector.config.oversample_omega else 0}")
[2025-10-07T03:28:09] tokens used: 120,253

[2025-10-07T03:28:11] thinking

**Examining code near line 740**
[2025-10-07T03:28:12] exec bash -lc "sed -n '720,820p' src/nanobrag_torch/simulator.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:28:13] bash -lc "sed -n '720,820p' src/nanobrag_torch/simulator.py" succeeded in 670ms:
                                /* enumerate mosaic domains */
                                for(mos_tic=0;mos_tic<mosaic_domains;++mos_tic)
                                {
                                    /* ... mosaic rotation ... */
                                    /* ... h,k,l calculation ... */
                                    /* ... F_cell and F_latt calculation ... */

                                    /* convert amplitudes into intensity (photons per steradian) */
                                    I += F_cell*F_cell*F_latt*F_latt;
                                }
                            }
                        }
        ```

        Args:
            pixel_batch_size: Optional batching for memory management.
            override_a_star: Optional override for the a_star vector for testing.
            oversample: Number of subpixel samples per axis. Defaults to detector config.
            oversample_omega: Apply solid angle per subpixel. Defaults to detector config.
            oversample_polar: Apply polarization per subpixel. Defaults to detector config.
[2025-10-07T03:28:13] tokens used: 120,682

[2025-10-07T03:28:18] thinking

**Verifying beam vector configuration**
[2025-10-07T03:28:23] exec bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.config import DetectorConfig, DetectorConvention
from nanobrag_torch.models.detector import Detector
from nanobrag_torch.config import CrystalConfig, BeamConfig
from nanobrag_torch.models.crystal import Crystal
from nanobrag_torch.simulator import Simulator
import torch

# Build detector config similar to command (just not full data)
detector_config = DetectorConfig(
    distance_mm=231.27466,
    pixel_size_mm=0.172,
    spixels=2527,
    fpixels=2463,
    beam_center_f=217.742295,
    beam_center_s=213.907080,
    detector_convention=DetectorConvention.CUSTOM,
    custom_fdet_vector=(-0.000088, 0.004914, -0.999988),
    custom_sdet_vector=(-0.005998, -0.999970, -0.004913),
    custom_odet_vector=(0.999982, -0.005998, -0.000118),
    custom_beam_vector=(0.00051387949, 0.0, -0.99999986),
    oversample=1
)

detector = Detector(detector_config)
print('detector beam vector', detector.beam_vector)

crystal_config = CrystalConfig(cell_a=100, cell_b=100, cell_c=100,
                               cell_alpha=90, cell_beta=90, cell_gamma=90,
                               default_F=100, N_cells=(36,47,29))
crystal = Crystal(crystal_config)
beam_config = BeamConfig(wavelength_A=0.9768, flux=1e18, exposure=1.0, beam_size_mm=1.0)

sim = Simulator(crystal=crystal, detector=detector, beam_config=beam_config,
                crystal_config=crystal_config)
print('incident beam direction', sim.incident_beam_direction)
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:28:26] bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.config import DetectorConfig, DetectorConvention
from nanobrag_torch.models.detector import Detector
from nanobrag_torch.config import CrystalConfig, BeamConfig
from nanobrag_torch.models.crystal import Crystal
from nanobrag_torch.simulator import Simulator
import torch

# Build detector config similar to command (just not full data)
detector_config = DetectorConfig(
    distance_mm=231.27466,
    pixel_size_mm=0.172,
    spixels=2527,
    fpixels=2463,
    beam_center_f=217.742295,
    beam_center_s=213.907080,
    detector_convention=DetectorConvention.CUSTOM,
    custom_fdet_vector=(-0.000088, 0.004914, -0.999988),
    custom_sdet_vector=(-0.005998, -0.999970, -0.004913),
    custom_odet_vector=(0.999982, -0.005998, -0.000118),
    custom_beam_vector=(0.00051387949, 0.0, -0.99999986),
    oversample=1
)

detector = Detector(detector_config)
print('detector beam vector', detector.beam_vector)

crystal_config = CrystalConfig(cell_a=100, cell_b=100, cell_c=100,
                               cell_alpha=90, cell_beta=90, cell_gamma=90,
                               default_F=100, N_cells=(36,47,29))
crystal = Crystal(crystal_config)
beam_config = BeamConfig(wavelength_A=0.9768, flux=1e18, exposure=1.0, beam_size_mm=1.0)

sim = Simulator(crystal=crystal, detector=detector, beam_config=beam_config,
                crystal_config=crystal_config)
print('incident beam direction', sim.incident_beam_direction)
PY
" exited 1 in 2.52s:
detector beam vector tensor([ 5.1388e-04,  0.0000e+00, -1.0000e+00])
Traceback (most recent call last):
  File "<stdin>", line 31, in <module>
TypeError: BeamConfig.__init__() got an unexpected keyword argument 'beam_size_mm'. Did you mean 'beamsize_mm'?
[2025-10-07T03:28:26] tokens used: 122,780

[2025-10-07T03:28:28] thinking

**Retrying with correct parameter**
[2025-10-07T03:28:32] exec bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.config import DetectorConfig, DetectorConvention, CrystalConfig, BeamConfig
from nanobrag_torch.models.detector import Detector
from nanobrag_torch.models.crystal import Crystal
from nanobrag_torch.simulator import Simulator
import torch

detector_config = DetectorConfig(
    distance_mm=231.27466,
    pixel_size_mm=0.172,
    spixels=2527,
    fpixels=2463,
    beam_center_f=217.742295,
    beam_center_s=213.907080,
    detector_convention=DetectorConvention.CUSTOM,
    custom_fdet_vector=(-0.000088, 0.004914, -0.999988),
    custom_sdet_vector=(-0.005998, -0.999970, -0.004913),
    custom_odet_vector=(0.999982, -0.005998, -0.000118),
    custom_beam_vector=(0.00051387949, 0.0, -0.99999986),
    oversample=1
)

detector = Detector(detector_config)
print('detector beam vector', detector.beam_vector)

crystal_config = CrystalConfig(cell_a=100, cell_b=100, cell_c=100,
                               cell_alpha=90, cell_beta=90, cell_gamma=90,
                               default_F=100, N_cells=(36,47,29))
crystal = Crystal(crystal_config)
beam_config = BeamConfig(wavelength_A=0.9768, flux=1e18, exposure=1.0, beamsize_mm=1.0)

sim = Simulator(crystal=crystal, detector=detector, beam_config=beam_config,
                crystal_config=crystal_config)
print('incident beam direction', sim.incident_beam_direction)
print('fluence', sim.fluence)
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:28:34] bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.config import DetectorConfig, DetectorConvention, CrystalConfig, BeamConfig
from nanobrag_torch.models.detector import Detector
from nanobrag_torch.models.crystal import Crystal
from nanobrag_torch.simulator import Simulator
import torch

detector_config = DetectorConfig(
    distance_mm=231.27466,
    pixel_size_mm=0.172,
    spixels=2527,
    fpixels=2463,
    beam_center_f=217.742295,
    beam_center_s=213.907080,
    detector_convention=DetectorConvention.CUSTOM,
    custom_fdet_vector=(-0.000088, 0.004914, -0.999988),
    custom_sdet_vector=(-0.005998, -0.999970, -0.004913),
    custom_odet_vector=(0.999982, -0.005998, -0.000118),
    custom_beam_vector=(0.00051387949, 0.0, -0.99999986),
    oversample=1
)

detector = Detector(detector_config)
print('detector beam vector', detector.beam_vector)

crystal_config = CrystalConfig(cell_a=100, cell_b=100, cell_c=100,
                               cell_alpha=90, cell_beta=90, cell_gamma=90,
                               default_F=100, N_cells=(36,47,29))
crystal = Crystal(crystal_config)
beam_config = BeamConfig(wavelength_A=0.9768, flux=1e18, exposure=1.0, beamsize_mm=1.0)

sim = Simulator(crystal=crystal, detector=detector, beam_config=beam_config,
                crystal_config=crystal_config)
print('incident beam direction', sim.incident_beam_direction)
print('fluence', sim.fluence)
PY
" succeeded in 2.48s:
detector beam vector tensor([ 5.1388e-04,  0.0000e+00, -1.0000e+00])
incident beam direction tensor([ 5.1388e-04,  0.0000e+00, -1.0000e+00])
fluence tensor(1.0000e+24)
[2025-10-07T03:28:34] tokens used: 124,004

[2025-10-07T03:28:41] thinking

**Analyzing fluence discrepancy and beam size handling**

I’m sorting through why fluence values differ between the plan and the simulation. It looks like the plan expected a fluence around 1e29 due to missing beam size unit handling, but the actual pyro run using `beamsize_mm=1.0` results in 1e24 as expected. This suggests the code correctly accounts for beam size in mm² when computing area. However, instrumentation commands might lack necessary flags like `-beam_vector` and `-oversample 1`, causing scaling problems. I also want to verify why the parity run still fails, potentially linked to zero structure factor `F_cell` due to reflections not matching HKL data, so I’ll check the C code’s handling of `F_cell` next.
[2025-10-07T03:28:44] exec bash -lc 'rg "F_cell" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:28:45] bash -lc 'rg "F_cell" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log' succeeded in 715ms:
TRACE_C: F_cell 190.27
[2025-10-07T03:28:45] tokens used: 125,105

[2025-10-07T03:28:59] thinking

**Analyzing simulation discrepancies and command impacts**

I'm exploring why pyro shows zero for `F_cell`, suspecting missing or misread HKL data and scattering vector issues from beam vector defaults. I want to summarize how CLI flags like `-nonoise` and `-pix0_vector_mm` behave, and consider running targeted computations to verify scattering values despite simulation heaviness. I’m forming hypotheses about instrumentation configuration errors leading to the observed scaling mismatches and plan to check actual run reports for confirmation.
[2025-10-07T03:29:00] exec bash -lc 'cat reports/2025-10-cli-flags/phase_i/supervisor_command/summary.json' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:29:01] bash -lc 'cat reports/2025-10-cli-flags/phase_i/supervisor_command/summary.json' succeeded in 761ms:
{
  "correlation": 0.9977842706924522,
  "mse": 1646200832.0,
  "rmse": 40573.40234375,
  "max_abs_diff": 54111648.0,
  "c_sum": 6490.8203125,
  "py_sum": 808353600.0,
  "sum_ratio": 124537.96875,
  "mean_peak_distance": 37.79219496139381,
  "max_peak_distance": 377.9219496139381,
  "runtime_c_ms": 551,
  "runtime_py_ms": 5853,
  "speedup": 0.09425157796001365,
  "args": [
    "-mat",
    "A.mat",
    "-floatfile",
    "img.bin",
    "-hkl",
    "scaled.hkl",
[2025-10-07T03:29:01] tokens used: 126,145

[2025-10-07T03:29:06] thinking

**Investigating oversample parameter consistency**
[2025-10-07T03:29:09] exec bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.config import DetectorConfig, DetectorConvention, CrystalConfig, BeamConfig
from nanobrag_torch.models.detector import Detector
from nanobrag_torch.models.crystal import Crystal
from nanobrag_torch.simulator import Simulator

detector_config = DetectorConfig(
    distance_mm=231.27466,
    pixel_size_mm=0.172,
    spixels=2527,
    fpixels=2463,
    beam_center_f=217.742295,
    beam_center_s=213.907080,
    detector_convention=DetectorConvention.CUSTOM,
    custom_fdet_vector=(-0.000088, 0.004914, -0.999988),
    custom_sdet_vector=(-0.005998, -0.999970, -0.004913),
    custom_odet_vector=(0.999982, -0.005998, -0.000118),
    custom_beam_vector=(0.00051387949, 0.0, -0.99999986),
    oversample=1
)

detector = Detector(detector_config)
crystal_config = CrystalConfig(cell_a=100, cell_b=100, cell_c=100,
                               cell_alpha=90, cell_beta=90, cell_gamma=90,
                               default_F=100, N_cells=(36,47,29))
crystal = Crystal(crystal_config)
beam_config = BeamConfig(wavelength_A=0.9768, flux=1e18, exposure=1.0, beamsize_mm=1.0)

sim = Simulator(crystal=crystal, detector=detector, beam_config=beam_config,
                crystal_config=crystal_config)
print('oversample config', sim.detector.config.oversample)
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:29:12] bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.config import DetectorConfig, DetectorConvention, CrystalConfig, BeamConfig
from nanobrag_torch.models.detector import Detector
from nanobrag_torch.models.crystal import Crystal
from nanobrag_torch.simulator import Simulator

detector_config = DetectorConfig(
    distance_mm=231.27466,
    pixel_size_mm=0.172,
    spixels=2527,
    fpixels=2463,
    beam_center_f=217.742295,
    beam_center_s=213.907080,
    detector_convention=DetectorConvention.CUSTOM,
    custom_fdet_vector=(-0.000088, 0.004914, -0.999988),
    custom_sdet_vector=(-0.005998, -0.999970, -0.004913),
    custom_odet_vector=(0.999982, -0.005998, -0.000118),
    custom_beam_vector=(0.00051387949, 0.0, -0.99999986),
    oversample=1
)

detector = Detector(detector_config)
crystal_config = CrystalConfig(cell_a=100, cell_b=100, cell_c=100,
                               cell_alpha=90, cell_beta=90, cell_gamma=90,
                               default_F=100, N_cells=(36,47,29))
crystal = Crystal(crystal_config)
beam_config = BeamConfig(wavelength_A=0.9768, flux=1e18, exposure=1.0, beamsize_mm=1.0)

sim = Simulator(crystal=crystal, detector=detector, beam_config=beam_config,
                crystal_config=crystal_config)
print('oversample config', sim.detector.config.oversample)
PY
" succeeded in 2.79s:
oversample config 1
[2025-10-07T03:29:12] tokens used: 127,709

[2025-10-07T03:29:17] thinking

**Investigating oversample configuration issues**
[2025-10-07T03:29:18] exec bash -lc 'rg "-oversample" -n src/nanobrag_torch/__main__.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:29:19] bash -lc 'rg "-oversample" -n src/nanobrag_torch/__main__.py' succeeded in 728ms:
1:#!/usr/bin/env python3
2:"""
3:Main entry point for nanoBragg PyTorch CLI.
4:
5:Implements the Reference CLI Binding Profile from spec-a.md,
6:mapping command-line flags to engine parameters per spec requirements.
7:"""
8:
9:import os
10:import sys
11:import argparse
12:import time
13:import warnings
14:import torch
15:import numpy as np
16:from pathlib import Path
17:from typing import Optional, Tuple, List, Dict, Any
18:
19:# Set environment variable for MKL conflicts
20:os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
[2025-10-07T03:29:19] tokens used: 129,483

[2025-10-07T03:29:21] thinking

**Searching for argument definition**
[2025-10-07T03:29:22] exec bash -lc "rg \"add_argument\\('-oversample'\" -n src/nanobrag_torch/__main__.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:29:22] bash -lc "rg \"add_argument\\('-oversample'\" -n src/nanobrag_torch/__main__.py" succeeded in 657ms:
298:    parser.add_argument('-oversample', type=int,
[2025-10-07T03:29:22] tokens used: 133,232
[2025-10-07T03:29:26] exec bash -lc "sed -n '280,320p' src/nanobrag_torch/__main__.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:29:27] bash -lc "sed -n '280,320p' src/nanobrag_torch/__main__.py" succeeded in 706ms:
    parser.add_argument('-mosaic_seed', type=int,
                        help='Seed for mosaic rotations')
    parser.add_argument('-misset', nargs='*',
                        help='Misset angles (deg) or "random"')
    parser.add_argument('-misset_seed', type=int,
                        help='Seed for random misset')

    # Sampling
    parser.add_argument('-phi', type=float, metavar='DEG',
                        help='Starting spindle rotation angle')
    parser.add_argument('-osc', type=float, metavar='DEG',
                        help='Oscillation range')
    parser.add_argument('-phistep', type=float, metavar='DEG',
                        help='Step size')
    parser.add_argument('-phisteps', type=int,
                        help='Number of phi steps')
    parser.add_argument('-dmin', type=float, metavar='Å',
                        help='Minimum d-spacing cutoff')
    parser.add_argument('-oversample', type=int,
                        help='Sub-pixel sampling per axis')
[2025-10-07T03:29:27] tokens used: 133,402

[2025-10-07T03:29:38] thinking

**Analyzing oversample default handling**
[2025-10-07T03:29:39] exec bash -lc 'rg "self.oversample" -n src/nanobrag_torch/config.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:29:40] bash -lc 'rg "self.oversample" -n src/nanobrag_torch/config.py' succeeded in 726ms:
354:        if self.oversample < -1 or self.oversample == 0:
[2025-10-07T03:29:40] tokens used: 134,488

[2025-10-07T03:29:45] thinking

**Checking code near line 350**
[2025-10-07T03:29:47] exec bash -lc "sed -n '320,380p' src/nanobrag_torch/config.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:29:48] bash -lc "sed -n '320,380p' src/nanobrag_torch/config.py" succeeded in 696ms:
            # Default depends on detector convention (per spec AT-GEO-004)
            if self.detector_convention == DetectorConvention.MOSFLM:
                # MOSFLM convention: twotheta axis is [0, 0, -1]
                self.twotheta_axis = torch.tensor([0.0, 0.0, -1.0])
            elif self.detector_convention == DetectorConvention.DENZO:
                # DENZO convention: Same as MOSFLM for twotheta axis [0, 0, -1]
                self.twotheta_axis = torch.tensor([0.0, 0.0, -1.0])
            elif self.detector_convention == DetectorConvention.XDS:
                # XDS convention: twotheta axis is [1, 0, 0]
                self.twotheta_axis = torch.tensor([1.0, 0.0, 0.0])
            elif self.detector_convention == DetectorConvention.DIALS:
                # DIALS convention: twotheta axis is [0, 1, 0]
                self.twotheta_axis = torch.tensor([0.0, 1.0, 0.0])
            elif self.detector_convention == DetectorConvention.ADXV:
                # ADXV convention: twotheta axis is [-1, 0, 0]
                self.twotheta_axis = torch.tensor([-1.0, 0.0, 0.0])
            else:
                # Default fallback to DIALS axis [0, 1, 0]
                self.twotheta_axis = torch.tensor([0.0, 1.0, 0.0])

[2025-10-07T03:29:48] tokens used: 135,231

[2025-10-07T03:29:55] thinking

**Analyzing oversample type issues**
[2025-10-07T03:29:59] exec bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.__main__ import create_parser, parse_and_validate_args
args_list = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
parser = create_parser()
args = parser.parse_args(args_list)
config = parse_and_validate_args(args)
print('oversample key present?', 'oversample' in config)
print('oversample value', config.get('oversample'))
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:30:02] bash -lc "KMP_DUPLICATE_LIB_OK=TRUE python - <<'PY'
from nanobrag_torch.__main__ import create_parser, parse_and_validate_args
args_list = [
    '-mat','A.mat','-floatfile','img.bin','-hkl','scaled.hkl','-nonoise','-nointerpolate',
    '-exposure','1','-flux','1e18','-beamsize','1.0','-spindle_axis','-1','0','0',
    '-Xbeam','217.742295','-Ybeam','213.907080','-distance','231.274660','-lambda','0.976800','-pixel','0.172',
    '-detpixels_x','2463','-detpixels_y','2527','-odet_vector','-0.000088','0.004914','-0.999988',
    '-sdet_vector','-0.005998','-0.999970','-0.004913','-fdet_vector','0.999982','-0.005998','-0.000118',
    '-pix0_vector_mm','-216.336293','215.205512','-230.200866','-beam_vector','0.00051387949','0.0','-0.99999986',
    '-Na','36','-Nb','47','-Nc','29','-osc','0.1','-phi','0','-phisteps','10',
    '-detector_rotx','0','-detector_roty','0','-detector_rotz','0','-twotheta','0'
]
parser = create_parser()
args = parser.parse_args(args_list)
config = parse_and_validate_args(args)
print('oversample key present?', 'oversample' in config)
print('oversample value', config.get('oversample'))
PY
" succeeded in 3.28s:
oversample key present? True
oversample value -1
[2025-10-07T03:30:02] tokens used: 136,919

[2025-10-07T03:30:11] thinking

**Investigating default oversample value**
[2025-10-07T03:30:11] exec bash -lc "rg \"config\\['oversample'\" -n src/nanobrag_torch/__main__.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:30:12] bash -lc "rg \"config\\['oversample'\" -n src/nanobrag_torch/__main__.py" succeeded in 736ms:
624:    config['oversample'] = args.oversample if args.oversample else -1  # -1 means auto-select
805:            print(f"  Oversample: {simulator_config['oversample']}")
[2025-10-07T03:30:12] tokens used: 138,469

[2025-10-07T03:30:41] thinking

**Analyzing parity failure hypotheses**

I’m exploring why parity fails despite the command including `-oversample 1`, noting instrumentation logs omit key flags like `-beam_vector`, causing discrepancies in scattering data. I’m forming three hypotheses: missing beam vector in instrumentation causes mismatches; a persistent `F_cell` vs lattice mismatch due to crystal orientation or scaling; and possible normalization issues in fluence conversion. I plan to verify these by reviewing traces and possibly rerunning with full command flags, while also acknowledging the ongoing vectorization planning and noting the need to communicate baseline harness progress.
[2025-10-07T03:30:42] exec bash -lc "python - <<'PY'
import random
print('heads' if random.random() < 0.5 else 'tails')
PY
" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:30:43] bash -lc "python - <<'PY'
import random
print('heads' if random.random() < 0.5 else 'tails')
PY
" succeeded in 927ms:
tails
[2025-10-07T03:30:43] tokens used: 140,209

[2025-10-07T03:31:16] thinking

**Evaluating plan update necessity**
[2025-10-07T03:31:24] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/plans/active/cli-noise-pix0/plan.md
@@ -13,2 +13,3 @@
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
@@ -239,3 +240,3 @@
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
[2025-10-07T03:31:24] apply_patch(auto_approved=true) exited 0 in 17ms:
Success. Updated the following files:
M plans/active/cli-noise-pix0/plan.md
[2025-10-07T03:31:24] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T03:31:24] tokens used: 143,138
[2025-10-07T03:31:24] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:31:29] thinking

**Updating next actions in fix_plan.md**
[2025-10-07T03:31:30] exec bash -lc "sed -n '480,520p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:31:31] bash -lc "sed -n '480,520p' docs/fix_plan.md" succeeded in 973ms:
        3. Incorrect fluence calculation (C: 1e24 photons/m², PyTorch: verify derivation from flux/exposure/beamsize)
        4. Missing last-value Ω/polarization multiply (per spec § oversample_* semantics with oversample=1)
      - **Tooling success:** nb-compare now handles non-standard detector dimensions (2463×2527 via -detpixels_x/-detpixels_y)
    Next Actions: Superseded by 2025-10-19 directive; follow Phase J checklist in `plans/active/cli-noise-pix0/plan.md` before any implementation work.
  * [2025-10-19] Attempt #28 (ralph) — Result: **EVIDENCE COMPLETE** (Phase J scaling diagnosis). **First divergence identified: `I_before_scaling` caused by F_latt calculation error.**
    Metrics: C I_before_scaling=1.48e15, PyTorch I_before_scaling=5.34e8 (ratio=3.6e-7, ~2772× difference). Root cause: F_latt components differ drastically (C: F_latt_a=35.9, F_latt_b=38.6, F_latt_c=25.7 → F_latt=35636; PyTorch: F_latt_a=-2.4, F_latt_b=11.8, F_latt_c=-2.7 → F_latt=76.9). This 463× F_latt error squares to create the observed ~2e5× intensity gap.
    Artifacts:
      - `reports/2025-10-cli-flags/phase_j/trace_c_scaling.log` - C scaling trace for pixel (1039,685)
      - `reports/2025-10-cli-flags/phase_j/trace_py_scaling.log` - PyTorch scaling trace for same pixel
      - `reports/2025-10-cli-flags/phase_j/analyze_scaling.py` - Analysis script computing factor-by-factor ratios
      - `reports/2025-10-cli-flags/phase_j/scaling_chain.md` - Full ratio analysis and diagnosis
    Observations/Hypotheses:
      - **F_latt components diverge before normalization:** C values (35.9, 38.6, 25.7) vs PyTorch (-2.4, 11.8, -2.7) indicate lattice shape factor calculation error
      - **Secondary issues (non-blocking):** Polarization C=0.9126 vs PyTorch=1.0 (9.6% discrepancy), omega C=4.16e-7 vs PyTorch=4.17e-7 (0.2% difference, within tolerance)
      - **Steps, r_e², fluence all match exactly:** Confirms normalization infrastructure is correct; issue is in physics computation upstream
      - **F_cell matches perfectly (300.58):** Structure factor lookup/interpolation working correctly
      - **Confirmed driver:** Pix0 override is skipped in the custom-vector branch, leaving PyTorch `Fbeam/Sbeam` near 0.0369 m vs C ≈0.2179 m and producing 1.14 mm pixel-position error that cascades into h/k/l and `F_latt`.
    Next Actions: Phase H5 (new) reinstates the override precedence, then Phase K will address any residual normalization deltas once lattice parity is restored.
  * [2025-10-17] Attempt #26 (ralph) — Result: success (Phase I polarization defaults complete). **BeamConfig.polarization_factor default aligned with C polar=1.0.**
    Metrics: CLI polarization tests 3/3 passed. Core tests: cli_flags 26/26, detector_geometry 12/12, crystal_geometry 19/19 (57 passed, 1 warning). Test collection: 632 tests collected.
[2025-10-07T03:31:31] tokens used: 145,313
[2025-10-07T03:31:31] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:31:35] thinking

**Searching for Next Actions list**
[2025-10-07T03:31:36] exec bash -lc "rg \"Next Actions \\(2025-11-10 refresh\\)\" -n docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:31:37] bash -lc "rg \"Next Actions \\(2025-11-10 refresh\\)\" -n docs/fix_plan.md" succeeded in 707ms:
458:- Next Actions (2025-11-10 refresh):
[2025-10-07T03:31:37] tokens used: 147,240
[2025-10-07T03:31:37] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T03:31:40] exec bash -lc "sed -n '458,470p' docs/fix_plan.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:31:41] bash -lc "sed -n '458,470p' docs/fix_plan.md" succeeded in 677ms:
- Next Actions (2025-11-10 refresh):
  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
- Attempts History:
  * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
    Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
    Artifacts:
      - `reports/2025-10-cli-flags/phase_i/supervisor_command/README.md` - Full evidence report with metrics, hypotheses, next actions
      - `reports/2025-10-cli-flags/phase_i/supervisor_command/summary.json` - Complete parity metrics (correlation, RMSE, sums, peak distances)
      - `reports/2025-10-cli-flags/phase_i/supervisor_command/c_stdout.txt` - C simulation output (max_I=446.254, fluence=1e24, steps=10)
      - `reports/2025-10-cli-flags/phase_i/supervisor_command/py_stdout.txt` - PyTorch output (max_I=5.411e7)
[2025-10-07T03:31:41] tokens used: 147,513
[2025-10-07T03:31:41] turn diff:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:31:46] thinking

**Updating Next Actions list**
[2025-10-07T03:31:52] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/docs/fix_plan.md
@@ -457,7 +457,8 @@
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
[2025-10-07T03:31:52] apply_patch(auto_approved=true) exited 0 in 17ms:
Success. Updated the following files:
M docs/fix_plan.md
[2025-10-07T03:31:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T03:31:52] tokens used: 149,177
[2025-10-07T03:31:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:31:58] thinking

**Preparing input.md content plan**

I’m organizing the key information needed to write the supervisor memo `input.md`, making sure the instructions for Step <3.5> are clear, especially the rerun of the trace harness with precise commands and references to relevant plan IDs and artifacts. I'll check the `trace_harness.py` to confirm the command details before finalizing.
[2025-10-07T03:31:59] exec bash -lc 'head -n 80 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:32:00] bash -lc 'head -n 80 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py' succeeded in 871ms:
#!/usr/bin/env python
"""
Phase L2b PyTorch Scaling Trace Harness

Captures the PyTorch side of the scaling chain comparison for supervisor command.
Mirrors the C trace fields from reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log.

Date: 2025-10-17
Engineer: Ralph (loop i=62, evidence-only mode)
Plan Reference: plans/active/cli-noise-pix0/plan.md Phase L2b
"""

import argparse
import json
import sys
import platform
import os
import torch
from pathlib import Path
from datetime import datetime
[2025-10-07T03:32:00] tokens used: 150,434
[2025-10-07T03:32:00] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:32:03] thinking

**Checking for specific flags**
[2025-10-07T03:32:04] exec bash -lc "sed -n '80,200p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:32:05] bash -lc "sed -n '80,200p' reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py" succeeded in 756ms:
            'beam_center_f': 217.742295,  # Xbeam in mm
            'detector_rotx_deg': 0.0,
            'detector_roty_deg': 0.0,
            'detector_rotz_deg': 0.0,
            'detector_twotheta_deg': 0.0,
            'detector_convention': DetectorConvention.CUSTOM,
            'detector_pivot': DetectorPivot.SAMPLE,
            'oversample': 1,
            # Custom detector vectors (per supervisor command)
            'custom_fdet_vector': (0.999982, -0.005998, -0.000118),
            'custom_sdet_vector': (-0.005998, -0.999970, -0.004913),
            'custom_odet_vector': (-0.000088, 0.004914, -0.999988),
            'custom_beam_vector': (0.00051387949, 0.0, -0.99999986),
            # Custom pix0 override (mm→m conversion)
            'pix0_override_m': (-0.216336293, 0.215205512, -0.230200866)
        },
        'beam': {
            'wavelength_A': 0.9768,
            'polarization_factor': 0.0,  # Kahn factor = 0.0 per c_stdout line 132
            'flux': 1e18,
[2025-10-07T03:32:05] tokens used: 151,546
[2025-10-07T03:32:05] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:32:24] thinking

**Drafting detailed harness rerun memo**

I’m sorting out the differences between the CLI-generated and harness-generated trace files, noticing the harness isn’t capturing the command line yet. To resolve this, I’m planning to craft a thorough `input.md` memo of about 100–200 lines, laying out the summary, priorities, instructions, pitfalls, and file pointers aligned with the latest harness rerun, including the exact command and fallback instructions. This memo will help orchestrate rerunning the harness so we get consistent trace data with full CLI flags and environment info.
[2025-10-07T03:32:39] exec bash -lc "cat <<'EOF' > input.md
Summary: Refresh Py scaling trace with full supervisor flags so Phase L2 analysis reflects the real command.
Mode: Parity
Focus: CLI-FLAGS-003 — Handle -nonoise and -pix0_vector_mm
Branch: main
Mapped tests: none — evidence-only
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log; reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json; reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
Do Now: CLI-FLAGS-003 — run "'`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and update notes with observed deltas.
If Blocked: capture terminal output, copy stderr/stdout into reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_log.md, note command + failure, and ping supervisor in Attempts History.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md Phase L2b pre-step mandates rerunning the harness with the authoritative supervisor command before further normalization work.
- docs/fix_plan.md:458-467 now lists Phase L2b0 as the leading action; without evidence that the full CLI flag set is honored the scaling diff remains contaminated.
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log currently shows `incident_vec 0 0 1` and `steps 160`, proving the trace omitted `-beam_vector` and `-oversample 1`.
- specs/spec-a-cli.md §3.2 requires exact CLI parameter parity; the trace must include `-flux`+`-beamsize` so fluence math mirrors C.
- docs/development/c_to_pytorch_config_map.md (detector section) states custom vectors force SAMPLE pivot; confirm the rerun preserves this so pix0 parity diagnostics stay valid.
- docs/development/testing_strategy.md §2 highlights command reproducibility; regenerating env/notes with the correct flag set satisfies evidence hygiene.
How-To Map:
- Ensure you run from repo root with PYTHONPATH=src so harness resolves project modules.
- Use device=cpu and dtype=float32 for the first pass; once trace lands you can clone command for CUDA if needed in later loops.
- After the harness finishes, open reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json and confirm it lists the refreshed timestamp and device.
- Append a short section to reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md summarizing the new steps/polarization/fluence values vs previous artifacts.
- If the harness emits warnings about missing HKL cache, re-run `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py ...` after ensuring `scaled.hkl` and `A.mat` exist in CWD.
Pitfalls To Avoid:
- Do not edit simulator code this loop; we are in evidence-only mode.
- Keep device/dtype neutral: no hard-coded `.cpu()` or dtype conversions in scripts.
- Do not rerun the entire nb-compare workflow yet; focus on the single trace pixel.
- Avoid regenerating C traces; existing `c_trace_scaling.log` remains the reference.
- Respect Protected Assets: leave docs/index.md and loop.sh untouched.
- When updating notes, stay in ASCII and avoid deleting prior observations.
- Do not run full pytest; evidence mode allows at most `pytest --collect-only -q` if you need to verify selectors (not required now).
- Ensure `KMP_DUPLICATE_LIB_OK=TRUE` stays set to prevent libomp crashes.
- Keep working tree clean; new artifacts go under reports/2025-10-cli-flags/phase_l/scaling_audit/ with timestamps if you branch off.
- Verify the harness command uses slow,fast ordering (685,1039) to match the existing C trace pixel.
Pointers:
- plans/active/cli-noise-pix0/plan.md (Phase L2) for context and acceptance criteria.
- docs/fix_plan.md:458-467 for the updated Next Actions checklist driving this loop.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log for ground-truth factors.
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log (existing) to compare against the new trace.
- specs/spec-a-cli.md §3.2 flag catalog to verify the CLI set remains compliant.
Next Up: If the refreshed trace matches the command, proceed to Phase L2b instrumentation test addition (`tests/test_trace_pixel.py` selector) before diving into normalization fixes.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:32:40] bash -lc "cat <<'EOF' > input.md
Summary: Refresh Py scaling trace with full supervisor flags so Phase L2 analysis reflects the real command.
Mode: Parity
Focus: CLI-FLAGS-003 — Handle -nonoise and -pix0_vector_mm
Branch: main
Mapped tests: none — evidence-only
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log; reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json; reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
Do Now: CLI-FLAGS-003 — run "'`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and update notes with observed deltas.
If Blocked: capture terminal output, copy stderr/stdout into reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_log.md, note command + failure, and ping supervisor in Attempts History.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md Phase L2b pre-step mandates rerunning the harness with the authoritative supervisor command before further normalization work.
- docs/fix_plan.md:458-467 now lists Phase L2b0 as the leading action; without evidence that the full CLI flag set is honored the scaling diff remains contaminated.
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log currently shows `incident_vec 0 0 1` and `steps 160`, proving the trace omitted `-beam_vector` and `-oversample 1`.
- specs/spec-a-cli.md §3.2 requires exact CLI parameter parity; the trace must include `-flux`+`-beamsize` so fluence math mirrors C.
- docs/development/c_to_pytorch_config_map.md (detector section) states custom vectors force SAMPLE pivot; confirm the rerun preserves this so pix0 parity diagnostics stay valid.
- docs/development/testing_strategy.md §2 highlights command reproducibility; regenerating env/notes with the correct flag set satisfies evidence hygiene.
How-To Map:
- Ensure you run from repo root with PYTHONPATH=src so harness resolves project modules.
- Use device=cpu and dtype=float32 for the first pass; once trace lands you can clone command for CUDA if needed in later loops.
- After the harness finishes, open reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json and confirm it lists the refreshed timestamp and device.
- Append a short section to reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md summarizing the new steps/polarization/fluence values vs previous artifacts.
- If the harness emits warnings about missing HKL cache, re-run `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py ...` after ensuring `scaled.hkl` and `A.mat` exist in CWD.
Pitfalls To Avoid:
- Do not edit simulator code this loop; we are in evidence-only mode.
- Keep device/dtype neutral: no hard-coded `.cpu()` or dtype conversions in scripts.
- Do not rerun the entire nb-compare workflow yet; focus on the single trace pixel.
- Avoid regenerating C traces; existing `c_trace_scaling.log` remains the reference.
- Respect Protected Assets: leave docs/index.md and loop.sh untouched.
- When updating notes, stay in ASCII and avoid deleting prior observations.
- Do not run full pytest; evidence mode allows at most `pytest --collect-only -q` if you need to verify selectors (not required now).
- Ensure `KMP_DUPLICATE_LIB_OK=TRUE` stays set to prevent libomp crashes.
- Keep working tree clean; new artifacts go under reports/2025-10-cli-flags/phase_l/scaling_audit/ with timestamps if you branch off.
- Verify the harness command uses slow,fast ordering (685,1039) to match the existing C trace pixel.
Pointers:
- plans/active/cli-noise-pix0/plan.md (Phase L2) for context and acceptance criteria.
- docs/fix_plan.md:458-467 for the updated Next Actions checklist driving this loop.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log for ground-truth factors.
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log (existing) to compare against the new trace.
- specs/spec-a-cli.md §3.2 flag catalog to verify the CLI set remains compliant.
Next Up: If the refreshed trace matches the command, proceed to Phase L2b instrumentation test addition (`tests/test_trace_pixel.py` selector) before diving into normalization fixes.
EOF
' succeeded in 890ms:

[2025-10-07T03:32:40] tokens used: 155,296
[2025-10-07T03:32:40] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:32:43] thinking

**Counting lines for length check**
[2025-10-07T03:32:45] exec bash -lc 'wc -l input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:32:46] bash -lc 'wc -l input.md' succeeded in 682ms:
      39 input.md
[2025-10-07T03:32:46] tokens used: 157,074
[2025-10-07T03:32:46] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:32:56] thinking

**Planning to expand documentation length**
[2025-10-07T03:33:24] exec bash -lc "cat <<'EOF' > input.md
Summary: Refresh the Py scaling trace using the full supervisor flag set so Phase L2 comparison reflects the real command.
Mode: Parity
Focus: CLI-FLAGS-003 — Handle -nonoise and -pix0_vector_mm
Branch: main
Mapped tests: none — evidence-only
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log; reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json; reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
Do Now: CLI-FLAGS-003 — run "'`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes on scaling factors.
If Blocked: copy stderr/stdout plus command into reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_log.md, annotate the obstacle in Attempts History, and halt pending supervisor guidance.
Context Reminders:
- Phase L2 remained inconclusive because prior traces omitted -beam_vector and auto-selected oversample=4, corrupting h/k/l and scaling factors.
- The supervisor command in reports/2025-10-cli-flags/phase_i/supervisor_command/README.md is the authoritative template; mirror it exactly.
- plans/active/cli-noise-pix0/plan.md documents the parity journey; refresh Phase L tables after generating new evidence.
- docs/development/c_to_pytorch_config_map.md stresses SAMPLE pivot when custom vectors appear; confirm harness output obeys that rule.
- specs/spec-a-cli.md §3.2 lists required CLI flags; every value in the supervisor command must appear in the harness parameter dict or CLI call.
- docs/architecture/detector.md §5.3 describes pix0 precedence; retaining SAMPLE pivot ensures pix0 parity remains validated once scaling is fixed.
- docs/development/testing_strategy.md §2 requires reproducible commands for evidence; archive the rerun details under reports/2025-10-cli-flags/phase_l/.
- Trace environment snapshots should include Python, torch, device, dtype, and git SHA so future loops know the context of the evidence.
- The existing C trace `c_trace_scaling.log` already covers pixel (685,1039); Py trace must target the same coordinates.
- Reports under phase_l are shared assets; add new files, do not overwrite C artifacts.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md Phase L2b pre-step (added 2025-11-11) demands a trace harness rerun with the full flag set before any normalization diagnosis resumes.
- docs/fix_plan.md:458-467 elevates Phase L2b0 to the top of the action list; satisfying that checkpoint unblocks instrumentation follow-ups.
- Existing Py trace shows `incident_vec 0 0 1` proving the command omitted the custom beam vector; the rerun must demonstrate the correct beam direction.
- Steps currently read 160 (auto oversample 4×4); rerunning with `-oversample 1` verifies PyTorch honors the CLI override and aligns with the C log.
- Fluence discrepancies (~1e29) were side-effects of missing `-flux`/`-beamsize`; including those flags tests the true normalization chain.
- Polarization reported as 1.0; with a correct beam vector we expect ≈0.9146 as in C, validating `_apply_polarization` output.
- pix0 parity already validated post Phase H6; keeping SAMPLE pivot via custom vectors ensures geometry stays locked while scaling is debugged.
- CLI -nonoise flag suppresses noise path; including it keeps runtime short and avoids unnecessary SMV writes in evidence mode.
- Running the harness consolidates artifacts under phase_l, satisfying the evidence chain mandated by testing_strategy.md §2.1.
- The rerun feeds into L2c comparison; without this data we cannot identify the first divergence accurately.
How-To Map:
- From repo root export `PYTHONPATH=src` (or prefix the command) so the harness finds the installed package modules.
- Ensure `KMP_DUPLICATE_LIB_OK=TRUE` prefixes the command to dodge libomp initialization crashes.
- Use CPU + float32 for this pass; record runtime so a future float64/CUDA rerun can be compared if needed.
- Command options: `--config supervisor` selects the embedded supervisor parameter set; no manual flag editing required.
- `--pixel 685 1039` uses slow, fast ordering to match C trace fields; double-check this before running.
- `--out .../trace_py_scaling.log` overwrites the stale trace; stash the previous file if you need historical reference before rerun.
- Confirm `A.mat` and `scaled.hkl` sit in repo root; the harness reads them directly.
- After execution open `trace_py_env.json` and verify timestamp, device, dtype, and git SHA updated.
- Append a short delta summary (steps, fluence, polar, capture_fraction) to `notes.md`; note any remaining divergence vs C.
- If the harness raises missing HKL range warnings, delete any stale `.hkl.1` copies and rerun so the loader finds the canonical file.
Verification Checklist:
- [ ] Command executed with PYTHONPATH and KMP env vars set.
- [ ] Harness stdout/stderr captured into the reports directory.
- [ ] trace_py_scaling.log regenerated and reflects the new beam vector and steps count.
- [ ] trace_py_env.json refreshed with current timestamp/device/dtype.
- [ ] notes.md updated with comparison metrics versus C trace.
- [ ] No additional artifacts leaked outside reports/2025-10-cli-flags/phase_l/.
Pitfalls To Avoid:
- Do not edit simulator or detector source files this turn; evidence only.
- Avoid running the full parity script (`nb-compare`) until the trace is trustworthy.
- Keep tensor operations device-neutral—no `.cpu()` when inspecting results in notes.
- Do not regenerate C traces; the existing log remains ground truth for Phase L2.
- Respect Protected Assets: leave docs/index.md, loop.sh, supervisor.sh untouched.
- Refrain from running `pytest` suites; evidence-mode allows `--collect-only` at most, which is unnecessary here.
- Ensure the working directory remains clean apart from the refreshed reports files.
- Do not convert notes to Markdown tables; stick with ASCII bullet/paragraph for quick diffs.
- Double-check the harness uses SAMPLE pivot (detector config shows DetectorPivot.SAMPLE) before trusting pix0 parity.
- Remember to retain the old trace file in version control history; no manual backups needed outside git.
Additional Guidance:
- If CUDA testing becomes necessary later, reuse the same harness with `--device cuda --dtype float32` after confirming GPU availability.
- Consider copying the old trace to `trace_py_scaling_prev.log` before overwriting if you want an immediate diff; otherwise rely on git.
- If runtime exceeds expectations, use `time` command wrapper to log execution duration next to the trace.
- After updating notes.md, include a mini-table or bullet list summarizing steps/polar/fluence differences versus the C trace for quick reference.
- Capture any unexpected warnings (e.g., HKL cache rebuild) in notes.md so subsequent loops know the context.
- Should you notice a mismatch in pix0 or beam centers, flag it immediately—geometry regressions must be halted before normalization work.
- Keep an eye on memory usage; if the harness strains RAM, narrow the ROI inside the script before rerunning (document the tweak).
- Use `git status` after the run to ensure only intended artifacts changed; if other files appear, investigate before proceeding.
- Update Attempts History with the rerun details (command, runtime, high-level deltas) once artifacts are saved.
- Prepare to hand the refreshed data into Phase L2c by noting which factor diverges first after the rerun.
Pointers:
- plans/active/cli-noise-pix0/plan.md (Phase L2 table) — authoritative checklist for scaling parity.
- docs/fix_plan.md:458-467 — supervisor item status and updated next steps.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C reference factors.
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log — Py trace to regenerate and compare.
- specs/spec-a-cli.md §3.2 — CLI flag behaviour and precedence rules for this command.
Next Up: Once the new trace confirms the authoritative command, tackle Phase L2b instrumentation regression test (`tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) before moving to L2c diff tooling.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:33:25] bash -lc "cat <<'EOF' > input.md
Summary: Refresh the Py scaling trace using the full supervisor flag set so Phase L2 comparison reflects the real command.
Mode: Parity
Focus: CLI-FLAGS-003 — Handle -nonoise and -pix0_vector_mm
Branch: main
Mapped tests: none — evidence-only
Artifacts: reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log; reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json; reports/2025-10-cli-flags/phase_l/scaling_audit/notes.md
Do Now: CLI-FLAGS-003 — run "'`PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes on scaling factors.
If Blocked: copy stderr/stdout plus command into reports/2025-10-cli-flags/phase_l/scaling_audit/attempt_log.md, annotate the obstacle in Attempts History, and halt pending supervisor guidance.
Context Reminders:
- Phase L2 remained inconclusive because prior traces omitted -beam_vector and auto-selected oversample=4, corrupting h/k/l and scaling factors.
- The supervisor command in reports/2025-10-cli-flags/phase_i/supervisor_command/README.md is the authoritative template; mirror it exactly.
- plans/active/cli-noise-pix0/plan.md documents the parity journey; refresh Phase L tables after generating new evidence.
- docs/development/c_to_pytorch_config_map.md stresses SAMPLE pivot when custom vectors appear; confirm harness output obeys that rule.
- specs/spec-a-cli.md §3.2 lists required CLI flags; every value in the supervisor command must appear in the harness parameter dict or CLI call.
- docs/architecture/detector.md §5.3 describes pix0 precedence; retaining SAMPLE pivot ensures pix0 parity remains validated once scaling is fixed.
- docs/development/testing_strategy.md §2 requires reproducible commands for evidence; archive the rerun details under reports/2025-10-cli-flags/phase_l/.
- Trace environment snapshots should include Python, torch, device, dtype, and git SHA so future loops know the context of the evidence.
- The existing C trace `c_trace_scaling.log` already covers pixel (685,1039); Py trace must target the same coordinates.
- Reports under phase_l are shared assets; add new files, do not overwrite C artifacts.
Priorities & Rationale:
- plans/active/cli-noise-pix0/plan.md Phase L2b pre-step (added 2025-11-11) demands a trace harness rerun with the full flag set before any normalization diagnosis resumes.
- docs/fix_plan.md:458-467 elevates Phase L2b0 to the top of the action list; satisfying that checkpoint unblocks instrumentation follow-ups.
- Existing Py trace shows `incident_vec 0 0 1` proving the command omitted the custom beam vector; the rerun must demonstrate the correct beam direction.
- Steps currently read 160 (auto oversample 4×4); rerunning with `-oversample 1` verifies PyTorch honors the CLI override and aligns with the C log.
- Fluence discrepancies (~1e29) were side-effects of missing `-flux`/`-beamsize`; including those flags tests the true normalization chain.
- Polarization reported as 1.0; with a correct beam vector we expect ≈0.9146 as in C, validating `_apply_polarization` output.
- pix0 parity already validated post Phase H6; keeping SAMPLE pivot via custom vectors ensures geometry stays locked while scaling is debugged.
- CLI -nonoise flag suppresses noise path; including it keeps runtime short and avoids unnecessary SMV writes in evidence mode.
- Running the harness consolidates artifacts under phase_l, satisfying the evidence chain mandated by testing_strategy.md §2.1.
- The rerun feeds into L2c comparison; without this data we cannot identify the first divergence accurately.
How-To Map:
- From repo root export `PYTHONPATH=src` (or prefix the command) so the harness finds the installed package modules.
- Ensure `KMP_DUPLICATE_LIB_OK=TRUE` prefixes the command to dodge libomp initialization crashes.
- Use CPU + float32 for this pass; record runtime so a future float64/CUDA rerun can be compared if needed.
- Command options: `--config supervisor` selects the embedded supervisor parameter set; no manual flag editing required.
- `--pixel 685 1039` uses slow, fast ordering to match C trace fields; double-check this before running.
- `--out .../trace_py_scaling.log` overwrites the stale trace; stash the previous file if you need historical reference before rerun.
- Confirm `A.mat` and `scaled.hkl` sit in repo root; the harness reads them directly.
- After execution open `trace_py_env.json` and verify timestamp, device, dtype, and git SHA updated.
- Append a short delta summary (steps, fluence, polar, capture_fraction) to `notes.md`; note any remaining divergence vs C.
- If the harness raises missing HKL range warnings, delete any stale `.hkl.1` copies and rerun so the loader finds the canonical file.
Verification Checklist:
- [ ] Command executed with PYTHONPATH and KMP env vars set.
- [ ] Harness stdout/stderr captured into the reports directory.
- [ ] trace_py_scaling.log regenerated and reflects the new beam vector and steps count.
- [ ] trace_py_env.json refreshed with current timestamp/device/dtype.
- [ ] notes.md updated with comparison metrics versus C trace.
- [ ] No additional artifacts leaked outside reports/2025-10-cli-flags/phase_l/.
Pitfalls To Avoid:
- Do not edit simulator or detector source files this turn; evidence only.
- Avoid running the full parity script (`nb-compare`) until the trace is trustworthy.
- Keep tensor operations device-neutral—no `.cpu()` when inspecting results in notes.
- Do not regenerate C traces; the existing log remains ground truth for Phase L2.
- Respect Protected Assets: leave docs/index.md, loop.sh, supervisor.sh untouched.
- Refrain from running `pytest` suites; evidence-mode allows `--collect-only` at most, which is unnecessary here.
- Ensure the working directory remains clean apart from the refreshed reports files.
- Do not convert notes to Markdown tables; stick with ASCII bullet/paragraph for quick diffs.
- Double-check the harness uses SAMPLE pivot (detector config shows DetectorPivot.SAMPLE) before trusting pix0 parity.
- Remember to retain the old trace file in version control history; no manual backups needed outside git.
Additional Guidance:
- If CUDA testing becomes necessary later, reuse the same harness with `--device cuda --dtype float32` after confirming GPU availability.
- Consider copying the old trace to `trace_py_scaling_prev.log` before overwriting if you want an immediate diff; otherwise rely on git.
- If runtime exceeds expectations, use `time` command wrapper to log execution duration next to the trace.
- After updating notes.md, include a mini-table or bullet list summarizing steps/polar/fluence differences versus the C trace for quick reference.
- Capture any unexpected warnings (e.g., HKL cache rebuild) in notes.md so subsequent loops know the context.
- Should you notice a mismatch in pix0 or beam centers, flag it immediately—geometry regressions must be halted before normalization work.
- Keep an eye on memory usage; if the harness strains RAM, narrow the ROI inside the script before rerunning (document the tweak).
- Use `git status` after the run to ensure only intended artifacts changed; if other files appear, investigate before proceeding.
- Update Attempts History with the rerun details (command, runtime, high-level deltas) once artifacts are saved.
- Prepare to hand the refreshed data into Phase L2c by noting which factor diverges first after the rerun.
Pointers:
- plans/active/cli-noise-pix0/plan.md (Phase L2 table) — authoritative checklist for scaling parity.
- docs/fix_plan.md:458-467 — supervisor item status and updated next steps.
- reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log — C reference factors.
- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log — Py trace to regenerate and compare.
- specs/spec-a-cli.md §3.2 — CLI flag behaviour and precedence rules for this command.
Next Up: Once the new trace confirms the authoritative command, tackle Phase L2b instrumentation regression test (`tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) before moving to L2c diff tooling.
EOF
' succeeded in 902ms:

[2025-10-07T03:33:25] tokens used: 159,628
[2025-10-07T03:33:25] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:33:27] thinking

**Counting lines**
[2025-10-07T03:33:28] exec bash -lc 'wc -l input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:33:29] bash -lc 'wc -l input.md' succeeded in 661ms:
      77 input.md
[2025-10-07T03:33:29] tokens used: 161,761
[2025-10-07T03:33:29] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:33:33] thinking

**Preparing to extend document with more sections**
[2025-10-07T03:33:44] exec bash -lc "cat <<'EOF' >> input.md
Reference Commands:
- To compare traces quickly later, prepare "'`python scripts/validation/diff_scaling_traces.py --c reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log --py reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log` (implementation pending Phase L2c).
- For sanity, `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python -m nanobrag_torch --help | head` reminds which flags the CLI exposes if questions arise mid-run.
- Use `git rev-parse HEAD` after the rerun to embed the commit SHA in notes.md for traceability.
- `ls -lh reports/2025-10-cli-flags/phase_l/scaling_audit/` helps verify file sizes changed as expected (trace log should update timestamp and size).
- `tail -n 60 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log` should show the refreshed beam vector, steps, F_latt, fluence, and intensity fields.
- The C log can be inspected quickly via `grep -n "F_latt" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log` for cross-checking values mentioned in notes.
- If you need to sanity-check HKL coverage, `head -n 20 scaled.hkl` will confirm the file is present and not empty.
- In case `trace_harness.py` raises module import errors, run `pip install -e .` to reestablish the editable install (document in Attempts if executed).
- To double-check oversample handling, search the simulator with `rg "auto-selected" src/nanobrag_torch/simulator.py` and confirm it no longer triggers once oversample=1 is set.
- When updating notes, include a pointer to docs/fix_plan.md Attempt #27 so the new findings sit alongside prior evidence.
Documentation Hooks:
- Mention in notes the exact commit SHA and date of this rerun for future archaeological digs.
- Cross-link the note entry to `plans/active/cli-noise-pix0/plan.md` Phase L2b so the engineer loop can trace provenance.
- If fluence remains mismatched, cite the relevant BeamConfig lines (src/nanobrag_torch/config.py around 430-470) in the note.
- Document any difference in `hkl_frac` vs C; specify whether the first divergence is still in fractional indices or in scaling terms.
- Record the observed `polar` value and relate it to the expected 0.9146 from C to show improvement/regression.
- Highlight changes in `steps` value; note if oversample stayed at 1 or reverted to auto selection.
- Note capture_fraction even if it remains 1.0; state that thickness absorption remains disabled.
- For completeness, mention `omega_pixel` difference (should remain within sub-percent of C).
- If `F_cell` reads 0, investigate HKL lookup in `crystal.get_structure_factor` and log the suspicion for Phase L3.
- Clarify whether the incident vector equals `[5.1388e-04, 0.0, -1.0]`; quote the log line so others can grep quickly.
EOF
' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:33:45] bash -lc "cat <<'EOF' >> input.md
Reference Commands:
- To compare traces quickly later, prepare "'`python scripts/validation/diff_scaling_traces.py --c reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log --py reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log` (implementation pending Phase L2c).
- For sanity, `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python -m nanobrag_torch --help | head` reminds which flags the CLI exposes if questions arise mid-run.
- Use `git rev-parse HEAD` after the rerun to embed the commit SHA in notes.md for traceability.
- `ls -lh reports/2025-10-cli-flags/phase_l/scaling_audit/` helps verify file sizes changed as expected (trace log should update timestamp and size).
- `tail -n 60 reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log` should show the refreshed beam vector, steps, F_latt, fluence, and intensity fields.
- The C log can be inspected quickly via `grep -n "F_latt" reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log` for cross-checking values mentioned in notes.
- If you need to sanity-check HKL coverage, `head -n 20 scaled.hkl` will confirm the file is present and not empty.
- In case `trace_harness.py` raises module import errors, run `pip install -e .` to reestablish the editable install (document in Attempts if executed).
- To double-check oversample handling, search the simulator with `rg "auto-selected" src/nanobrag_torch/simulator.py` and confirm it no longer triggers once oversample=1 is set.
- When updating notes, include a pointer to docs/fix_plan.md Attempt #27 so the new findings sit alongside prior evidence.
Documentation Hooks:
- Mention in notes the exact commit SHA and date of this rerun for future archaeological digs.
- Cross-link the note entry to `plans/active/cli-noise-pix0/plan.md` Phase L2b so the engineer loop can trace provenance.
- If fluence remains mismatched, cite the relevant BeamConfig lines (src/nanobrag_torch/config.py around 430-470) in the note.
- Document any difference in `hkl_frac` vs C; specify whether the first divergence is still in fractional indices or in scaling terms.
- Record the observed `polar` value and relate it to the expected 0.9146 from C to show improvement/regression.
- Highlight changes in `steps` value; note if oversample stayed at 1 or reverted to auto selection.
- Note capture_fraction even if it remains 1.0; state that thickness absorption remains disabled.
- For completeness, mention `omega_pixel` difference (should remain within sub-percent of C).
- If `F_cell` reads 0, investigate HKL lookup in `crystal.get_structure_factor` and log the suspicion for Phase L3.
- Clarify whether the incident vector equals `[5.1388e-04, 0.0, -1.0]`; quote the log line so others can grep quickly.
EOF
' succeeded in 886ms:

[2025-10-07T03:33:45] tokens used: 162,662
[2025-10-07T03:33:45] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:33:48] thinking

**Checking line count**
[2025-10-07T03:33:48] exec bash -lc 'wc -l input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:33:49] bash -lc 'wc -l input.md' succeeded in 766ms:
      99 input.md
[2025-10-07T03:33:49] tokens used: 163,686
[2025-10-07T03:33:49] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:33:53] thinking

**Extending content to meet length requirement**
[2025-10-07T03:33:55] exec bash -lc "echo 'Audit Trail: update docs/fix_plan.md Attempt log after the rerun so future loops know Phase L2b0 is satisfied.' >> input.md" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:33:55] bash -lc "echo 'Audit Trail: update docs/fix_plan.md Attempt log after the rerun so future loops know Phase L2b0 is satisfied.' >> input.md" succeeded in 685ms:

[2025-10-07T03:33:55] tokens used: 163,931
[2025-10-07T03:33:55] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:07] thinking

**Counting final lines**
[2025-10-07T03:34:09] exec bash -lc 'wc -l input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:10] bash -lc 'wc -l input.md' succeeded in 756ms:
     100 input.md
[2025-10-07T03:34:10] tokens used: 164,159
[2025-10-07T03:34:10] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:13] thinking

**Checking git status**
[2025-10-07T03:34:13] exec bash -lc 'git status --short' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:14] bash -lc 'git status --short' succeeded in 741ms:
 M docs/fix_plan.md
 M input.md
 M plans/active/cli-noise-pix0/plan.md
[2025-10-07T03:34:14] tokens used: 164,348
[2025-10-07T03:34:14] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:17] thinking

**Verifying diffs and plan updates**
[2025-10-07T03:34:17] exec bash -lc 'git diff -- docs/fix_plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:18] bash -lc 'git diff -- docs/fix_plan.md' succeeded in 733ms:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e235657..9350112 100644
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
[2025-10-07T03:34:18] tokens used: 164,539
[2025-10-07T03:34:18] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:20] thinking

**Reviewing plan diff**
[2025-10-07T03:34:21] exec bash -lc 'git diff -- plans/active/cli-noise-pix0/plan.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:21] bash -lc 'git diff -- plans/active/cli-noise-pix0/plan.md' succeeded in 695ms:
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552a..052757b 100644
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@ Exit Criteria: C and PyTorch traces capturing scaling factors are stored under `
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
[2025-10-07T03:34:21] tokens used: 165,814
[2025-10-07T03:34:21] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:27] thinking

**Preparing to commit changes**
[2025-10-07T03:34:28] exec bash -lc 'git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md input.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:29] bash -lc 'git add docs/fix_plan.md plans/active/cli-noise-pix0/plan.md input.md' succeeded in 685ms:

[2025-10-07T03:34:29] tokens used: 167,738
[2025-10-07T03:34:29] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:32] thinking

**Preparing commit message**
[2025-10-07T03:34:33] exec bash -lc 'git commit -m "SUPERVISOR: CLI flag trace prep - tests: not run"' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:34] bash -lc 'git commit -m "SUPERVISOR: CLI flag trace prep - tests: not run"' succeeded in 697ms:
[feature/spec-based-2 b63b8fd] SUPERVISOR: CLI flag trace prep - tests: not run
 3 files changed, 102 insertions(+), 45 deletions(-)
[2025-10-07T03:34:34] tokens used: 168,052
[2025-10-07T03:34:34] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:36] thinking

**Pushing committed changes**
[2025-10-07T03:34:37] exec bash -lc 'git push' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:40] bash -lc 'git push' succeeded in 2.38s:
To github.com:hoidn/nanoBragg.git
   c3c6eef..b63b8fd  feature/spec-based-2 -> feature/spec-based-2
[2025-10-07T03:34:40] tokens used: 168,253
[2025-10-07T03:34:40] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:34:43] thinking

**Updating galph_memory.md with new entry**
[2025-10-07T03:34:46] exec bash -lc 'echo "## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
- Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
- Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out ...` and capture fresh notes.
- Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
" >> galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:34:49] bash -lc 'echo "## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
- Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
- Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out ...` and capture fresh notes.
- Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
" >> galph_memory.md' succeeded in 3.09s:
/Users/ollie/Documents/nanoBragg3/reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py:124: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
  'timestamp_iso': datetime.utcnow().isoformat() + 'Z'
Traceback (most recent call last):
  File "/Users/ollie/Documents/nanoBragg3/reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py", line 302, in <module>
    main()
    ~~~~^^
  File "/Users/ollie/Documents/nanoBragg3/reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py", line 239, in main
    simulator = Simulator(crystal_config, detector_config, beam_config, dtype=dtype, device=device)
  File "/Users/ollie/Documents/nanoBragg3/src/nanobrag_torch/simulator.py", line 498, in __init__
    self.incident_beam_direction = self.detector.beam_vector.clone()
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DetectorConfig' object has no attribute 'beam_vector'
[2025-10-07T03:34:49] tokens used: 168,686
[2025-10-07T03:34:50] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:01] thinking

**Investigating unintended command execution**
[2025-10-07T03:35:01] exec bash -lc 'tail -n 20 galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:35:02] bash -lc 'tail -n 20 galph_memory.md' succeeded in 685ms:
- Synced cleanly; re-read docs/index.md, specs/spec-a.md shards pointer, arch.md, c_to_pytorch_config_map.md, debugging/debugging.md, docs/fix_plan.md, testing_strategy.md, and plans/active/* for continuity before analysis.
- Deep dive vs long-term Goal #1 found TRACE_PY still emits placeholder scaling data (`polar=1`, `capture_fraction=1`, `steps=phi_steps`) so Phase L2b cannot progress; identified need to thread real tensors from `_apply_polarization` and `_apply_detector_absorption` through the debug path.
- Coin flip → heads: Reviewed Ralph’s recent commits (`3e47b38`, `f3f116b`, `f466b39`); C trace capture solid, but evidence-only harness attempt stalled because instrumentation can’t expose values—no regressions spotted.
- Updated `plans/active/cli-noise-pix0/plan.md` Phase L2 context and table to require simulator instrumentation fixes, a new TRACE_PY regression test, and harness rerun sequencing; synced `docs/fix_plan.md` next actions accordingly.
- Authored new `input.md` directing Ralph to patch TRACE_PY, add `tests/test_trace_pixel.py`, rerun the scaling harness, and archive comparison artifacts under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
- Follow-up for Ralph: Execute Phase L2b per updated plan—implement real scaling trace output, add the regression test, regenerate PyTorch trace + comparison JSON, and log Attempt update before moving to L2c.

## 2025-11-10 (galph loop — L2b harness rerun directive)
- Synced clean, reviewed core docs (`docs/index.md`, specs/spec-a.md shards, arch.md, config map, debugging SOP, testing strategy), CLI plan, and fix_plan entries before analysis per SOP.
- Confirmed Ralph’s latest work: commit 6b055dc replaced TRACE_PY placeholders with real tensors and added `tests/test_trace_pixel.py`; fix_plan Attempt #57 documents the change. Harness artifacts still missing (`trace_py_scaling.log` absent) because the prior run failed (`trace_py_fullrun.log` shows ValueError unpacking `read_hkl_file`).
- Evidence review: C scaling trace (`reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log:260-300`) reports `k_frac≈-0.607` and `F_latt≈-2.38`, while the latest PyTorch trace (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after_133134.log:46-65`) still shows `hkl_frac≈(2,2,-13)` and `F_latt≈4.47e4`. Polarization also stays at 1.0 vs C’s 0.9146. Scaling gap therefore persists upstream of normalization.
- Hypotheses logged: (1) fractional Miller index mismatch leading to runaway `F_latt`; (2) polarization factor still diverges despite new instrumentation. Both require a fresh PyTorch scaling trace to quantify.
- Rewrote `input.md` directing Ralph to rerun `trace_harness.py` with the supervisor command (slow=685, fast=1039), capture stdout/stderr, parse key factors into `scaling_audit_summary.md`, and keep `pytest --collect-only` in evidence mode; set “Mapped tests: none — evidence-only”.
- Next steps for Ralph: execute Phase L2b Step 3 (harness rerun), document deltas in notes/summary JSON, then move to Phase L2c comparison once numbers land. No new planning artifacts created; existing plan remains authoritative.
## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
- Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
- Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
- Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.

[2025-10-07T03:35:02] tokens used: 169,939
[2025-10-07T03:35:02] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:12] thinking

**Fixing missing command in entry line**
[2025-10-07T03:35:13] apply_patch auto_approved=true:
M /Users/ollie/Documents/nanoBragg3/galph_memory.md
@@ -1428,4 +1428,3 @@
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
[2025-10-07T03:35:13] apply_patch(auto_approved=true) exited 0 in 31ms:
Success. Updated the following files:
M galph_memory.md
[2025-10-07T03:35:13] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

[2025-10-07T03:35:13] tokens used: 171,497
[2025-10-07T03:35:13] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:23] thinking

**Handling commit for memory update**
[2025-10-07T03:35:24] exec bash -lc 'git add galph_memory.md' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:35:25] bash -lc 'git add galph_memory.md' succeeded in 893ms:

[2025-10-07T03:35:25] tokens used: 172,115
[2025-10-07T03:35:25] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:28] thinking

**Committing changes with clear message**
[2025-10-07T03:35:29] exec bash -lc 'git commit -m "SUPERVISOR: memory log sync - tests: not run"' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:35:29] bash -lc 'git commit -m "SUPERVISOR: memory log sync - tests: not run"' succeeded in 707ms:
[feature/spec-based-2 6d19618] SUPERVISOR: memory log sync - tests: not run
 1 file changed, 5 insertions(+)
[2025-10-07T03:35:29] tokens used: 172,349
[2025-10-07T03:35:29] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:32] thinking

**Performing a push operation**
[2025-10-07T03:35:33] exec bash -lc 'git push' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:35:35] bash -lc 'git push' succeeded in 2.05s:
To github.com:hoidn/nanoBragg.git
   b63b8fd..6d19618  feature/spec-based-2 -> feature/spec-based-2
[2025-10-07T03:35:35] tokens used: 172,590
[2025-10-07T03:35:35] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:37] thinking

**Checking working tree status**
[2025-10-07T03:35:38] exec bash -lc 'git status --short' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:35:39] bash -lc 'git status --short' succeeded in 757ms:
 M reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json
[2025-10-07T03:35:39] tokens used: 172,820
[2025-10-07T03:35:39] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:44] thinking

**Reverting unintended environment changes**
[2025-10-07T03:35:46] exec bash -lc 'git checkout -- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:35:46] bash -lc 'git checkout -- reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_env.json' succeeded in 693ms:

[2025-10-07T03:35:46] tokens used: 173,121
[2025-10-07T03:35:46] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:35:50] thinking

**Checking status**
[2025-10-07T03:35:51] exec bash -lc 'git status --short' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:35:52] bash -lc 'git status --short' succeeded in 722ms:

[2025-10-07T03:35:52] tokens used: 173,287
[2025-10-07T03:35:52] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:36:02] thinking

**Preparing final summary and references**

I’m gearing up to deliver a final response that wraps up the analysis and hypotheses with solid evidence from file lines and instructions, making sure to pinpoint exact line numbers for clarity and referencing plan updates clearly.
[2025-10-07T03:36:03] exec bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '1,80p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:36:04] bash -lc "nl -ba plans/active/cli-noise-pix0/plan.md | sed -n '1,80p'" succeeded in 873ms:
     1	## Context
     2	- Initiative: CLI Parity for nanoBragg PyTorch vs C (supports long-term goal in prompts/supervisor.md)
     3	- Phase Goal: Accept `-nonoise` and `-pix0_vector_mm` flags with C-equivalent semantics so the parallel comparison command in prompts/supervisor.md executes end-to-end.
     4	- Dependencies: specs/spec-a-cli.md §§3.2–3.4, docs/architecture/detector.md §5, docs/development/c_to_pytorch_config_map.md (detector pivot + noise), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860 (flag behavior), docs/debugging/detector_geometry_checklist.md (pix0 validation), docs/development/testing_strategy.md §2 (CLI parity tests).
     5	- Current gap snapshot (2025-10-18 refresh): Phase H4 parity landed (Attempt #25) with pix0 deltas < 2e-8 m and regression tolerances tightened; the remaining blocker before executing the supervisor command end-to-end is Phase I polarization alignment.
     6	- Gap snapshot update (2025-10-19): Attempt #27 parity run reveals 1.24538e5× intensity scaling mismatch despite polarization fix; Phases J–L track the normalization diagnosis, fix, and closure required before long-term Goal #1 completes.
     7	- Gap snapshot refresh (2025-10-22): Attempt #31 restored C precedence for custom detector vectors (pix0 override now gated again). Outstanding work: H5c PyTorch traces and Phase K normalization to eliminate the `F_latt`-driven intensity gap.
     8	- Gap snapshot update (2025-10-24): Attempt #35 reran the PyTorch trace harness post-unit-fix; metrics still show |Δpix0| ≈ 1.145 mm, so a fresh diagnostics phase is required before resuming normalization work.
     9	- Gap snapshot refresh (2025-10-26): Attempt #40 completed Phase H6f — custom detector vectors now force SAMPLE pivot parity, parity visuals relocated under `reports/2025-10-cli-flags/phase_h6/visuals/`, and targeted regression tests cover pivot selection. Outstanding work at that point: Phase H6g trace rerun to confirm |Δpix0| < 5e-5 m before returning to Phase K scalings.
    10	- Gap snapshot update (2025-10-27): Attempt #41 validated Phase H6g — SAMPLE pivot parity holds with pix0 deltas ≤2.85 µm, beam center deltas 0.0, and supporting artifacts under `reports/2025-10-cli-flags/phase_h6/post_fix/`. Remaining blockers: rerun the scaling chain (Phase K2), execute the scaling regression + doc updates (Phase K3), then resume Phase L parity once normalization matches C.
    11	- Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
    12	- Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
    13	- Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
    14	- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
    15	- Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
    16	- Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
    17	- Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
    18	- Gap recap (2025-11-05): Manual inspection of `src/nanobrag_torch/models/crystal.py` shows the cross-product rescale still runs even when MOSFLM reciprocal vectors are supplied (lines 681-705), and `BeamConfig.polarization_factor` defaults to 1.0, so scaling remains off despite SAMPLE pivot parity.
    19	- Gap snapshot update (2025-11-06): Traces (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after.log`) show the SAMPLE pivot fix leaves residual close-distance deltas (~2.8 µm), which push the fractional Miller index from 2.0012 (C) to 1.9993 (PyTorch). Those 0.002 shifts around integer values amplify sincg results (F_latt_b ≈ +21.6%). Need dtype diagnostics to confirm float32 rounding vs geometry formula drift before coding a fix.
    20	- Gap snapshot refresh (2025-11-07): dtype sweep (`reports/2025-10-cli-flags/phase_k/f_latt_fix/dtype_sweep/`) rules out precision and exposes a φ-grid mismatch—PyTorch logs `k≈1.9997` (φ=0°) while C logs `k≈1.9928` (φ=0.09°). Next action: capture per-φ traces to align sampling before rerunning normalization tests.
[2025-10-07T03:36:04] tokens used: 173,748
[2025-10-07T03:36:04] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:36:08] thinking

**Checking line numbers in fix_plan.md**
[2025-10-07T03:36:10] exec bash -lc "nl -ba docs/fix_plan.md | sed -n '440,520p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:36:11] bash -lc "nl -ba docs/fix_plan.md | sed -n '440,520p'" succeeded in 722ms:
   440	- Immediate Next Actions (2025-10-15):
   441	  * Update `parse_and_validate_args` (and any helper that constructs `CrystalConfig`) to thread the parsed dtype/device into HKL/Fdump loaders so CLI `-dtype` honours float64 from the start.
   442	  * Add a regression test (e.g., `tests/test_cli_dtype.py`) that invokes the parser with `-dtype float64` and asserts `config['hkl_data'][0].dtype == torch.float64`, covering both HKL and Fdump paths.
   443	  * Re-run AT-IO-003 and the CLI smoke (help + minimal render) to ensure no regressions, capturing logs under `reports/DTYPE-CLI/<date>/`.
   444	- Attempts History: None — new item.
   445	- Risks/Assumptions: Ensure `write_fdump` continues to emit float64 on disk (spec requirement) while the in-memory tensor honours caller dtype; watch for latent callers that relied on the old float64 default during plan DTYPE-DEFAULT-001 migration.
   446	- Exit Criteria: CLI runs with `-dtype float64` produce double-precision HKL/Fdump tensors end-to-end, regression test passes, and existing dtype-sensitive tests (AT-IO-003, CLI smoke, gradchecks) remain green.
   447	
   448	## [CLI-FLAGS-003] Handle -nonoise and -pix0_vector_mm
   449	- Spec/AT: specs/spec-a-cli.md flag catalogue, docs/architecture/detector.md §5 (pix0 workflow), docs/development/c_to_pytorch_config_map.md (pivot rules), golden_suite_generator/nanoBragg.c lines 720–1040 & 1730–1860
   450	- Priority: High
   451	- Status: in_progress (Phases A–H complete; K3a/K3b/K3d landed via Attempts #43–44, pending per-φ evidence + normalization closure)
   452	- Owner/Date: ralph/2025-10-05
   453	- Plan Reference: `plans/active/cli-noise-pix0/plan.md`
   454	- Reproduction (C & PyTorch):
   455	  * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   456	  * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
   457	- First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
   458	- Next Actions (2025-11-10 refresh, amended 2025-11-11):
   459	  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
[2025-10-07T03:36:11] tokens used: 175,354
[2025-10-07T03:36:11] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:36:13] thinking

**Checking galph_memory.md references**
[2025-10-07T03:36:14] exec bash -lc 'nl -ba galph_memory.md | tail -n 20' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:36:15] bash -lc 'nl -ba galph_memory.md | tail -n 20' succeeded in 733ms:
  1411	## 2025-11-10 (galph loop — Phase L2b instrumentation)
  1412	- Synced cleanly; re-read docs/index.md, specs/spec-a.md shards pointer, arch.md, c_to_pytorch_config_map.md, debugging/debugging.md, docs/fix_plan.md, testing_strategy.md, and plans/active/* for continuity before analysis.
  1413	- Deep dive vs long-term Goal #1 found TRACE_PY still emits placeholder scaling data (`polar=1`, `capture_fraction=1`, `steps=phi_steps`) so Phase L2b cannot progress; identified need to thread real tensors from `_apply_polarization` and `_apply_detector_absorption` through the debug path.
  1414	- Coin flip → heads: Reviewed Ralph’s recent commits (`3e47b38`, `f3f116b`, `f466b39`); C trace capture solid, but evidence-only harness attempt stalled because instrumentation can’t expose values—no regressions spotted.
  1415	- Updated `plans/active/cli-noise-pix0/plan.md` Phase L2 context and table to require simulator instrumentation fixes, a new TRACE_PY regression test, and harness rerun sequencing; synced `docs/fix_plan.md` next actions accordingly.
  1416	- Authored new `input.md` directing Ralph to patch TRACE_PY, add `tests/test_trace_pixel.py`, rerun the scaling harness, and archive comparison artifacts under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
  1417	- Follow-up for Ralph: Execute Phase L2b per updated plan—implement real scaling trace output, add the regression test, regenerate PyTorch trace + comparison JSON, and log Attempt update before moving to L2c.
  1418	
  1419	## 2025-11-10 (galph loop — L2b harness rerun directive)
  1420	- Synced clean, reviewed core docs (`docs/index.md`, specs/spec-a.md shards, arch.md, config map, debugging SOP, testing strategy), CLI plan, and fix_plan entries before analysis per SOP.
  1421	- Confirmed Ralph’s latest work: commit 6b055dc replaced TRACE_PY placeholders with real tensors and added `tests/test_trace_pixel.py`; fix_plan Attempt #57 documents the change. Harness artifacts still missing (`trace_py_scaling.log` absent) because the prior run failed (`trace_py_fullrun.log` shows ValueError unpacking `read_hkl_file`).
  1422	- Evidence review: C scaling trace (`reports/2025-10-cli-flags/phase_l/scaling_audit/c_trace_scaling.log:260-300`) reports `k_frac≈-0.607` and `F_latt≈-2.38`, while the latest PyTorch trace (`reports/2025-10-cli-flags/phase_k/f_latt_fix/trace_py_after_133134.log:46-65`) still shows `hkl_frac≈(2,2,-13)` and `F_latt≈4.47e4`. Polarization also stays at 1.0 vs C’s 0.9146. Scaling gap therefore persists upstream of normalization.
  1423	- Hypotheses logged: (1) fractional Miller index mismatch leading to runaway `F_latt`; (2) polarization factor still diverges despite new instrumentation. Both require a fresh PyTorch scaling trace to quantify.
  1424	- Rewrote `input.md` directing Ralph to rerun `trace_harness.py` with the supervisor command (slow=685, fast=1039), capture stdout/stderr, parse key factors into `scaling_audit_summary.md`, and keep `pytest --collect-only` in evidence mode; set “Mapped tests: none — evidence-only”.
  1425	- Next steps for Ralph: execute Phase L2b Step 3 (harness rerun), document deltas in notes/summary JSON, then move to Phase L2c comparison once numbers land. No new planning artifacts created; existing plan remains authoritative.
  1426	## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
  1427	- Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
  1428	- Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
  1429	- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
  1430	- Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
[2025-10-07T03:36:15] tokens used: 177,063
[2025-10-07T03:36:15] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:36:29] thinking

**Summarizing final message content**
[2025-10-07T03:36:30] exec bash -lc 'nl -ba reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log | head -n 40' in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:36:31] bash -lc 'nl -ba reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log | head -n 40' succeeded in 849ms:
     1	TRACE_PY: pix0_vector_meters 0.231274664402008 0.217322006821632 -0.211817994713783
     2	TRACE_PY: fdet_vec 0 0 1
     3	TRACE_PY: sdet_vec 0 -1 0
     4	TRACE_PY: pixel_pos_meters 0.231274664402008 0.099416010081768 -0.0330239981412888
     5	TRACE_PY: R_distance_meters 0.253893882036209
     6	TRACE_PY: omega_pixel_sr 4.18050007056081e-07
     7	TRACE_PY: close_distance_meters 0.231274664402008
     8	TRACE_PY: obliquity_factor 0.910910725593567
     9	TRACE_PY: diffracted_vec 0.910910785198212 0.391565203666687 -0.130070075392723
    10	TRACE_PY: incident_vec 0 0 1
    11	TRACE_PY: lambda_meters 9.7680002450943e-11
    12	TRACE_PY: lambda_angstroms 0.97680002450943
    13	TRACE_PY: scattering_vec_A_inv 9325458432 4008652544 -11569103872
    14	TRACE_PY: rot_a_angstroms -14.3562698364258 -21.8717937469482 -5.58202123641968
    15	TRACE_PY: rot_b_angstroms -11.4986982345581 0.717319786548615 -29.1132164001465
    16	TRACE_PY: rot_c_angstroms 21.0699501037598 -24.3892974853516 -9.75265312194824
    17	TRACE_PY: rot_a_star_A_inv -0.0290510933846235 -0.0293958801776171 0.010749876499176
    18	TRACE_PY: rot_b_star_A_inv -0.00312639237381518 0.0104376431554556 -0.0328566692769527
    19	TRACE_PY: rot_c_star_A_inv 0.0259604379534721 -0.0143330143764615 -0.0106066130101681
    20	TRACE_PY: hkl_frac -15.6976222991943 23.24587059021 21.1548194885254
[2025-10-07T03:36:31] tokens used: 179,101
[2025-10-07T03:36:31] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:36:35] thinking

**Noting vectorization plan status**
[2025-10-07T03:36:36] exec bash -lc "nl -ba plans/active/vectorization.md | sed -n '1,80p'" in /Users/ollie/Documents/nanoBragg3
[2025-10-07T03:36:36] bash -lc "nl -ba plans/active/vectorization.md | sed -n '1,80p'" succeeded in 667ms:
     1	## Context
     2	- Initiative: PERF-PYTORCH-004 vectorization backlog; supports long-term goal "Parallel parity + performance" from docs/fix_plan.md
     3	- Phase Goal: Deliver a production-ready, fully vectorized tricubic interpolation pipeline (gather + polynomial evaluation) and follow with detector absorption vectorization without regressing physics, gradients, or device neutrality.
     4	- Dependencies: specs/spec-a-core.md §4 (Structure factors), specs/spec-a-parallel.md §2.3 (Interpolation acceptance tests), docs/architecture/pytorch_design.md §§2.2–2.4, docs/architecture/c_code_overview.md (tricubic routines), docs/development/testing_strategy.md §§2–4, docs/development/pytorch_runtime_checklist.md, `nanoBragg.c` lines 2604–3278 (polin3/polin2/polint + detector absorption), `tests/test_at_str_002.py`, `tests/test_at_abs_001.py`, `reports/benchmarks/20250930-165726-compile-cache/` (current performance references). No dedicated tricubic/absorption microbenchmarks exist yet; Phase A will author reusable harnesses under `scripts/benchmarks/` so future loops can rerun baselines without ad-hoc snippets.
     5	- Gap Snapshot (2025-11-09): Phase A remains untouched; no artifacts exist under `reports/2025-10-vectorization/phase_a/`, so Ralph must prioritise A1–A3 baseline capture before any implementation edits. docs/fix_plan.md item VECTOR-TRICUBIC-001 lists these as the gating next actions.
     6	
     7	### Phase A — Evidence & Baseline Capture
     8	Goal: Document the current (non-vectorized) behavior, warnings, and performance so we can prove the impact of the refactor.
     9	Prereqs: Editable install (`pip install -e .`), C binary available for parity (when needed), environment variable `KMP_DUPLICATE_LIB_OK=TRUE`.
    10	Exit Criteria: Baseline pytest log, timing snippet results, and notes stored under `reports/2025-10-vectorization/phase_a/` with references in docs/fix_plan.md Attempt history.
    11	
    12	| ID | Task Description | State | How/Why & Guidance |
    13	| --- | --- | --- | --- |
    14	| A1 | Re-run tricubic acceptance tests | [ ] | `env KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_at_str_002.py -v` ⇒ save log to `reports/2025-10-vectorization/phase_a/test_at_str_002.log`; confirm nearest-neighbour fallback warning message appears for batched inputs. |
    15	| A2 | Measure current tricubic runtime | [ ] | Author `scripts/benchmarks/tricubic_baseline.py` (device/dtype neutral, honours `KMP_DUPLICATE_LIB_OK=TRUE`) and run `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/tricubic_baseline.py --sizes 256 512 --repeats 5`. Capture CPU timings (and CUDA when available) in `phase_a/tricubic_baseline.md` plus raw JSON/CSV under `phase_a/tricubic_baseline_results.json`. |
    16	| A3 | Record detector absorption baseline | [ ] | Create `scripts/benchmarks/absorption_baseline.py` mirroring A2's interface; execute `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python scripts/benchmarks/absorption_baseline.py --sizes 256 512 --repeats 5` alongside `env KMP_DUPLICATE_LIB_OK=TRUE pytest tests/test_at_abs_001.py -v`. Store logs in `phase_a/absorption_baseline.md` and metrics in `phase_a/absorption_baseline_results.json`. |
    17	
    18	### Phase B — Tricubic Vectorization Design
    19	Goal: Lock the tensor-shape design, broadcasting plan, and gradient checks before code changes.
    20	Prereqs: Phase A artifacts uploaded and referenced in docs/fix_plan.md.
[2025-10-07T03:36:36] tokens used: 180,298
[2025-10-07T03:36:36] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation


[2025-10-07T03:36:47] thinking

**Organizing final message structure**

I'll outline the final message with sections for a quick explanation, analysis bullets detailing observed issues and plans, a hypotheses list linking evidence and next steps, and a clear next steps summary to guide Ralph on rerunning with full flags.
[2025-10-07T03:36:47] codex

Updated the CLI parity plan and fix_plan so Phase L2 starts with a rerun of the scaling trace using the complete supervisor flag set, and delivered a detailed input memo guiding that work.

**Analysis**
- The current Py trace still reflects the wrong geometry: `trace_py_scaling.log:1-31` shows the incident vector stuck at `0,0,1`, `steps 160`, and fluence ≈1.26×10⁵× high because `-beam_vector`, `-oversample 1`, `-flux`, and `-beamsize 1.0` were not applied during the capture.
- I added a new gap snapshot plus a pre-step in `plans/active/cli-noise-pix0/plan.md:14` and updated Next Actions in `docs/fix_plan.md:458-463` so future work can’t proceed until that trace harness is rerun with the authoritative command.
- Supervisor memory log `galph_memory.md:1427-1430` now records the expectation that the rerun also refreshes `trace_py_env.json` and `notes.md` before moving on to instrumentation tests.
- Long-term goal 2 (tricubic vectorization) remains stalled at Phase A with no baseline artifacts, as reiterated in `plans/active/vectorization.md:1-40`; we still need timing logs and benchmark scaffolding before design/implementation can start.

**Hypotheses**
1. **High confidence:** Trace harness omissions (beam vector, oversample, flux, beamsize) are the primary driver of the massive scaling mismatch; rerunning `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the full supervisor flags should drop `steps` back to 10 and restore the custom beam direction. *Next confirming step:* execute the command spelled out in input.md and verify the new `trace_py_scaling.log`.
2. **Medium confidence:** Once the harness uses the correct flags, any remaining divergence will likely surface first in `hkl_frac`/`F_cell`; if those still misalign with the C trace, we’ll need to inspect scattering vector unit handling inside `Simulator` in a follow-up loop. *Next confirming step:* diff the refreshed Py trace against `c_trace_scaling.log` focusing on the fractional indices.
3. **Medium confidence:** Vectorization work will continue to slip until Phase A baselines are captured; documenting current tricubic/absorption performance (per plan tasks A1–A3) is prerequisite evidence before design changes. *Next confirming step:* schedule a loop dedicated to gathering the Phase A artefacts once the CLI parity item advances.

**Next Steps**
- Follow `input.md:1-100`: rerun `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32`, then update `trace_py_env.json` and `notes.md` with the new metrics.
- Record the attempt in `docs/fix_plan.md` Attempts History and stash console logs under `reports/2025-10-cli-flags/phase_l/scaling_audit/`.
- Once the trace reflects the correct flags, proceed to the planned regression test (`tests/test_trace_pixel.py`) and the Phase L2c comparison script; defer vectorization work until the CLI parity path is unblocked.
[2025-10-07T03:36:57] tokens used: 182,972
[2025-10-07T03:36:57] turn diff:
diff --git a/docs/fix_plan.md b/docs/fix_plan.md
index e2356572206cef38ee71fa24dda92440ad302fd2..9350112c830983ae4d4f8aed83b593b2477573fa
--- a/docs/fix_plan.md
+++ b/docs/fix_plan.md
@@ -455,11 +455,12 @@
   * C: Run the supervisor command from `prompts/supervisor.md` (with and without `-nonoise`) using `NB_C_BIN=./golden_suite_generator/nanoBragg`; capture whether the noisefile is skipped and log `DETECTOR_PIX0_VECTOR`.
   * PyTorch: After implementation, `nanoBragg` CLI should parse the same command, respect the pix0 override, and skip noise writes when `-nonoise` is present.
 - First Divergence (if known): Phase K3e evidence reveals a **fundamental lattice/geometry mismatch**, not a φ-grid offset. C reports `k_frac≈−3.857` across all φ steps while PyTorch reports `k_frac≈−9.899` (Δk≈6.04 at φ=0°). This 6-unit discrepancy indicates the base reciprocal lattice vectors or scattering geometry differ before any φ rotation is applied.
-- Next Actions (2025-11-10 refresh):
-  1. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
-  2. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
-  3. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
-  4. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
+- Next Actions (2025-11-10 refresh, amended 2025-11-11):
+  1. Phase L2b0 — Sync `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` with the authoritative supervisor command (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`) and capture a fresh `trace_py_env.json` proving the flag set before re-analysing metrics.
+  2. Phase L2b — Patch `src/nanobrag_torch/simulator.py` trace output so `TRACE_PY` surfaces the *actual* scaling factors: total `steps` (sources×mosaic×φ×oversample²), real polarization (reuse `_apply_polarization` tensor instead of hardcoded 1.0), and capture_fraction from `_apply_detector_absorption`. Keep tensors device/dtype neutral and gate the logging behind `trace_pixel` only.
+  3. Phase L2b — After instrumentation is fixed, add a targeted regression test (plan suggests `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) so future trace regressions surface immediately, then rerun/finish `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to capture `trace_py_scaling.log`, env/config snapshots, and notes for pixel (685,1039) using the supervisor command parameters.
+  4. Phase L2c — Diff TRACE_C vs TRACE_PY via `compare_scaling_traces.py`, summarize deltas in `scaling_audit_summary.md`, and log Attempt #55 with quantified first divergence.
+  5. Prep L3 design notes once the divergent factor is confirmed (update plan Phase L3 guidance + identify targeted regression selector).
 - Attempts History:
   * [2025-10-06] Attempt #27 (ralph) — Result: **PARITY FAILURE** (Phase I3 supervisor command). **Intensity scaling discrepancy: 124,538× sum ratio.**
     Metrics: Correlation=0.9978 (< 0.999 threshold), sum_ratio=124,538 (should be ~1.0), C max_I=446, PyTorch max_I=5.411e7 (121,000× discrepancy), mean_peak_distance=37.79 px (> 1 px threshold).
diff --git a/galph_memory.md b/galph_memory.md
index b59b24dfc3d62a50a4bf59d8889b20f226784ad7..79282732344290bda9b74c510195a2741f60aa21
--- a/galph_memory.md
+++ b/galph_memory.md
@@ -1426,6 +1426,5 @@
 ## 2025-11-11 (galph loop — Phase L2b0 harness reminder)
 - Added new gap snapshot in plans/active/cli-noise-pix0/plan.md noting prior Py trace omitted -beam_vector/-oversample/-flux/-beamsize, which forced auto oversample=4 and +Z beam.
 - Updated Phase L2b guidance to require rerunning trace_harness.py with the authoritative supervisor command before analysing scaling factors; docs/fix_plan.md now lists Phase L2b0 as step 1.
-- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via  and capture fresh notes.
+- Issued input.md loop instructions directing Ralph to regenerate trace_py_scaling.log via `PYTHONPATH=src KMP_DUPLICATE_LIB_OK=TRUE python reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py --config supervisor --pixel 685 1039 --out reports/2025-10-cli-flags/phase_l/scaling_audit/trace_py_scaling.log --device cpu --dtype float32` and capture fresh notes.
 - Expect Ralph to refresh trace_py_env.json/notes.md, then proceed to instrumentation regression test (tests/test_trace_pixel.py) once Phase L2b0 evidence lands.
-
diff --git a/plans/active/cli-noise-pix0/plan.md b/plans/active/cli-noise-pix0/plan.md
index 134552af42c386200c78c8a4006793ea7d35c782..052757b15f7a7261a9d379e657c59819fee7cd1d
--- a/plans/active/cli-noise-pix0/plan.md
+++ b/plans/active/cli-noise-pix0/plan.md
@@ -11,6 +11,7 @@
 - Gap snapshot refresh (2025-11-09): Attempt #49 closed Phase K3g3 — `tests/test_cli_scaling.py::test_f_latt_square_matches_c` now passes after isolating the HKL cache, but the full supervisor command still shows correlation ≈ 0.9965 and sum_ratio ≈ 1.26×10⁵. Need fresh HKL/normalization parity evidence (Phase L) before rerunning nb-compare.
 - Gap snapshot refresh (2025-11-10): Attempt #56 (evidence loop) proved the existing `TRACE_PY` path prints placeholder scaling data (`capture_fraction 1`, `polar 1.0`, `steps`=φ-steps only). Without exposing the real per-pixel factors computed in `Simulator.run`, Phase L2b cannot produce a trustworthy PyTorch trace. Instrumentation must pull actual values from `_apply_detector_absorption`, polarization kernels, and total-step calculation before rerunning the harness.
 - Gap snapshot update (2025-10-17): Attempt #53 (Phase L1c) realigned HKL Fdump I/O with C padding (`read_fdump`/`write_fdump`) and added regression coverage (`TestHKLFdumpParity::test_scaled_hkl_roundtrip`) with max |ΔF| = 0.0. Proceed to L1d to regenerate parity metrics before reviving scaling traces.
+- Gap snapshot update (2025-11-11): Current Phase L2 traces omit key supervisor flags (`-beam_vector`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`). Resulting Py traces default to a +Z beam and auto-selected oversample=4, inflating `steps` and zeroing `F_cell`. Before further analysis, rerun the trace harness with the authoritative command so scaling deltas reflect real parity behavior.
 - Gap snapshot addendum (2025-10-06T17:59 replayed 2025-11-09): Re-running Phase L1 with `Fdump_scaled_20251006175946.bin` shows the cache stores 182,700 doubles (≈9,534 more than the 173,166 expected from header ranges) and that reflections shift by Δk≈+10–12 / Δl≈−2…−8 relative to HKL indices. Use this dataset to reverse-engineer the C layout before touching PyTorch HKL IO.
 - Gap insight (2025-10-31): Scaling evidence shows MOSFLM matrix runs still rescale real vectors using formula lengths; C skips that step unless `-cell` is provided, so PyTorch must gate the `vector_rescale` analogue when `mosflm_a_star` is set. The same trace confirms the polarization Kahn factor should default to 0.0 (C’s `polarization`), not 1.0, so BeamConfig defaults need to be realigned before Phase L.
 - Gap correction (2025-10-22): Fresh C evidence (`reports/2025-10-cli-flags/phase_h5/c_precedence_2025-10-22.md`) proves nanoBragg ignores `-pix0_vector_mm` whenever custom detector vectors are present. PyTorch now deviates because the lattice factor uses fractional indices (h−h0); fix required before normalization parity.
@@ -237,7 +238,7 @@
 | ID | Task Description | State | How/Why & Guidance |
 | --- | --- | --- | --- |
 | L2a | Instrument C scaling chain | [D] | ✅ 2025-10-06 Attempt #55: Reused existing TRACE_C hooks (nanoBragg.c 3367-3382), captured `c_trace_scaling.log` for pixel (685,1039), and recorded setup in `instrumentation_notes.md`. Artifacts live under `reports/2025-10-cli-flags/phase_l/scaling_audit/`; ready to mirror fields on the PyTorch side. |
-| L2b | Expose and log PyTorch scaling factors | [ ] | *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, update/extend `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` to drive the supervisor command configs, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
+| L2b | Expose and log PyTorch scaling factors | [ ] | *Pre-step (2025-11-11)*: Update `reports/2025-10-cli-flags/phase_l/scaling_audit/trace_harness.py` so the CLI invocation matches the supervisor command verbatim (include `-beam_vector 0.00051387949 0.0 -0.99999986`, `-oversample 1`, `-flux 1e18`, `-beamsize 1.0`, and `-nonoise`). Confirm via refreshed `trace_py_env.json` that these flags are present before analysing metrics. *Step 1*: Patch `src/nanobrag_torch/simulator.py` trace path (see `git blame` around 1193-1355) so `TRACE_PY` prints the **actual** per-pixel values from the live computation: reuse the polarization tensor returned by `_apply_polarization`, capture the post-absorption scalar from `_apply_detector_absorption`, and emit the full `steps` product (`sources × mosaic × φ × oversample²`) instead of bare `phi_steps`. Avoid recomputing placeholders; thread the tensors through the debug-config dict so gradient paths stay untouched. *Step 2*: Add a minimal regression test (e.g., `tests/test_trace_pixel.py::test_scaling_trace_matches_physics`) that asserts the new trace reports C-matched values for a 1×1 ROI. *Step 3*: Once instrumentation is accurate, rerun the harness with the authoritative command, call Simulator with `trace_pixel=[685,1039]`, and persist stdout/JSON snapshots to `trace_py_scaling.log`, `trace_py_env.json`, and `notes.md`. |
 | L2c | Diff traces & record first divergence | [ ] | Build a comparison notebook or script (`compare_scaling_traces.py`) that aligns TRACE_C vs TRACE_PY lines and reports percentage deltas per factor. Summarize findings in `scaling_audit_summary.md`, explicitly calling out the earliest divergent term and hypothesized root cause. Update docs/fix_plan.md Attempt #55 with key metrics. |
 
 ### Phase L3 — Normalization Fix Implementation

Reading prompt from stdin...
To github.com:hoidn/nanoBragg.git
   6d19618..4dd4d46  HEAD -> feature/spec-based-2
Waiting for Ralph to complete i=65...
Already up to date.
[sync] state: actor=ralph status=waiting-ralph iter=65
Updating 4dd4d46..a36b0be
Fast-forward
 sync/state.json | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)
From github.com:hoidn/nanoBragg
   4dd4d46..a36b0be  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=ralph status=running-ralph iter=65
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating a36b0be..1d2e6ac
Fast-forward
 docs/fix_plan.md                                   |  19 ++++
 .../phase_l/scaling_audit/harness_run.log          |   9 ++
 .../phase_l/scaling_audit/notes.md                 | 108 +++++++++++++++++++++
 .../phase_l/scaling_audit/trace_harness.py         |   5 +-
 .../phase_l/scaling_audit/trace_py_env.json        |  15 +--
 .../phase_l/scaling_audit/trace_py_scaling.log     |  41 ++------
 6 files changed, 152 insertions(+), 45 deletions(-)
 create mode 100644 reports/2025-10-cli-flags/phase_l/scaling_audit/harness_run.log
From github.com:hoidn/nanoBragg
   a36b0be..1d2e6ac  feature/spec-based-2 -> origin/feature/spec-based-2
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Already up to date.
Updating 1d2e6ac..1f2bcf9
Fast-forward
 sync/state.json | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)
From github.com:hoidn/nanoBragg
   1d2e6ac..1f2bcf9  feature/spec-based-2 -> origin/feature/spec-based-2
[sync] state: actor=galph status=complete iter=66
Ralph completed iteration 65; proceeding to 66
